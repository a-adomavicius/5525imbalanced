{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2377e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff44b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(preds, truth):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for j in range(len(preds)):\n",
    "        if truth[j] == 1 and preds[j] == 1:\n",
    "            tp += 1\n",
    "        elif truth[j] == 1:\n",
    "            fn += 1\n",
    "        elif preds[j] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else: \n",
    "        f1 = 0\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbe8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/YearPredictionMSD.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f45e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion by year 1922: 1.1642685967652738e-05\n",
      "Proportion by year 1923: 1.1642685967652738e-05\n",
      "Proportion by year 1924: 2.1344924274030017e-05\n",
      "Proportion by year 1925: 3.4928057902958214e-05\n",
      "Proportion by year 1926: 7.179656346719188e-05\n",
      "Proportion by year 1927: 0.00015329536524076105\n",
      "Proportion by year 1928: 0.00025419864362708476\n",
      "Proportion by year 1929: 0.0004346602761257022\n",
      "Proportion by year 1930: 0.0005122781825767205\n",
      "Proportion by year 1931: 0.0005801938507213614\n",
      "Proportion by year 1932: 0.0006015387749953914\n",
      "Proportion by year 1933: 0.0006131814609630442\n",
      "Proportion by year 1934: 0.0006694544431400324\n",
      "Proportion by year 1935: 0.0007160251870106433\n",
      "Proportion by year 1936: 0.0007645363785425298\n",
      "Proportion by year 1937: 0.0008188689130582426\n",
      "Proportion by year 1938: 0.0008557374186224762\n",
      "Proportion by year 1939: 0.0009236530867671171\n",
      "Proportion by year 1940: 0.001024556365153441\n",
      "Proportion by year 1941: 0.0010866506903142554\n",
      "Proportion by year 1942: 0.0011332214341848665\n",
      "Proportion by year 1943: 0.0011603877014427228\n",
      "Proportion by year 1944: 0.0011894944163618546\n",
      "Proportion by year 1945: 0.0012477078462001185\n",
      "Proportion by year 1946: 0.0013039808283771066\n",
      "Proportion by year 1947: 0.0014145863450698076\n",
      "Proportion by year 1948: 0.0014980255945046522\n",
      "Proportion by year 1949: 0.0016144524541811796\n",
      "Proportion by year 1950: 0.0017755096100670424\n",
      "Proportion by year 1951: 0.0019191027370014263\n",
      "Proportion by year 1952: 0.0020685172069196366\n",
      "Proportion by year 1953: 0.002326596745869272\n",
      "Proportion by year 1954: 0.002565271808206153\n",
      "Proportion by year 1955: 0.0030988949150569036\n",
      "Proportion by year 1956: 0.004195247843677537\n",
      "Proportion by year 1957: 0.005353695097458984\n",
      "Proportion by year 1958: 0.006484976083982575\n",
      "Proportion by year 1959: 0.007633721099457645\n",
      "Proportion by year 1960: 0.008456470907838439\n",
      "Proportion by year 1961: 0.009564466522426724\n",
      "Proportion by year 1962: 0.010738437357498375\n",
      "Proportion by year 1963: 0.012488721147968836\n",
      "Proportion by year 1964: 0.014322444187874142\n",
      "Proportion by year 1965: 0.016495745568502654\n",
      "Proportion by year 1966: 0.01916774199807896\n",
      "Proportion by year 1967: 0.02250143108015019\n",
      "Proportion by year 1968: 0.026124246863751466\n",
      "Proportion by year 1969: 0.030412636195170227\n",
      "Proportion by year 1970: 0.03497074775150627\n",
      "Proportion by year 1971: 0.03910584171768427\n",
      "Proportion by year 1972: 0.043545585966682517\n",
      "Proportion by year 1973: 0.0485829880953536\n",
      "Proportion by year 1974: 0.052820925787579194\n",
      "Proportion by year 1975: 0.057637116882864874\n",
      "Proportion by year 1976: 0.0618653523367841\n",
      "Proportion by year 1977: 0.06672035238529529\n",
      "Proportion by year 1978: 0.07239810224218728\n",
      "Proportion by year 1979: 0.07842901357343139\n",
      "Proportion by year 1980: 0.08444634177104658\n",
      "Proportion by year 1981: 0.09058203727599957\n",
      "Proportion by year 1982: 0.09756182751360738\n",
      "Proportion by year 1983: 0.10413218329468608\n",
      "Proportion by year 1984: 0.11066761101786182\n",
      "Proportion by year 1985: 0.1176105327499054\n",
      "Proportion by year 1986: 0.12579728143282656\n",
      "Proportion by year 1987: 0.13573625435387943\n",
      "Proportion by year 1988: 0.14662410618129604\n",
      "Proportion by year 1989: 0.1595668920820033\n",
      "Proportion by year 1990: 0.17364678031221803\n",
      "Proportion by year 1991: 0.1904258312392669\n",
      "Proportion by year 1992: 0.20894352327081858\n",
      "Proportion by year 1993: 0.22936673490574275\n",
      "Proportion by year 1994: 0.25288690100806255\n",
      "Proportion by year 1995: 0.27861141565359127\n",
      "Proportion by year 1996: 0.3060299411074135\n",
      "Proportion by year 1997: 0.33548981750089746\n",
      "Proportion by year 1998: 0.36617605681630755\n",
      "Proportion by year 1999: 0.4015659412626493\n",
      "Proportion by year 2000: 0.43898747441034647\n",
      "Proportion by year 2001: 0.48088173941728357\n",
      "Proportion by year 2002: 0.5263871775218543\n",
      "Proportion by year 2003: 0.5795205153828988\n",
      "Proportion by year 2004: 0.6369713492902813\n",
      "Proportion by year 2005: 0.704793875947181\n",
      "Proportion by year 2006: 0.777626638465494\n",
      "Proportion by year 2007: 0.854088038110392\n",
      "Proportion by year 2008: 0.9215379988163269\n",
      "Proportion by year 2009: 0.9817656133269945\n",
      "Proportion by year 2010: 0.9999980595523387\n"
     ]
    }
   ],
   "source": [
    "for year in range(1922, 2011):\n",
    "    #print(\"Songs by year \" + str(year) + \": \" + str(len(df[df[0] <= year])))\n",
    "    print(\"Proportion by year \" + str(year) + \": \" + str(len(df[df[0] <= year])/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb79cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922        6\n",
      "1924        5\n",
      "1925        7\n",
      "1926       19\n",
      "1927       42\n",
      "1928       52\n",
      "1929       93\n",
      "1930       40\n",
      "1931       35\n",
      "1932       11\n",
      "1933        6\n",
      "1934       29\n",
      "1935       24\n",
      "1936       25\n",
      "1937       28\n",
      "1938       19\n",
      "1939       35\n",
      "1940       52\n",
      "1941       32\n",
      "1942       24\n",
      "1943       14\n",
      "1944       15\n",
      "1945       30\n",
      "1946       29\n",
      "1947       57\n",
      "1948       43\n",
      "1949       60\n",
      "1950       83\n",
      "1951       74\n",
      "1952       77\n",
      "1953      133\n",
      "1954      123\n",
      "1955      275\n",
      "1956      565\n",
      "1957      597\n",
      "1958      583\n",
      "1959      592\n",
      "1960      424\n",
      "1961      571\n",
      "1962      605\n",
      "1963      902\n",
      "1964      945\n",
      "1965     1120\n",
      "1966     1377\n",
      "1967     1718\n",
      "1968     1867\n",
      "1969     2210\n",
      "1970     2349\n",
      "1971     2131\n",
      "1972     2288\n",
      "1973     2596\n",
      "1974     2184\n",
      "1975     2482\n",
      "1976     2179\n",
      "1977     2502\n",
      "1978     2926\n",
      "1979     3108\n",
      "1980     3101\n",
      "1981     3162\n",
      "1982     3597\n",
      "1983     3386\n",
      "1984     3368\n",
      "1985     3578\n",
      "1986     4219\n",
      "1987     5122\n",
      "1988     5611\n",
      "1989     6670\n",
      "1990     7256\n",
      "1991     8647\n",
      "1992     9543\n",
      "1993    10525\n",
      "1994    12121\n",
      "1995    13257\n",
      "1996    14130\n",
      "1997    15182\n",
      "1998    15814\n",
      "1999    18238\n",
      "2000    19285\n",
      "2001    21590\n",
      "2002    23451\n",
      "2003    27382\n",
      "2004    29607\n",
      "2005    34952\n",
      "2006    37534\n",
      "2007    39404\n",
      "2008    34760\n",
      "2009    31038\n",
      "2010     9396\n",
      "2011        1\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "year = df[0]\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(year.value_counts().sort_index())\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['before_1989'] = df[0] <= 1989\n",
    "df['before_1989'] = df['before_1989'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23994d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    433113\n",
       "1     82232\n",
       "Name: before_1989, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['before_1989'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95c7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'before_1989'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199c7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[target_feature] == 0, target_feature] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b24fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>before_1989</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515340</th>\n",
       "      <td>2006</td>\n",
       "      <td>51.28467</td>\n",
       "      <td>45.88068</td>\n",
       "      <td>22.19582</td>\n",
       "      <td>-5.53319</td>\n",
       "      <td>-3.61835</td>\n",
       "      <td>-16.36914</td>\n",
       "      <td>2.12652</td>\n",
       "      <td>5.18160</td>\n",
       "      <td>-8.66890</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.75991</td>\n",
       "      <td>-30.92584</td>\n",
       "      <td>26.33968</td>\n",
       "      <td>-5.03390</td>\n",
       "      <td>21.86037</td>\n",
       "      <td>-142.29410</td>\n",
       "      <td>3.42901</td>\n",
       "      <td>-41.14721</td>\n",
       "      <td>-15.46052</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515341</th>\n",
       "      <td>2006</td>\n",
       "      <td>49.87870</td>\n",
       "      <td>37.93125</td>\n",
       "      <td>18.65987</td>\n",
       "      <td>-3.63581</td>\n",
       "      <td>-27.75665</td>\n",
       "      <td>-18.52988</td>\n",
       "      <td>7.76108</td>\n",
       "      <td>3.56109</td>\n",
       "      <td>-2.50351</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.75535</td>\n",
       "      <td>-61.05473</td>\n",
       "      <td>56.65182</td>\n",
       "      <td>15.29965</td>\n",
       "      <td>95.88193</td>\n",
       "      <td>-10.63242</td>\n",
       "      <td>12.96552</td>\n",
       "      <td>92.11633</td>\n",
       "      <td>10.88815</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515342</th>\n",
       "      <td>2006</td>\n",
       "      <td>45.12852</td>\n",
       "      <td>12.65758</td>\n",
       "      <td>-38.72018</td>\n",
       "      <td>8.80882</td>\n",
       "      <td>-29.29985</td>\n",
       "      <td>-2.28706</td>\n",
       "      <td>-18.40424</td>\n",
       "      <td>-22.28726</td>\n",
       "      <td>-4.52429</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.15954</td>\n",
       "      <td>-123.98443</td>\n",
       "      <td>121.26989</td>\n",
       "      <td>10.89629</td>\n",
       "      <td>34.62409</td>\n",
       "      <td>-248.61020</td>\n",
       "      <td>-6.07171</td>\n",
       "      <td>53.96319</td>\n",
       "      <td>-8.09364</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515343</th>\n",
       "      <td>2006</td>\n",
       "      <td>44.16614</td>\n",
       "      <td>32.38368</td>\n",
       "      <td>-3.34971</td>\n",
       "      <td>-2.49165</td>\n",
       "      <td>-19.59278</td>\n",
       "      <td>-18.67098</td>\n",
       "      <td>8.78428</td>\n",
       "      <td>4.02039</td>\n",
       "      <td>-12.01230</td>\n",
       "      <td>...</td>\n",
       "      <td>282.77624</td>\n",
       "      <td>-4.63677</td>\n",
       "      <td>144.00125</td>\n",
       "      <td>21.62652</td>\n",
       "      <td>-29.72432</td>\n",
       "      <td>71.47198</td>\n",
       "      <td>20.32240</td>\n",
       "      <td>14.83107</td>\n",
       "      <td>39.74909</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515344</th>\n",
       "      <td>2005</td>\n",
       "      <td>51.85726</td>\n",
       "      <td>59.11655</td>\n",
       "      <td>26.39436</td>\n",
       "      <td>-5.46030</td>\n",
       "      <td>-20.69012</td>\n",
       "      <td>-19.95528</td>\n",
       "      <td>-6.72771</td>\n",
       "      <td>2.29590</td>\n",
       "      <td>10.31018</td>\n",
       "      <td>...</td>\n",
       "      <td>-69.18291</td>\n",
       "      <td>60.58456</td>\n",
       "      <td>28.64599</td>\n",
       "      <td>-4.39620</td>\n",
       "      <td>-64.56491</td>\n",
       "      <td>-45.61012</td>\n",
       "      <td>-5.51512</td>\n",
       "      <td>32.35602</td>\n",
       "      <td>12.17352</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515345 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
       "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
       "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
       "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
       "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
       "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
       "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
       "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
       "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
       "\n",
       "               7         8         9  ...         82         83         84  \\\n",
       "0      -25.01202 -12.23257   7.83089  ...  -54.40548   58.99367   15.37344   \n",
       "1        8.76630  -0.92019  18.76548  ...  -19.68073   33.04964   42.87836   \n",
       "2       -3.27872  -2.35035  16.07017  ...   26.05866  -50.92779   10.93792   \n",
       "3        5.05097 -10.34124   3.55005  ... -171.70734  -16.96705  -46.67617   \n",
       "4      -12.48207  -9.37636  12.63699  ...  -55.95724   64.92712  -17.72522   \n",
       "...          ...       ...       ...  ...        ...        ...        ...   \n",
       "515340   2.12652   5.18160  -8.66890  ...   -3.75991  -30.92584   26.33968   \n",
       "515341   7.76108   3.56109  -2.50351  ...  -32.75535  -61.05473   56.65182   \n",
       "515342 -18.40424 -22.28726  -4.52429  ...  -71.15954 -123.98443  121.26989   \n",
       "515343   8.78428   4.02039 -12.01230  ...  282.77624   -4.63677  144.00125   \n",
       "515344  -6.72771   2.29590  10.31018  ...  -69.18291   60.58456   28.64599   \n",
       "\n",
       "              85        86         87        88         89        90  \\\n",
       "0        1.11144 -23.08793   68.40795  -1.82223  -27.46348   2.26327   \n",
       "1       -9.90378 -32.22788   70.49388  12.04941   58.43453  26.92061   \n",
       "2       -0.07568  43.20130 -115.00698  -0.05859   39.67068  -0.66345   \n",
       "3      -12.51516  82.58061  -72.08993   9.90558  199.62971  18.85382   \n",
       "4       -1.49237  -7.50035   51.76631   7.88713   55.66926  28.74903   \n",
       "...          ...       ...        ...       ...        ...       ...   \n",
       "515340  -5.03390  21.86037 -142.29410   3.42901  -41.14721 -15.46052   \n",
       "515341  15.29965  95.88193  -10.63242  12.96552   92.11633  10.88815   \n",
       "515342  10.89629  34.62409 -248.61020  -6.07171   53.96319  -8.09364   \n",
       "515343  21.62652 -29.72432   71.47198  20.32240   14.83107  39.74909   \n",
       "515344  -4.39620 -64.56491  -45.61012  -5.51512   32.35602  12.17352   \n",
       "\n",
       "        before_1989  \n",
       "0                -1  \n",
       "1                -1  \n",
       "2                -1  \n",
       "3                -1  \n",
       "4                -1  \n",
       "...             ...  \n",
       "515340           -1  \n",
       "515341           -1  \n",
       "515342           -1  \n",
       "515343           -1  \n",
       "515344           -1  \n",
       "\n",
       "[515345 rows x 92 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458a877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([0, target_feature], axis = 1) # dropping year attribute (determines label we want to classify)\n",
    "y = df[target_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb32644",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:463715], X[463715:]\n",
    "y_train, y_test = y[:463715], y[463715:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d02923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to np arrays to be able to use training functions\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606f5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: choice of evaluation metric (f1, precision, recall, etc.)\n",
    "# Proportion: proportion of test set to predict as 1s, if needed (logistic regression may predict all 0 by default)\n",
    "def my_cross_val_imbalanced(model, metric, proportion, X, y, k=10):\n",
    "    (n, d) = X.shape\n",
    "    validation_metrics = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        val_set = X[round(i*n/k):round((i+1)*n/k), :]\n",
    "        val_labels = y[round(i*n/k):round((i+1)*n/k)]\n",
    "        train_set = np.delete(X, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        train_labels = np.delete(y, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        model.fit(train_set, train_labels)\n",
    "        if proportion == None:\n",
    "            y_preds = model.predict(val_set)\n",
    "        else:\n",
    "            y_preds = model.predict_proportion(val_set, proportion)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        score = 0\n",
    "        for j in range(len(y_preds)):\n",
    "            if val_labels[j] == 1 and y_preds[j] == 1:\n",
    "                tp += 1\n",
    "            elif val_labels[j] == 1:\n",
    "                fn += 1\n",
    "            elif y_preds[j] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        if tp == 0: # to avoid division by zero error for trivial models\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)  \n",
    "        if metric == 'precision':\n",
    "            score = precision\n",
    "        if metric == 'recall':\n",
    "            score = recall\n",
    "        if metric == 'f1':\n",
    "            if precision + recall == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = 2 * precision * recall / (precision + recall)\n",
    "        if metric == 'auprc':\n",
    "            score = sklearn.metrics.average_precision_score(val_labels, y_preds)\n",
    "        validation_metrics[i] = score\n",
    "    return validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064826b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "051f46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWeightedAdaboost:\n",
    "    \n",
    "    def __init__(self, estimator, num_estimators, w1):\n",
    "        \n",
    "        self.estimator=estimator # just declaring the type of estimator! do not change it but make copies\n",
    "        self.num_estimators=num_estimators\n",
    "        self.classifiers = []\n",
    "        self.alphas = []\n",
    "        self.w1 = w1\n",
    "        \n",
    "    def fit(self, X, y): # y labeled in 1 and -1 (make sure to do that before!)\n",
    "        \n",
    "        (n, d) = X.shape\n",
    "        weights = np.array([(1/n) for i in range(n)])\n",
    "        adaboost_predictions = np.zeros(n)\n",
    "        \n",
    "        # Initializing extra weight for ones\n",
    "        one_indices = np.where(y==1)\n",
    "        weights[one_indices] *= self.w1\n",
    "        weights = weights/np.sum(weights)\n",
    "        \n",
    "        for t in range(self.num_estimators):\n",
    "            \n",
    "            #X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=t)\n",
    "            estimator = copy.deepcopy(self.estimator)\n",
    "            \n",
    "            #generating dataset to fit from weight distribution\n",
    "            samples = np.random.choice(np.array(range(n)), size=n, replace=True, p=weights)\n",
    "            X_samp = X[samples]\n",
    "            y_samp = y[samples]\n",
    "            \n",
    "            estimator.fit(X_samp, y_samp)\n",
    "            print(np.argmax(estimator.feature_importances_)) # prints most important column, also add in dataframe to figure out what it is???\n",
    "            predictions = estimator.predict(X) #do you just do the training set here?\n",
    "            \n",
    "            error = 0\n",
    "            for i in range(n):\n",
    "                if predictions[i] != y[i]: # introduce parameter here to add extra class weight for when y = 1?\n",
    "                    error += weights[i] \n",
    "            #error = error / (np.sum(weights))\n",
    "\n",
    "            alpha = (1/2)*np.log((1 - error)/error)\n",
    "            \n",
    "            for i in range(n):\n",
    "                if predictions[i] != y[i]: #same here introduce some weight...\n",
    "                    weights[i] *= np.exp(alpha)\n",
    "                else:\n",
    "                    weights[i] *= np.exp(-alpha)\n",
    "            \n",
    "            #Normalize weights\n",
    "            weights = weights/np.sum(weights)\n",
    "            \n",
    "            self.classifiers.append(estimator)\n",
    "            self.alphas.append(alpha)\n",
    "            \n",
    "            # Add weak classifier weights \n",
    "            #self.classifiers.append(alpha*predictions)\n",
    "        #print(self.classifiers)\n",
    "        #print(self.alphas)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        (n, d) = X.shape\n",
    "        predictions = np.zeros(n)\n",
    "        for t in range(self.num_estimators):\n",
    "            predictions += self.alphas[t] * self.classifiers[t].predict(X)\n",
    "        print(predictions)\n",
    "        return np.sign(predictions)\n",
    "    \n",
    "    def predict_values(self, X):\n",
    "        (n, d) = X.shape\n",
    "        predictions = np.zeros(n)\n",
    "        for t in range(self.num_estimators):\n",
    "            predictions += self.alphas[t] * self.classifiers[t].predict(X)\n",
    "        #print(predictions)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa02a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 1e-05\n",
      "C: 1\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.20201207243460761\n",
      "F1 score for fold 1: 0.22524420609078719\n",
      "F1 score for fold 2: 0.2101028752308098\n",
      "F1 score for fold 3: 0.22360162769846195\n",
      "F1 score for fold 4: 0.20371757336419966\n",
      "F1 score for fold 5: 0.2064295575570311\n",
      "F1 score for fold 6: 0.20394307350829086\n",
      "F1 score for fold 7: 0.20428500249128048\n",
      "F1 score for fold 8: 0.20043202199384694\n",
      "F1 score for fold 9: 0.19704301075268815\n",
      "Mean validation F1 score: 0.2076811021122004\n",
      "Validation F1 score stdev: 0.008991268339634869\n",
      "Eta: 1e-05\n",
      "C: 1\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.16549678695007414\n",
      "F1 score for fold 1: 0.1658905192865302\n",
      "F1 score for fold 2: 0.1602910990856503\n",
      "F1 score for fold 3: 0.17426485099664496\n",
      "F1 score for fold 4: 0.18631216149679958\n",
      "F1 score for fold 5: 0.1763622974963181\n",
      "F1 score for fold 6: 0.1499287410926366\n",
      "F1 score for fold 7: 0.16834666943780524\n",
      "F1 score for fold 8: 0.16430441455267672\n",
      "F1 score for fold 9: 0.17441096404768608\n",
      "Mean validation F1 score: 0.1685608504442822\n",
      "Validation F1 score stdev: 0.009449643547138627\n",
      "Eta: 1e-05\n",
      "C: 1\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.27714646464646464\n",
      "F1 score for fold 1: 0.30985094512448996\n",
      "F1 score for fold 2: 0.28995314940135347\n",
      "F1 score for fold 3: 0.2785330948121646\n",
      "F1 score for fold 4: 0.2841589058352454\n",
      "F1 score for fold 5: 0.29286208088489457\n",
      "F1 score for fold 6: 0.27920504120213285\n",
      "F1 score for fold 7: 0.2599678825418674\n",
      "F1 score for fold 8: 0.3016133206798378\n",
      "F1 score for fold 9: 0.2890178454557354\n",
      "Mean validation F1 score: 0.2862308730584186\n",
      "Validation F1 score stdev: 0.013211000231736606\n",
      "Eta: 1e-05\n",
      "C: 1\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.27974905558553703\n",
      "F1 score for fold 1: 0.31660086482421507\n",
      "F1 score for fold 2: 0.2948688785623761\n",
      "F1 score for fold 3: 0.2791972007658282\n",
      "F1 score for fold 4: 0.2810670569451836\n",
      "F1 score for fold 5: 0.29986413922494665\n",
      "F1 score for fold 6: 0.2919070434470845\n",
      "F1 score for fold 7: 0.2645655533902292\n",
      "F1 score for fold 8: 0.31120839502993625\n",
      "F1 score for fold 9: 0.29027623587801216\n",
      "Mean validation F1 score: 0.2909304423653349\n",
      "Validation F1 score stdev: 0.01490622695356427\n",
      "Eta: 1e-05\n",
      "C: 1\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.27980939414567735\n",
      "F1 score for fold 1: 0.315391927652031\n",
      "F1 score for fold 2: 0.29289698379550083\n",
      "F1 score for fold 3: 0.2766212650265213\n",
      "F1 score for fold 4: 0.2786417693590917\n",
      "F1 score for fold 5: 0.30110417160776876\n",
      "F1 score for fold 6: 0.29564158882214403\n",
      "F1 score for fold 7: 0.2665408465893772\n",
      "F1 score for fold 8: 0.3114141651644591\n",
      "F1 score for fold 9: 0.28988038530750504\n",
      "Mean validation F1 score: 0.2907942497470076\n",
      "Validation F1 score stdev: 0.014908807418748848\n",
      "Eta: 1e-05\n",
      "C: 1\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.2744302643573382\n",
      "F1 score for fold 1: 0.31280509065550904\n",
      "F1 score for fold 2: 0.28538288238049564\n",
      "F1 score for fold 3: 0.2732469169142137\n",
      "F1 score for fold 4: 0.27280976362234016\n",
      "F1 score for fold 5: 0.2941529969507445\n",
      "F1 score for fold 6: 0.29283419596644655\n",
      "F1 score for fold 7: 0.26154193985982366\n",
      "F1 score for fold 8: 0.3050234752132608\n",
      "F1 score for fold 9: 0.28725378406778906\n",
      "Mean validation F1 score: 0.2859481309987961\n",
      "Validation F1 score stdev: 0.01504721883037329\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.2020039002084594\n",
      "F1 score for fold 1: 0.22374312396059867\n",
      "F1 score for fold 2: 0.209236069701186\n",
      "F1 score for fold 3: 0.22540077390823662\n",
      "F1 score for fold 4: 0.20467475495236137\n",
      "F1 score for fold 5: 0.20647291941875826\n",
      "F1 score for fold 6: 0.20072013093289687\n",
      "F1 score for fold 7: 0.20398258511169795\n",
      "F1 score for fold 8: 0.20220588235294118\n",
      "F1 score for fold 9: 0.19673905689736826\n",
      "Mean validation F1 score: 0.20751791974445047\n",
      "Validation F1 score stdev: 0.00910327378775776\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.16807715016728988\n",
      "F1 score for fold 1: 0.17033492822966503\n",
      "F1 score for fold 2: 0.16791493296347665\n",
      "F1 score for fold 3: 0.17766051011433595\n",
      "F1 score for fold 4: 0.19369325392951284\n",
      "F1 score for fold 5: 0.1793343724213808\n",
      "F1 score for fold 6: 0.1556266465939029\n",
      "F1 score for fold 7: 0.16764979819931697\n",
      "F1 score for fold 8: 0.17131616595135907\n",
      "F1 score for fold 9: 0.17727187908191824\n",
      "Mean validation F1 score: 0.17288796376521584\n",
      "Validation F1 score stdev: 0.00947980562125768\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.2732717856338662\n",
      "F1 score for fold 1: 0.3069298683991337\n",
      "F1 score for fold 2: 0.2859741834878281\n",
      "F1 score for fold 3: 0.27591136526090065\n",
      "F1 score for fold 4: 0.2820512820512821\n",
      "F1 score for fold 5: 0.29073482428115016\n",
      "F1 score for fold 6: 0.27923764250187066\n",
      "F1 score for fold 7: 0.25702589577296353\n",
      "F1 score for fold 8: 0.30035396702063366\n",
      "F1 score for fold 9: 0.2842897130860742\n",
      "Mean validation F1 score: 0.28357805274957026\n",
      "Validation F1 score stdev: 0.01332437745025595\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.2769671348697668\n",
      "F1 score for fold 1: 0.3148089474684747\n",
      "F1 score for fold 2: 0.2930525487108837\n",
      "F1 score for fold 3: 0.27807593586452883\n",
      "F1 score for fold 4: 0.2796313736041975\n",
      "F1 score for fold 5: 0.2981528620847203\n",
      "F1 score for fold 6: 0.2892646428689146\n",
      "F1 score for fold 7: 0.26223942418686763\n",
      "F1 score for fold 8: 0.3091963761976146\n",
      "F1 score for fold 9: 0.28820154323023145\n",
      "Mean validation F1 score: 0.28895907890861994\n",
      "Validation F1 score stdev: 0.014976344143077499\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.27917782758432186\n",
      "F1 score for fold 1: 0.31537491312517696\n",
      "F1 score for fold 2: 0.2926273387395983\n",
      "F1 score for fold 3: 0.2769354191736863\n",
      "F1 score for fold 4: 0.2771230549337936\n",
      "F1 score for fold 5: 0.29997091255255576\n",
      "F1 score for fold 6: 0.29467367638468484\n",
      "F1 score for fold 7: 0.26648634027589935\n",
      "F1 score for fold 8: 0.3107258824761705\n",
      "F1 score for fold 9: 0.2890786612210259\n",
      "Mean validation F1 score: 0.2902174026466914\n",
      "Validation F1 score stdev: 0.014861024524967905\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.2761160105464134\n",
      "F1 score for fold 1: 0.31468288444830583\n",
      "F1 score for fold 2: 0.28715645290402914\n",
      "F1 score for fold 3: 0.27482388836541477\n",
      "F1 score for fold 4: 0.27487123881810793\n",
      "F1 score for fold 5: 0.2963981547196593\n",
      "F1 score for fold 6: 0.29420525017226434\n",
      "F1 score for fold 7: 0.26345443072207697\n",
      "F1 score for fold 8: 0.306583141004506\n",
      "F1 score for fold 9: 0.28826944895552936\n",
      "Mean validation F1 score: 0.28765609006563075\n",
      "Validation F1 score stdev: 0.015018766537144053\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.19994570381430704\n",
      "F1 score for fold 1: 0.22063092703696535\n",
      "F1 score for fold 2: 0.20864936793080507\n",
      "F1 score for fold 3: 0.22130861960838963\n",
      "F1 score for fold 4: 0.20296070835639182\n",
      "F1 score for fold 5: 0.20246256239600666\n",
      "F1 score for fold 6: 0.20312397021024187\n",
      "F1 score for fold 7: 0.20054589857779054\n",
      "F1 score for fold 8: 0.19912733042443473\n",
      "F1 score for fold 9: 0.1948342485255237\n",
      "Mean validation F1 score: 0.20535893368808567\n",
      "Validation F1 score stdev: 0.00848327162188375\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.16761308008615625\n",
      "F1 score for fold 1: 0.16851980542043082\n",
      "F1 score for fold 2: 0.16562038404726737\n",
      "F1 score for fold 3: 0.17540758200746415\n",
      "F1 score for fold 4: 0.1907181240840254\n",
      "F1 score for fold 5: 0.17782236002188584\n",
      "F1 score for fold 6: 0.15587101626398422\n",
      "F1 score for fold 7: 0.16864240799917538\n",
      "F1 score for fold 8: 0.16872427983539093\n",
      "F1 score for fold 9: 0.177356385945343\n",
      "Mean validation F1 score: 0.17162954257111235\n",
      "Validation F1 score stdev: 0.008817042289000876\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.2752162028946543\n",
      "F1 score for fold 1: 0.30746924064791414\n",
      "F1 score for fold 2: 0.286722313053954\n",
      "F1 score for fold 3: 0.27400654925214624\n",
      "F1 score for fold 4: 0.2804055562999687\n",
      "F1 score for fold 5: 0.2905133259062473\n",
      "F1 score for fold 6: 0.27613894222377383\n",
      "F1 score for fold 7: 0.25818513451892383\n",
      "F1 score for fold 8: 0.3000898472596586\n",
      "F1 score for fold 9: 0.2865049509231634\n",
      "Mean validation F1 score: 0.28352520629804046\n",
      "Validation F1 score stdev: 0.013347999913112071\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.27980461512548427\n",
      "F1 score for fold 1: 0.31575976164340597\n",
      "F1 score for fold 2: 0.2933747814260735\n",
      "F1 score for fold 3: 0.2799012345679012\n",
      "F1 score for fold 4: 0.2809818399521054\n",
      "F1 score for fold 5: 0.3005358641616631\n",
      "F1 score for fold 6: 0.29087000423438975\n",
      "F1 score for fold 7: 0.2645193759436336\n",
      "F1 score for fold 8: 0.31116259613218783\n",
      "F1 score for fold 9: 0.28929176992304684\n",
      "Mean validation F1 score: 0.2906201843109891\n",
      "Validation F1 score stdev: 0.014723270488050987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 1e-05\n",
      "C: 100\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.27878689147728813\n",
      "F1 score for fold 1: 0.3146337718175769\n",
      "F1 score for fold 2: 0.2920981213606303\n",
      "F1 score for fold 3: 0.2760309895902665\n",
      "F1 score for fold 4: 0.27747400021443125\n",
      "F1 score for fold 5: 0.2998530646515533\n",
      "F1 score for fold 6: 0.2942698706099815\n",
      "F1 score for fold 7: 0.26549100782279095\n",
      "F1 score for fold 8: 0.3102055704645528\n",
      "F1 score for fold 9: 0.2896915370419193\n",
      "Mean validation F1 score: 0.28985348250509907\n",
      "Validation F1 score stdev: 0.014876637662554561\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.27390204435125465\n",
      "F1 score for fold 1: 0.31277350808821924\n",
      "F1 score for fold 2: 0.2859498239828885\n",
      "F1 score for fold 3: 0.2727681380922413\n",
      "F1 score for fold 4: 0.2732710958034986\n",
      "F1 score for fold 5: 0.29443407130790433\n",
      "F1 score for fold 6: 0.29260679568358156\n",
      "F1 score for fold 7: 0.2618242481626764\n",
      "F1 score for fold 8: 0.30487643716135854\n",
      "F1 score for fold 9: 0.28684280996161743\n",
      "Mean validation F1 score: 0.285924897259524\n",
      "Validation F1 score stdev: 0.015021701117731176\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.1902794988296847\n",
      "F1 score for fold 1: 0.20338316286388672\n",
      "F1 score for fold 2: 0.1933297180043384\n",
      "F1 score for fold 3: 0.2054688052003109\n",
      "F1 score for fold 4: 0.18680782893362122\n",
      "F1 score for fold 5: 0.1883678808613403\n",
      "F1 score for fold 6: 0.1833826297391772\n",
      "F1 score for fold 7: 0.18755490483162518\n",
      "F1 score for fold 8: 0.18599959653015938\n",
      "F1 score for fold 9: 0.17971493493079943\n",
      "Mean validation F1 score: 0.19042889607249433\n",
      "Validation F1 score stdev: 0.0078238497281768\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.06488644871474919\n",
      "F1 score for fold 1: 0.06045557594731728\n",
      "F1 score for fold 2: 0.05824111822947\n",
      "F1 score for fold 3: 0.05322338830584707\n",
      "F1 score for fold 4: 0.06795752654590881\n",
      "F1 score for fold 5: 0.06525472238122496\n",
      "F1 score for fold 6: 0.053813757604117926\n",
      "F1 score for fold 7: 0.058394160583941604\n",
      "F1 score for fold 8: 0.05878438331854481\n",
      "F1 score for fold 9: 0.07170474516695957\n",
      "Mean validation F1 score: 0.061271582679808125\n",
      "Validation F1 score stdev: 0.0057200810404124695\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.23853467561521252\n",
      "F1 score for fold 1: 0.2646537180429986\n",
      "F1 score for fold 2: 0.24776855797263958\n",
      "F1 score for fold 3: 0.23260336126672534\n",
      "F1 score for fold 4: 0.23747680890538034\n",
      "F1 score for fold 5: 0.2496765270155713\n",
      "F1 score for fold 6: 0.24172155553550484\n",
      "F1 score for fold 7: 0.2230347349177331\n",
      "F1 score for fold 8: 0.2615528865473054\n",
      "F1 score for fold 9: 0.2471084013269972\n",
      "Mean validation F1 score: 0.24441312271460683\n",
      "Validation F1 score stdev: 0.011983735455117975\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.25646756061221726\n",
      "F1 score for fold 1: 0.28933480802654443\n",
      "F1 score for fold 2: 0.26924085485774224\n",
      "F1 score for fold 3: 0.2525272547076314\n",
      "F1 score for fold 4: 0.25597532767925985\n",
      "F1 score for fold 5: 0.27543035993740217\n",
      "F1 score for fold 6: 0.26815022677586714\n",
      "F1 score for fold 7: 0.24500469357650526\n",
      "F1 score for fold 8: 0.28620333290272576\n",
      "F1 score for fold 9: 0.26758674820943845\n",
      "Mean validation F1 score: 0.2665921167285334\n",
      "Validation F1 score stdev: 0.013672736342763005\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.26651241626086253\n",
      "F1 score for fold 1: 0.30341172573699904\n",
      "F1 score for fold 2: 0.27846972756116106\n",
      "F1 score for fold 3: 0.26194942795994397\n",
      "F1 score for fold 4: 0.2636288362402842\n",
      "F1 score for fold 5: 0.287350311143649\n",
      "F1 score for fold 6: 0.2829872914596517\n",
      "F1 score for fold 7: 0.25578072913910793\n",
      "F1 score for fold 8: 0.29697908597986056\n",
      "F1 score for fold 9: 0.27826086956521734\n",
      "Mean validation F1 score: 0.2775330421046737\n",
      "Validation F1 score stdev: 0.014853224877704624\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.2693007183777394\n",
      "F1 score for fold 1: 0.3082531159277935\n",
      "F1 score for fold 2: 0.2810673330061131\n",
      "F1 score for fold 3: 0.2665198732783607\n",
      "F1 score for fold 4: 0.2679842255564117\n",
      "F1 score for fold 5: 0.2885667274425223\n",
      "F1 score for fold 6: 0.28924240738680607\n",
      "F1 score for fold 7: 0.25694413282477\n",
      "F1 score for fold 8: 0.298749339439845\n",
      "F1 score for fold 9: 0.28104779329733587\n",
      "Mean validation F1 score: 0.2807675666537698\n",
      "Validation F1 score stdev: 0.015089973418276913\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.18977202711028957\n",
      "F1 score for fold 1: 0.20672982685396932\n",
      "F1 score for fold 2: 0.19383200324631408\n",
      "F1 score for fold 3: 0.2055626147112805\n",
      "F1 score for fold 4: 0.1875915974596971\n",
      "F1 score for fold 5: 0.18829209414604706\n",
      "F1 score for fold 6: 0.18388027649151062\n",
      "F1 score for fold 7: 0.18779685787358424\n",
      "F1 score for fold 8: 0.18929819844044096\n",
      "F1 score for fold 9: 0.1836916947173181\n",
      "Mean validation F1 score: 0.19164471910504516\n",
      "Validation F1 score stdev: 0.0077529138497014006\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.08956646021915197\n",
      "F1 score for fold 1: 0.08098591549295774\n",
      "F1 score for fold 2: 0.07984790874524716\n",
      "F1 score for fold 3: 0.08377088305489261\n",
      "F1 score for fold 4: 0.09540932518278797\n",
      "F1 score for fold 5: 0.09384615384615384\n",
      "F1 score for fold 6: 0.07450628366247755\n",
      "F1 score for fold 7: 0.08375919232207404\n",
      "F1 score for fold 8: 0.07656836461126006\n",
      "F1 score for fold 9: 0.09465992130410342\n",
      "Mean validation F1 score: 0.08529204084411063\n",
      "Validation F1 score stdev: 0.007258374869111481\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.23967476670054516\n",
      "F1 score for fold 1: 0.2647235319331333\n",
      "F1 score for fold 2: 0.2500779267043683\n",
      "F1 score for fold 3: 0.23380423352992274\n",
      "F1 score for fold 4: 0.2412873563218391\n",
      "F1 score for fold 5: 0.2528948996729426\n",
      "F1 score for fold 6: 0.24290487859211404\n",
      "F1 score for fold 7: 0.22582744514689476\n",
      "F1 score for fold 8: 0.26241791176340956\n",
      "F1 score for fold 9: 0.2494128074451584\n",
      "Mean validation F1 score: 0.24630257578103273\n",
      "Validation F1 score stdev: 0.011499936471159124\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.2570484284365304\n",
      "F1 score for fold 1: 0.2916837879697822\n",
      "F1 score for fold 2: 0.27112987140473155\n",
      "F1 score for fold 3: 0.2539211809674443\n",
      "F1 score for fold 4: 0.2565114179678357\n",
      "F1 score for fold 5: 0.27577922077922074\n",
      "F1 score for fold 6: 0.2684747530723343\n",
      "F1 score for fold 7: 0.24658266769158788\n",
      "F1 score for fold 8: 0.2875947733505404\n",
      "F1 score for fold 9: 0.2685133766700421\n",
      "Mean validation F1 score: 0.26772394783100495\n",
      "Validation F1 score stdev: 0.013869952882629417\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.26646057056079836\n",
      "F1 score for fold 1: 0.30376040275351895\n",
      "F1 score for fold 2: 0.2790009250693802\n",
      "F1 score for fold 3: 0.26235619940349386\n",
      "F1 score for fold 4: 0.2642676563928783\n",
      "F1 score for fold 5: 0.28724301528729573\n",
      "F1 score for fold 6: 0.2834803211794129\n",
      "F1 score for fold 7: 0.2562380038387716\n",
      "F1 score for fold 8: 0.2968481375358166\n",
      "F1 score for fold 9: 0.2787088600913194\n",
      "Mean validation F1 score: 0.2778364092112686\n",
      "Validation F1 score stdev: 0.014749926343328657\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.26863096724400154\n",
      "F1 score for fold 1: 0.30818885617708\n",
      "F1 score for fold 2: 0.28064077713165336\n",
      "F1 score for fold 3: 0.26607419376556923\n",
      "F1 score for fold 4: 0.26772187676653475\n",
      "F1 score for fold 5: 0.288494002665482\n",
      "F1 score for fold 6: 0.2891849877859205\n",
      "F1 score for fold 7: 0.25694320014334354\n",
      "F1 score for fold 8: 0.2985127089786683\n",
      "F1 score for fold 9: 0.28053290558303257\n",
      "Mean validation F1 score: 0.28049244762412856\n",
      "Validation F1 score stdev: 0.015158050468742398\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.1797186647071174\n",
      "F1 score for fold 1: 0.1967125484431378\n",
      "F1 score for fold 2: 0.18565692925377264\n",
      "F1 score for fold 3: 0.19467109538594843\n",
      "F1 score for fold 4: 0.17989870889507095\n",
      "F1 score for fold 5: 0.18054794520547945\n",
      "F1 score for fold 6: 0.17498973867834178\n",
      "F1 score for fold 7: 0.1821038455783331\n",
      "F1 score for fold 8: 0.17952755905511814\n",
      "F1 score for fold 9: 0.1747110332749562\n",
      "Mean validation F1 score: 0.18285380684772756\n",
      "Validation F1 score stdev: 0.00709382785388544\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.07967749585013043\n",
      "F1 score for fold 1: 0.07598090097571102\n",
      "F1 score for fold 2: 0.07060133630289532\n",
      "F1 score for fold 3: 0.0677642567406347\n",
      "F1 score for fold 4: 0.08700848571770048\n",
      "F1 score for fold 5: 0.08545514151458856\n",
      "F1 score for fold 6: 0.06552227084034556\n",
      "F1 score for fold 7: 0.07328282200968823\n",
      "F1 score for fold 8: 0.07382193383682586\n",
      "F1 score for fold 9: 0.08249496981891348\n",
      "Mean validation F1 score: 0.07616096136074338\n",
      "Validation F1 score stdev: 0.006963614149593075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.0001\n",
      "C: 100\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.23726622027245442\n",
      "F1 score for fold 1: 0.2662045950455654\n",
      "F1 score for fold 2: 0.24960056808095152\n",
      "F1 score for fold 3: 0.2331187508560471\n",
      "F1 score for fold 4: 0.2400697984111677\n",
      "F1 score for fold 5: 0.2498676081200353\n",
      "F1 score for fold 6: 0.24173175630439636\n",
      "F1 score for fold 7: 0.22397696130800315\n",
      "F1 score for fold 8: 0.26121908127208476\n",
      "F1 score for fold 9: 0.24636844209497577\n",
      "Mean validation F1 score: 0.24494237817656814\n",
      "Validation F1 score stdev: 0.011988843169201128\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.25669650454375276\n",
      "F1 score for fold 1: 0.29153405474220245\n",
      "F1 score for fold 2: 0.2701526491048762\n",
      "F1 score for fold 3: 0.25396403284246916\n",
      "F1 score for fold 4: 0.2569531065637326\n",
      "F1 score for fold 5: 0.27618268128990014\n",
      "F1 score for fold 6: 0.2696378097679616\n",
      "F1 score for fold 7: 0.24580930216870717\n",
      "F1 score for fold 8: 0.2891126535666569\n",
      "F1 score for fold 9: 0.2682277107245042\n",
      "Mean validation F1 score: 0.2678270505314764\n",
      "Validation F1 score stdev: 0.01418752801134713\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.2663496488207483\n",
      "F1 score for fold 1: 0.30372151964207206\n",
      "F1 score for fold 2: 0.27863612147043154\n",
      "F1 score for fold 3: 0.26214373892974074\n",
      "F1 score for fold 4: 0.26405576198096065\n",
      "F1 score for fold 5: 0.2877174346036383\n",
      "F1 score for fold 6: 0.2829783283376217\n",
      "F1 score for fold 7: 0.25688566438264143\n",
      "F1 score for fold 8: 0.2965142241718984\n",
      "F1 score for fold 9: 0.27864057015211147\n",
      "Mean validation F1 score: 0.2777643012491865\n",
      "Validation F1 score stdev: 0.014667151869240731\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.26893965223320826\n",
      "F1 score for fold 1: 0.307970856419877\n",
      "F1 score for fold 2: 0.28106106240099954\n",
      "F1 score for fold 3: 0.2660797700323392\n",
      "F1 score for fold 4: 0.2679898495559181\n",
      "F1 score for fold 5: 0.2890006229420664\n",
      "F1 score for fold 6: 0.2889309155634664\n",
      "F1 score for fold 7: 0.2569769787720517\n",
      "F1 score for fold 8: 0.299159589915079\n",
      "F1 score for fold 9: 0.2808840912639115\n",
      "Mean validation F1 score: 0.2806993389098917\n",
      "Validation F1 score stdev: 0.015158018332296009\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.14365626266720713\n",
      "F1 score for fold 1: 0.1407895733878912\n",
      "F1 score for fold 2: 0.14078478002378123\n",
      "F1 score for fold 3: 0.15542763157894735\n",
      "F1 score for fold 4: 0.1343867079328881\n",
      "F1 score for fold 5: 0.15072958708475628\n",
      "F1 score for fold 6: 0.12836603383906586\n",
      "F1 score for fold 7: 0.1338328831576196\n",
      "F1 score for fold 8: 0.1427567356501367\n",
      "F1 score for fold 9: 0.1356175298804781\n",
      "Mean validation F1 score: 0.14063477252027715\n",
      "Validation F1 score stdev: 0.00771628951334505\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.016465422612513724\n",
      "F1 score for fold 1: 0.012269938650306749\n",
      "F1 score for fold 2: 0.011550308008213552\n",
      "F1 score for fold 3: 0.012025143481825635\n",
      "F1 score for fold 4: 0.013214039917412249\n",
      "F1 score for fold 5: 0.012112036336109008\n",
      "F1 score for fold 6: 0.009466547268773187\n",
      "F1 score for fold 7: 0.0157615704255624\n",
      "F1 score for fold 8: 0.012901655306718598\n",
      "F1 score for fold 9: 0.014341590612777054\n",
      "Mean validation F1 score: 0.013010825262021217\n",
      "Validation F1 score stdev: 0.001958272522442056\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.18583728354454873\n",
      "F1 score for fold 1: 0.2116591928251121\n",
      "F1 score for fold 2: 0.19018290406868235\n",
      "F1 score for fold 3: 0.17761023096010692\n",
      "F1 score for fold 4: 0.1787933836886892\n",
      "F1 score for fold 5: 0.19720260351751834\n",
      "F1 score for fold 6: 0.19576670671965984\n",
      "F1 score for fold 7: 0.17674862735736452\n",
      "F1 score for fold 8: 0.20995789859051803\n",
      "F1 score for fold 9: 0.1956270512642722\n",
      "Mean validation F1 score: 0.1919385882536472\n",
      "Validation F1 score stdev: 0.011910954540575364\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.2303782788143398\n",
      "F1 score for fold 1: 0.2632420675537359\n",
      "F1 score for fold 2: 0.23813792874456285\n",
      "F1 score for fold 3: 0.2203287890938252\n",
      "F1 score for fold 4: 0.2204649905379832\n",
      "F1 score for fold 5: 0.24164338537387017\n",
      "F1 score for fold 6: 0.24155193992490617\n",
      "F1 score for fold 7: 0.21972451055598174\n",
      "F1 score for fold 8: 0.25708272624251255\n",
      "F1 score for fold 9: 0.23870670354273674\n",
      "Mean validation F1 score: 0.2371261320384454\n",
      "Validation F1 score stdev: 0.014254533845461905\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.2498630437164457\n",
      "F1 score for fold 1: 0.2870922574288233\n",
      "F1 score for fold 2: 0.26236973934474533\n",
      "F1 score for fold 3: 0.24352833539637264\n",
      "F1 score for fold 4: 0.2423599782490484\n",
      "F1 score for fold 5: 0.26649273201639956\n",
      "F1 score for fold 6: 0.2682531672465539\n",
      "F1 score for fold 7: 0.24139770605494798\n",
      "F1 score for fold 8: 0.2784107278489301\n",
      "F1 score for fold 9: 0.26141455088542775\n",
      "Mean validation F1 score: 0.2601182238187695\n",
      "Validation F1 score stdev: 0.014906313515121283\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.26116667420916867\n",
      "F1 score for fold 1: 0.3029577035326916\n",
      "F1 score for fold 2: 0.2762401390560235\n",
      "F1 score for fold 3: 0.25758186623661694\n",
      "F1 score for fold 4: 0.2582346973283733\n",
      "F1 score for fold 5: 0.27946396876109336\n",
      "F1 score for fold 6: 0.28318740995234404\n",
      "F1 score for fold 7: 0.2520056001244472\n",
      "F1 score for fold 8: 0.2910660298821365\n",
      "F1 score for fold 9: 0.2744408590185824\n",
      "Mean validation F1 score: 0.2736344948101478\n",
      "Validation F1 score stdev: 0.015538615854206699\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.14018922254216373\n",
      "F1 score for fold 1: 0.1391104294478528\n",
      "F1 score for fold 2: 0.13763164241498513\n",
      "F1 score for fold 3: 0.15375626043405677\n",
      "F1 score for fold 4: 0.1316246382802811\n",
      "F1 score for fold 5: 0.1498350871682111\n",
      "F1 score for fold 6: 0.12550346383115837\n",
      "F1 score for fold 7: 0.13125165387668694\n",
      "F1 score for fold 8: 0.14130693069306932\n",
      "F1 score for fold 9: 0.13333333333333333\n",
      "Mean validation F1 score: 0.13835426620217986\n",
      "Validation F1 score stdev: 0.008174948434064418\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.01780821917808219\n",
      "F1 score for fold 1: 0.012513280604415063\n",
      "F1 score for fold 2: 0.011534025374855825\n",
      "F1 score for fold 3: 0.011473842371260756\n",
      "F1 score for fold 4: 0.0132013201320132\n",
      "F1 score for fold 5: 0.011337868480725623\n",
      "F1 score for fold 6: 0.010465858328015316\n",
      "F1 score for fold 7: 0.016323024054982815\n",
      "F1 score for fold 8: 0.0121654501216545\n",
      "F1 score for fold 9: 0.01405883884405103\n",
      "Mean validation F1 score: 0.013088172749005634\n",
      "Validation F1 score stdev: 0.0022325220910640545\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.18496102261172712\n",
      "F1 score for fold 1: 0.21177843612532543\n",
      "F1 score for fold 2: 0.18932424228272543\n",
      "F1 score for fold 3: 0.17741088896311494\n",
      "F1 score for fold 4: 0.17790530846484937\n",
      "F1 score for fold 5: 0.1964063005219641\n",
      "F1 score for fold 6: 0.19503628044553312\n",
      "F1 score for fold 7: 0.1768746422438466\n",
      "F1 score for fold 8: 0.20883755042170885\n",
      "F1 score for fold 9: 0.19437765859071574\n",
      "Mean validation F1 score: 0.19129123306715107\n",
      "Validation F1 score stdev: 0.011836129846057648\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.23002887391722807\n",
      "F1 score for fold 1: 0.2622432859399684\n",
      "F1 score for fold 2: 0.23727452227167764\n",
      "F1 score for fold 3: 0.21919838329403837\n",
      "F1 score for fold 4: 0.21998978375617229\n",
      "F1 score for fold 5: 0.24077695647850442\n",
      "F1 score for fold 6: 0.2406299840510367\n",
      "F1 score for fold 7: 0.21906298312831887\n",
      "F1 score for fold 8: 0.2562628605023353\n",
      "F1 score for fold 9: 0.2378091286307054\n",
      "Mean validation F1 score: 0.23632767619699857\n",
      "Validation F1 score stdev: 0.014172196407096582\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.25028615032430374\n",
      "F1 score for fold 1: 0.28746287927695285\n",
      "F1 score for fold 2: 0.2628757368042035\n",
      "F1 score for fold 3: 0.24404873477038425\n",
      "F1 score for fold 4: 0.24283743202662122\n",
      "F1 score for fold 5: 0.2672039020252359\n",
      "F1 score for fold 6: 0.26900894227207794\n",
      "F1 score for fold 7: 0.24199156028557023\n",
      "F1 score for fold 8: 0.27888300784814746\n",
      "F1 score for fold 9: 0.2622837737450918\n",
      "Mean validation F1 score: 0.26068821193785896\n",
      "Validation F1 score stdev: 0.014900481452399092\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.2605568075758611\n",
      "F1 score for fold 1: 0.30204117351068716\n",
      "F1 score for fold 2: 0.2751360673729478\n",
      "F1 score for fold 3: 0.2568373663477772\n",
      "F1 score for fold 4: 0.2569822270583968\n",
      "F1 score for fold 5: 0.2783663819061852\n",
      "F1 score for fold 6: 0.28221241698979366\n",
      "F1 score for fold 7: 0.2512284463503976\n",
      "F1 score for fold 8: 0.2896706586826347\n",
      "F1 score for fold 9: 0.2731464899713467\n",
      "Mean validation F1 score: 0.2726178035766028\n",
      "Validation F1 score stdev: 0.015442552140196571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.001\n",
      "C: 100\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.13819261213720319\n",
      "F1 score for fold 1: 0.13588339631371943\n",
      "F1 score for fold 2: 0.1353528843055108\n",
      "F1 score for fold 3: 0.15016778523489932\n",
      "F1 score for fold 4: 0.12811151676070362\n",
      "F1 score for fold 5: 0.14293602587362939\n",
      "F1 score for fold 6: 0.12179539021431458\n",
      "F1 score for fold 7: 0.12818464269862406\n",
      "F1 score for fold 8: 0.13689058281721456\n",
      "F1 score for fold 9: 0.1309659553673237\n",
      "Mean validation F1 score: 0.13484807917231428\n",
      "Validation F1 score stdev: 0.007694276706889087\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.015136920324755745\n",
      "F1 score for fold 1: 0.011104548139397518\n",
      "F1 score for fold 2: 0.008997429305912597\n",
      "F1 score for fold 3: 0.011219045012997673\n",
      "F1 score for fold 4: 0.012967305835287626\n",
      "F1 score for fold 5: 0.011630847029077115\n",
      "F1 score for fold 6: 0.007930416986441546\n",
      "F1 score for fold 7: 0.015203671830177852\n",
      "F1 score for fold 8: 0.009764433052605883\n",
      "F1 score for fold 9: 0.014349073832507175\n",
      "Mean validation F1 score: 0.011830369134916073\n",
      "Validation F1 score stdev: 0.002416455241895441\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.1875329910264408\n",
      "F1 score for fold 1: 0.21359309669958193\n",
      "F1 score for fold 2: 0.19226856561546285\n",
      "F1 score for fold 3: 0.17949808592088473\n",
      "F1 score for fold 4: 0.1803557910673732\n",
      "F1 score for fold 5: 0.19878277582025353\n",
      "F1 score for fold 6: 0.19614133174464965\n",
      "F1 score for fold 7: 0.1787957497048406\n",
      "F1 score for fold 8: 0.21132897603485837\n",
      "F1 score for fold 9: 0.19716247139588103\n",
      "Mean validation F1 score: 0.1935459835030227\n",
      "Validation F1 score stdev: 0.011798484042040832\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.23100448552399075\n",
      "F1 score for fold 1: 0.2632869471069643\n",
      "F1 score for fold 2: 0.2388326384752829\n",
      "F1 score for fold 3: 0.22034293324454804\n",
      "F1 score for fold 4: 0.21982976146418598\n",
      "F1 score for fold 5: 0.24167540639748294\n",
      "F1 score for fold 6: 0.24268112370625927\n",
      "F1 score for fold 7: 0.22004516471838467\n",
      "F1 score for fold 8: 0.2574334140435835\n",
      "F1 score for fold 9: 0.2397168234407263\n",
      "Mean validation F1 score: 0.23748486981214087\n",
      "Validation F1 score stdev: 0.014376156971879838\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.2498641156647462\n",
      "F1 score for fold 1: 0.28766347441046236\n",
      "F1 score for fold 2: 0.26262894820801874\n",
      "F1 score for fold 3: 0.24362122344400558\n",
      "F1 score for fold 4: 0.2424242424242424\n",
      "F1 score for fold 5: 0.2668992125151948\n",
      "F1 score for fold 6: 0.26878262244951756\n",
      "F1 score for fold 7: 0.24151802254803367\n",
      "F1 score for fold 8: 0.2788859126726133\n",
      "F1 score for fold 9: 0.261465583401699\n",
      "Mean validation F1 score: 0.2603753357738533\n",
      "Validation F1 score stdev: 0.015085823994527245\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.26140719109326516\n",
      "F1 score for fold 1: 0.3027897183963506\n",
      "F1 score for fold 2: 0.275852776717343\n",
      "F1 score for fold 3: 0.257137054004649\n",
      "F1 score for fold 4: 0.2580176251476333\n",
      "F1 score for fold 5: 0.2794068308394283\n",
      "F1 score for fold 6: 0.2828120465251267\n",
      "F1 score for fold 7: 0.2522373366744227\n",
      "F1 score for fold 8: 0.2908208296557811\n",
      "F1 score for fold 9: 0.2741433021806854\n",
      "Mean validation F1 score: 0.2734624711234685\n",
      "Validation F1 score stdev: 0.015463588614008988\n",
      "Best eta value: 1e-05\n",
      "Best C value: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MySVM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest eta value: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(best_eta))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest C value: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(best_c))\n\u001b[1;32m---> 38\u001b[0m best_svm \u001b[38;5;241m=\u001b[39m \u001b[43mMySVM\u001b[49m(num_features, \u001b[38;5;241m100000\u001b[39m, best_eta, best_c)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# fit model using all training data\u001b[39;00m\n\u001b[0;32m     41\u001b[0m best_svm\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MySVM' is not defined"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "eta_vals = [0.00001, 0.0001, 0.001]\n",
    "C_vals = [1, 10, 100]\n",
    "weights = [1, 2, 5, 10, 20, 50]\n",
    "best_score = 0\n",
    "best_eta, best_c, best_weight = (0, 0, 0)\n",
    "\n",
    "(_, num_features) = X_train.shape\n",
    "proportion = len(y_train[y_train == 1])/len(y_train)\n",
    "\n",
    "for eta_val in eta_vals:\n",
    "    for c_val in C_vals:\n",
    "        for w in weights:\n",
    "\n",
    "            # instantiate svm object\n",
    "            svm = MyWeightedSVM(num_features, 100000, eta_val, c_val, w)\n",
    "\n",
    "            # call to your CV function to compute error rates for each fold\n",
    "            #cv_scores = my_cross_val_imbalanced(svm, 'f1', proportion, X_train, y_train, k=10)\n",
    "            cv_scores = my_cross_val_imbalanced(svm, 'f1', None, X_train, y_train, k=10)\n",
    "\n",
    "            # print error rates from CV\n",
    "            print(\"Eta: \" + str(eta_val))\n",
    "            print(\"C: \" + str(c_val))\n",
    "            print(\"Weight: \" + str(w))\n",
    "            for i in range(10):\n",
    "                print(\"F1 score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "            mean_score = sum(cv_scores)/len(cv_scores)\n",
    "            print(\"Mean validation F1 score: \" + str(mean_score))\n",
    "            print(\"Validation F1 score stdev: \" + str(np.std(cv_scores)))\n",
    "            if mean_score >= best_score:\n",
    "                best_score = mean_score\n",
    "                best_eta, best_c, best_weight = (eta_val, c_val, w)\n",
    "\n",
    "# instantiate svm object for best value of eta and C\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "print(\"Best C value: \" + str(best_c))\n",
    "best_svm = MyWeightedSVM(num_features, 100000, best_eta, best_c, best_weight)\n",
    "\n",
    "# fit model using all training data\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "#y_preds = best_svm.predict_proportion(X_test, proportion)\n",
    "y_preds = best_svm.predict(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_preds)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e17fff76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "19\n",
      "56\n",
      "40\n",
      "40\n",
      "38\n",
      "73\n",
      "0\n",
      "0\n",
      "62\n",
      "62\n",
      "1\n",
      "39\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "77\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "27\n",
      "0\n",
      "5\n",
      "5\n",
      "89\n",
      "19\n",
      "11\n",
      "2\n",
      "1\n",
      "0\n",
      "35\n",
      "35\n",
      "83\n",
      "0\n",
      "5\n",
      "22\n",
      "82\n",
      "87\n",
      "30\n",
      "35\n",
      "[-1.2781628  -1.09902983 -1.47393076 ... -1.58742231 -1.27099472\n",
      " -1.71309036]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "56\n",
      "19\n",
      "40\n",
      "73\n",
      "0\n",
      "0\n",
      "19\n",
      "19\n",
      "62\n",
      "62\n",
      "38\n",
      "1\n",
      "0\n",
      "39\n",
      "39\n",
      "84\n",
      "5\n",
      "38\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "77\n",
      "84\n",
      "0\n",
      "0\n",
      "27\n",
      "27\n",
      "35\n",
      "89\n",
      "35\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "19\n",
      "19\n",
      "11\n",
      "83\n",
      "68\n",
      "30\n",
      "5\n",
      "22\n",
      "[-1.82933222 -1.42492116 -1.66112943 ... -0.14927004 -0.67178517\n",
      " -0.30430882]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "0\n",
      "0\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "2\n",
      "0\n",
      "2\n",
      "27\n",
      "27\n",
      "1\n",
      "0\n",
      "2\n",
      "38\n",
      "0\n",
      "22\n",
      "35\n",
      "35\n",
      "89\n",
      "0\n",
      "0\n",
      "0\n",
      "11\n",
      "45\n",
      "0\n",
      "13\n",
      "84\n",
      "0\n",
      "0\n",
      "57\n",
      "[ 0.37311424  0.08939389  0.07618956 ... -1.96438545 -1.62690732\n",
      " -1.70923001]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "19\n",
      "19\n",
      "62\n",
      "62\n",
      "0\n",
      "13\n",
      "73\n",
      "0\n",
      "40\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "39\n",
      "84\n",
      "77\n",
      "0\n",
      "1\n",
      "27\n",
      "48\n",
      "35\n",
      "35\n",
      "89\n",
      "0\n",
      "84\n",
      "0\n",
      "38\n",
      "11\n",
      "83\n",
      "0\n",
      "0\n",
      "38\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "39\n",
      "68\n",
      "45\n",
      "[-1.69321844 -1.67999812 -1.85362029 ... -0.67706805 -0.69277823\n",
      " -1.07845719]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "19\n",
      "56\n",
      "56\n",
      "62\n",
      "0\n",
      "13\n",
      "0\n",
      "40\n",
      "73\n",
      "73\n",
      "1\n",
      "0\n",
      "0\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "2\n",
      "2\n",
      "77\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "1\n",
      "27\n",
      "27\n",
      "0\n",
      "0\n",
      "84\n",
      "35\n",
      "0\n",
      "89\n",
      "19\n",
      "5\n",
      "19\n",
      "19\n",
      "35\n",
      "35\n",
      "83\n",
      "68\n",
      "57\n",
      "57\n",
      "0\n",
      "38\n",
      "[-1.17182063 -0.99287512 -1.18828077 ... -1.06204661 -0.80225366\n",
      " -0.62361972]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "19\n",
      "56\n",
      "56\n",
      "62\n",
      "73\n",
      "0\n",
      "40\n",
      "73\n",
      "1\n",
      "0\n",
      "0\n",
      "39\n",
      "2\n",
      "0\n",
      "77\n",
      "84\n",
      "84\n",
      "35\n",
      "27\n",
      "35\n",
      "0\n",
      "1\n",
      "19\n",
      "19\n",
      "35\n",
      "35\n",
      "0\n",
      "0\n",
      "89\n",
      "22\n",
      "38\n",
      "22\n",
      "0\n",
      "0\n",
      "82\n",
      "2\n",
      "0\n",
      "87\n",
      "1\n",
      "68\n",
      "49\n",
      "11\n",
      "[-0.74929388 -1.05009424 -1.03850307 ... -0.18164627 -0.9910516\n",
      " -1.06179752]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "77\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "38\n",
      "27\n",
      "19\n",
      "19\n",
      "89\n",
      "35\n",
      "35\n",
      "22\n",
      "0\n",
      "0\n",
      "56\n",
      "84\n",
      "5\n",
      "83\n",
      "5\n",
      "5\n",
      "19\n",
      "0\n",
      "68\n",
      "57\n",
      "1\n",
      "[-0.59179722 -0.60629219 -0.8014339  ... -1.04469967 -1.27178526\n",
      " -0.75960124]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "56\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "62\n",
      "38\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "39\n",
      "39\n",
      "84\n",
      "77\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "27\n",
      "27\n",
      "19\n",
      "19\n",
      "0\n",
      "0\n",
      "89\n",
      "2\n",
      "68\n",
      "5\n",
      "48\n",
      "5\n",
      "35\n",
      "35\n",
      "84\n",
      "84\n",
      "35\n",
      "2\n",
      "1\n",
      "2\n",
      "[-0.78513365 -0.93802427 -0.50126628 ... -1.20262566 -1.06082348\n",
      " -1.30116283]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "0\n",
      "2\n",
      "2\n",
      "39\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "77\n",
      "1\n",
      "27\n",
      "0\n",
      "19\n",
      "19\n",
      "0\n",
      "5\n",
      "38\n",
      "0\n",
      "38\n",
      "35\n",
      "89\n",
      "35\n",
      "83\n",
      "0\n",
      "11\n",
      "1\n",
      "0\n",
      "0\n",
      "68\n",
      "84\n",
      "5\n",
      "[-0.15846471 -0.27625071 -0.38243565 ... -1.22760637 -1.24588661\n",
      " -1.19738711]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "73\n",
      "73\n",
      "1\n",
      "38\n",
      "39\n",
      "5\n",
      "84\n",
      "19\n",
      "84\n",
      "0\n",
      "77\n",
      "0\n",
      "0\n",
      "1\n",
      "38\n",
      "84\n",
      "27\n",
      "27\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "83\n",
      "89\n",
      "89\n",
      "35\n",
      "35\n",
      "68\n",
      "1\n",
      "19\n",
      "0\n",
      "0\n",
      "0\n",
      "45\n",
      "35\n",
      "[-1.02315858 -1.08268931 -0.14308694 ... -0.09986013 -1.62063608\n",
      " -0.80865497]\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.17844297482306534\n",
      "F1 score for fold 1: 0.20907297830374755\n",
      "F1 score for fold 2: 0.1942390464628628\n",
      "F1 score for fold 3: 0.19442194190487208\n",
      "F1 score for fold 4: 0.19110224204718865\n",
      "F1 score for fold 5: 0.19752021563342317\n",
      "F1 score for fold 6: 0.1866081229418222\n",
      "F1 score for fold 7: 0.19932268988872762\n",
      "F1 score for fold 8: 0.2001456361177572\n",
      "F1 score for fold 9: 0.19359145527369828\n",
      "Mean validation F1 score: 0.1944467303397165\n",
      "Validation F1 score stdev: 0.007804811022428494\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "62\n",
      "56\n",
      "62\n",
      "62\n",
      "0\n",
      "19\n",
      "40\n",
      "1\n",
      "73\n",
      "0\n",
      "40\n",
      "0\n",
      "39\n",
      "39\n",
      "2\n",
      "2\n",
      "0\n",
      "38\n",
      "84\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "19\n",
      "1\n",
      "77\n",
      "0\n",
      "84\n",
      "27\n",
      "19\n",
      "27\n",
      "27\n",
      "30\n",
      "35\n",
      "5\n",
      "22\n",
      "89\n",
      "89\n",
      "35\n",
      "82\n",
      "35\n",
      "1\n",
      "[-0.74819421 -0.66002726 -1.01770225 ... -1.12012869 -0.97542929\n",
      " -1.09144128]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "73\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "77\n",
      "35\n",
      "27\n",
      "38\n",
      "35\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "38\n",
      "19\n",
      "38\n",
      "38\n",
      "89\n",
      "0\n",
      "0\n",
      "22\n",
      "5\n",
      "35\n",
      "1\n",
      "83\n",
      "5\n",
      "68\n",
      "0\n",
      "84\n",
      "[-1.16397554 -0.90991537 -1.09680611 ... -0.0355578  -0.57797522\n",
      "  0.07888133]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "0\n",
      "19\n",
      "40\n",
      "40\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "38\n",
      "38\n",
      "2\n",
      "84\n",
      "77\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "39\n",
      "19\n",
      "5\n",
      "27\n",
      "5\n",
      "5\n",
      "89\n",
      "0\n",
      "0\n",
      "19\n",
      "22\n",
      "5\n",
      "19\n",
      "35\n",
      "45\n",
      "45\n",
      "0\n",
      "0\n",
      "1\n",
      "[ 0.62935286  0.37186221  0.32555193 ... -1.53457281 -0.92510064\n",
      " -1.24376369]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "19\n",
      "56\n",
      "62\n",
      "13\n",
      "0\n",
      "56\n",
      "56\n",
      "13\n",
      "0\n",
      "0\n",
      "73\n",
      "40\n",
      "40\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "77\n",
      "84\n",
      "1\n",
      "0\n",
      "0\n",
      "27\n",
      "27\n",
      "19\n",
      "38\n",
      "39\n",
      "39\n",
      "19\n",
      "19\n",
      "2\n",
      "0\n",
      "11\n",
      "2\n",
      "2\n",
      "1\n",
      "35\n",
      "57\n",
      "0\n",
      "1\n",
      "0\n",
      "83\n",
      "35\n",
      "89\n",
      "89\n",
      "[-1.37689389 -1.31588489 -1.55441142 ... -0.61208325 -0.36320556\n",
      " -1.08677469]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "19\n",
      "19\n",
      "40\n",
      "73\n",
      "73\n",
      "62\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "77\n",
      "84\n",
      "84\n",
      "5\n",
      "5\n",
      "0\n",
      "35\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "38\n",
      "27\n",
      "0\n",
      "0\n",
      "1\n",
      "89\n",
      "89\n",
      "30\n",
      "39\n",
      "5\n",
      "22\n",
      "1\n",
      "0\n",
      "0\n",
      "45\n",
      "39\n",
      "27\n",
      "35\n",
      "83\n",
      "[-0.6003252  -0.84362292 -0.94822276 ... -0.78552801 -0.2115869\n",
      " -0.224862  ]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "73\n",
      "1\n",
      "73\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "77\n",
      "2\n",
      "19\n",
      "0\n",
      "0\n",
      "38\n",
      "1\n",
      "0\n",
      "2\n",
      "27\n",
      "27\n",
      "89\n",
      "35\n",
      "5\n",
      "89\n",
      "5\n",
      "5\n",
      "19\n",
      "11\n",
      "45\n",
      "35\n",
      "2\n",
      "82\n",
      "87\n",
      "0\n",
      "0\n",
      "1\n",
      "[-0.4682144  -0.39926056 -0.43883125 ...  0.12876066  0.04783741\n",
      " -0.65705945]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "56\n",
      "19\n",
      "19\n",
      "38\n",
      "40\n",
      "0\n",
      "73\n",
      "38\n",
      "38\n",
      "62\n",
      "0\n",
      "1\n",
      "62\n",
      "0\n",
      "0\n",
      "84\n",
      "39\n",
      "84\n",
      "2\n",
      "84\n",
      "84\n",
      "77\n",
      "35\n",
      "35\n",
      "89\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "27\n",
      "5\n",
      "35\n",
      "19\n",
      "48\n",
      "5\n",
      "22\n",
      "1\n",
      "5\n",
      "84\n",
      "11\n",
      "30\n",
      "26\n",
      "83\n",
      "[-0.27922396 -0.22711866 -0.37003885 ... -0.46547237 -1.09001842\n",
      " -0.52377347]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "62\n",
      "0\n",
      "19\n",
      "56\n",
      "40\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "39\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "38\n",
      "38\n",
      "1\n",
      "0\n",
      "77\n",
      "27\n",
      "0\n",
      "0\n",
      "35\n",
      "35\n",
      "89\n",
      "83\n",
      "0\n",
      "0\n",
      "1\n",
      "35\n",
      "35\n",
      "84\n",
      "57\n",
      "68\n",
      "68\n",
      "45\n",
      "0\n",
      "2\n",
      "70\n",
      "[-0.50359603 -0.86484221 -0.28553971 ... -0.92682031 -0.67496908\n",
      " -0.92314769]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "73\n",
      "40\n",
      "40\n",
      "0\n",
      "19\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "39\n",
      "2\n",
      "2\n",
      "84\n",
      "5\n",
      "84\n",
      "77\n",
      "1\n",
      "0\n",
      "84\n",
      "0\n",
      "27\n",
      "38\n",
      "38\n",
      "19\n",
      "19\n",
      "1\n",
      "89\n",
      "48\n",
      "5\n",
      "22\n",
      "89\n",
      "35\n",
      "89\n",
      "0\n",
      "0\n",
      "26\n",
      "30\n",
      "11\n",
      "45\n",
      "[ 0.23774615 -0.09327764 -0.15201733 ... -0.97202115 -0.83114959\n",
      " -0.716777  ]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "40\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "39\n",
      "77\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "1\n",
      "35\n",
      "83\n",
      "27\n",
      "89\n",
      "38\n",
      "0\n",
      "45\n",
      "68\n",
      "0\n",
      "0\n",
      "27\n",
      "27\n",
      "1\n",
      "0\n",
      "5\n",
      "5\n",
      "19\n",
      "0\n",
      "19\n",
      "19\n",
      "49\n",
      "49\n",
      "[-0.50192758 -0.8168233   0.1290911  ...  0.21436161 -1.37387252\n",
      " -0.57629238]\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.35935174466677694\n",
      "F1 score for fold 1: 0.39820770398207705\n",
      "F1 score for fold 2: 0.3834432924913604\n",
      "F1 score for fold 3: 0.3672034660344969\n",
      "F1 score for fold 4: 0.373903600367555\n",
      "F1 score for fold 5: 0.3780724199426223\n",
      "F1 score for fold 6: 0.3650793650793651\n",
      "F1 score for fold 7: 0.3805569970593323\n",
      "F1 score for fold 8: 0.39705547544964714\n",
      "F1 score for fold 9: 0.3792659368963297\n",
      "Mean validation F1 score: 0.3782140001969563\n",
      "Validation F1 score stdev: 0.012069904147590682\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "40\n",
      "40\n",
      "0\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "35\n",
      "0\n",
      "39\n",
      "0\n",
      "0\n",
      "38\n",
      "84\n",
      "77\n",
      "1\n",
      "0\n",
      "38\n",
      "0\n",
      "0\n",
      "27\n",
      "27\n",
      "89\n",
      "0\n",
      "0\n",
      "19\n",
      "19\n",
      "11\n",
      "83\n",
      "84\n",
      "84\n",
      "68\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "45\n",
      "0\n",
      "[-0.35820479 -0.27236398 -0.6104585  ... -0.56371398 -0.6708747\n",
      " -0.69891336]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "0\n",
      "19\n",
      "40\n",
      "40\n",
      "73\n",
      "38\n",
      "38\n",
      "62\n",
      "62\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "77\n",
      "84\n",
      "84\n",
      "0\n",
      "27\n",
      "1\n",
      "19\n",
      "19\n",
      "39\n",
      "0\n",
      "0\n",
      "48\n",
      "0\n",
      "0\n",
      "89\n",
      "35\n",
      "35\n",
      "1\n",
      "68\n",
      "0\n",
      "0\n",
      "83\n",
      "1\n",
      "84\n",
      "1\n",
      "0\n",
      "35\n",
      "35\n",
      "[-1.09493342 -0.3910687  -0.62462914 ...  0.26039132  0.41584677\n",
      "  0.49316251]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "19\n",
      "56\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "73\n",
      "73\n",
      "1\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "77\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "27\n",
      "0\n",
      "38\n",
      "19\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "35\n",
      "35\n",
      "89\n",
      "2\n",
      "35\n",
      "45\n",
      "0\n",
      "0\n",
      "46\n",
      "0\n",
      "0\n",
      "11\n",
      "82\n",
      "87\n",
      "5\n",
      "[ 1.29122695  0.99836318  0.98133442 ... -1.24918837 -0.75063162\n",
      " -0.88414056]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "56\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "1\n",
      "62\n",
      "62\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "1\n",
      "0\n",
      "0\n",
      "38\n",
      "27\n",
      "27\n",
      "89\n",
      "19\n",
      "0\n",
      "35\n",
      "35\n",
      "19\n",
      "19\n",
      "1\n",
      "5\n",
      "19\n",
      "68\n",
      "0\n",
      "83\n",
      "68\n",
      "68\n",
      "84\n",
      "45\n",
      "35\n",
      "35\n",
      "[-0.69336524 -0.82169076 -0.79553016 ...  0.26419132  0.05524117\n",
      " -0.49283501]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "40\n",
      "40\n",
      "0\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "39\n",
      "0\n",
      "39\n",
      "39\n",
      "84\n",
      "77\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "27\n",
      "0\n",
      "0\n",
      "35\n",
      "38\n",
      "35\n",
      "89\n",
      "19\n",
      "35\n",
      "22\n",
      "5\n",
      "0\n",
      "5\n",
      "35\n",
      "30\n",
      "83\n",
      "83\n",
      "5\n",
      "5\n",
      "22\n",
      "1\n",
      "26\n",
      "68\n",
      "[-0.70058166 -0.40268795 -0.73810163 ... -0.4400414  -0.23351689\n",
      "  0.35352801]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "19\n",
      "0\n",
      "40\n",
      "40\n",
      "73\n",
      "1\n",
      "62\n",
      "39\n",
      "39\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "84\n",
      "77\n",
      "84\n",
      "0\n",
      "0\n",
      "19\n",
      "19\n",
      "89\n",
      "27\n",
      "27\n",
      "0\n",
      "1\n",
      "0\n",
      "22\n",
      "35\n",
      "38\n",
      "0\n",
      "30\n",
      "30\n",
      "35\n",
      "35\n",
      "5\n",
      "22\n",
      "26\n",
      "1\n",
      "83\n",
      "0\n",
      "68\n",
      "[ 0.15019945 -0.24603608 -0.43347881 ...  0.88391185  0.08259094\n",
      " -0.07799202]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "56\n",
      "40\n",
      "73\n",
      "62\n",
      "73\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "38\n",
      "39\n",
      "84\n",
      "77\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "27\n",
      "38\n",
      "27\n",
      "35\n",
      "19\n",
      "19\n",
      "89\n",
      "27\n",
      "27\n",
      "0\n",
      "0\n",
      "22\n",
      "84\n",
      "5\n",
      "19\n",
      "35\n",
      "45\n",
      "0\n",
      "1\n",
      "84\n",
      "5\n",
      "1\n",
      "[ 0.43017465  0.61646589 -0.05660211 ...  0.05153047 -0.39522445\n",
      " -0.01509439]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "56\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "73\n",
      "73\n",
      "1\n",
      "2\n",
      "0\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "0\n",
      "0\n",
      "27\n",
      "27\n",
      "38\n",
      "38\n",
      "1\n",
      "0\n",
      "19\n",
      "1\n",
      "35\n",
      "35\n",
      "89\n",
      "0\n",
      "0\n",
      "83\n",
      "35\n",
      "68\n",
      "68\n",
      "84\n",
      "30\n",
      "5\n",
      "30\n",
      "45\n",
      "0\n",
      "1\n",
      "0\n",
      "[ 0.03272095 -0.37287382  0.18009888 ... -0.38847102 -0.80437408\n",
      " -0.99909556]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "0\n",
      "0\n",
      "40\n",
      "73\n",
      "73\n",
      "62\n",
      "1\n",
      "0\n",
      "39\n",
      "2\n",
      "0\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "35\n",
      "35\n",
      "1\n",
      "0\n",
      "38\n",
      "27\n",
      "19\n",
      "48\n",
      "89\n",
      "89\n",
      "0\n",
      "27\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "5\n",
      "5\n",
      "1\n",
      "0\n",
      "68\n",
      "83\n",
      "5\n",
      "19\n",
      "68\n",
      "26\n",
      "[ 0.75273067  0.59366853  0.61467449 ... -0.19049439 -0.6324687\n",
      " -0.15827419]\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "0\n",
      "19\n",
      "19\n",
      "40\n",
      "73\n",
      "73\n",
      "62\n",
      "38\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "77\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "27\n",
      "0\n",
      "0\n",
      "35\n",
      "89\n",
      "35\n",
      "68\n",
      "1\n",
      "84\n",
      "0\n",
      "0\n",
      "5\n",
      "35\n",
      "83\n",
      "45\n",
      "45\n",
      "0\n",
      "5\n",
      "22\n",
      "11\n",
      "1\n",
      "0\n",
      "[-0.19738856 -0.48362729  0.42492256 ...  0.24281564 -0.99615588\n",
      " -0.17996538]\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.4285386134974962\n",
      "F1 score for fold 1: 0.47344511439663\n",
      "F1 score for fold 2: 0.4574491454681253\n",
      "F1 score for fold 3: 0.44060259551683456\n",
      "F1 score for fold 4: 0.45294687774034237\n",
      "F1 score for fold 5: 0.4678536970441189\n",
      "F1 score for fold 6: 0.45073328250437283\n",
      "F1 score for fold 7: 0.4480920314253648\n",
      "F1 score for fold 8: 0.48055919615552634\n",
      "F1 score for fold 9: 0.46193055113093345\n",
      "Mean validation F1 score: 0.4562151104879745\n",
      "Validation F1 score stdev: 0.014749944572647035\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "5\n",
      "5\n",
      "1\n",
      "39\n",
      "62\n",
      "62\n",
      "19\n",
      "73\n",
      "84\n",
      "5\n",
      "19\n",
      "19\n",
      "77\n",
      "2\n",
      "0\n",
      "0\n",
      "27\n",
      "35\n",
      "27\n",
      "1\n",
      "0\n",
      "27\n",
      "40\n",
      "0\n",
      "0\n",
      "11\n",
      "22\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "89\n",
      "2\n",
      "45\n",
      "35\n",
      "30\n",
      "30\n",
      "84\n",
      "38\n",
      "38\n",
      "83\n",
      "68\n",
      "5\n",
      "22\n",
      "[ 0.32287476 -0.06717737  0.09951044 ... -0.62490136 -0.44861624\n",
      " -0.45865589]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "5\n",
      "5\n",
      "73\n",
      "1\n",
      "62\n",
      "62\n",
      "0\n",
      "0\n",
      "39\n",
      "84\n",
      "19\n",
      "5\n",
      "84\n",
      "84\n",
      "77\n",
      "19\n",
      "0\n",
      "1\n",
      "27\n",
      "27\n",
      "0\n",
      "0\n",
      "38\n",
      "89\n",
      "0\n",
      "38\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "35\n",
      "57\n",
      "83\n",
      "83\n",
      "0\n",
      "68\n",
      "0\n",
      "45\n",
      "35\n",
      "0\n",
      "1\n",
      "84\n",
      "11\n",
      "0\n",
      "[-0.31328227  0.01947651 -0.42774921 ...  0.87876113  0.51828192\n",
      "  1.30068674]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "5\n",
      "5\n",
      "40\n",
      "73\n",
      "73\n",
      "1\n",
      "62\n",
      "0\n",
      "0\n",
      "5\n",
      "19\n",
      "19\n",
      "84\n",
      "84\n",
      "77\n",
      "0\n",
      "1\n",
      "27\n",
      "27\n",
      "0\n",
      "0\n",
      "2\n",
      "35\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "39\n",
      "89\n",
      "48\n",
      "2\n",
      "89\n",
      "38\n",
      "38\n",
      "84\n",
      "35\n",
      "19\n",
      "35\n",
      "45\n",
      "0\n",
      "0\n",
      "11\n",
      "1\n",
      "0\n",
      "0\n",
      "68\n",
      "0\n",
      "[ 1.30294431  1.51816719  1.19528446 ... -0.87753103 -0.59412113\n",
      " -0.47835375]\n",
      "0\n",
      "2\n",
      "13\n",
      "5\n",
      "56\n",
      "62\n",
      "0\n",
      "1\n",
      "62\n",
      "73\n",
      "0\n",
      "19\n",
      "0\n",
      "0\n",
      "40\n",
      "84\n",
      "39\n",
      "84\n",
      "0\n",
      "0\n",
      "77\n",
      "1\n",
      "0\n",
      "27\n",
      "2\n",
      "0\n",
      "89\n",
      "35\n",
      "2\n",
      "19\n",
      "5\n",
      "56\n",
      "35\n",
      "35\n",
      "45\n",
      "0\n",
      "1\n",
      "1\n",
      "83\n",
      "84\n",
      "57\n",
      "57\n",
      "0\n",
      "38\n",
      "0\n",
      "68\n",
      "0\n",
      "0\n",
      "11\n",
      "1\n",
      "[-0.23379431 -1.07888545 -1.00233432 ...  0.62504344  0.25454871\n",
      " -0.65133393]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "5\n",
      "5\n",
      "1\n",
      "62\n",
      "73\n",
      "39\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "19\n",
      "77\n",
      "19\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "27\n",
      "35\n",
      "35\n",
      "0\n",
      "89\n",
      "5\n",
      "0\n",
      "5\n",
      "22\n",
      "2\n",
      "19\n",
      "57\n",
      "38\n",
      "83\n",
      "84\n",
      "5\n",
      "57\n",
      "1\n",
      "5\n",
      "68\n",
      "45\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "[ 0.4236476   0.29134744 -0.13205879 ... -0.00065995  0.39609053\n",
      "  0.46297731]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "5\n",
      "5\n",
      "73\n",
      "1\n",
      "62\n",
      "19\n",
      "19\n",
      "5\n",
      "39\n",
      "84\n",
      "77\n",
      "0\n",
      "19\n",
      "0\n",
      "38\n",
      "38\n",
      "27\n",
      "27\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "89\n",
      "35\n",
      "35\n",
      "45\n",
      "39\n",
      "2\n",
      "2\n",
      "83\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "68\n",
      "0\n",
      "0\n",
      "0\n",
      "57\n",
      "5\n",
      "1\n",
      "0\n",
      "22\n",
      "84\n",
      "[ 0.31703559  0.41730871 -0.12806287 ...  0.84371349  0.33781316\n",
      " -0.02260021]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "5\n",
      "73\n",
      "5\n",
      "62\n",
      "1\n",
      "62\n",
      "5\n",
      "19\n",
      "19\n",
      "77\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "39\n",
      "11\n",
      "1\n",
      "0\n",
      "27\n",
      "2\n",
      "0\n",
      "39\n",
      "68\n",
      "83\n",
      "35\n",
      "35\n",
      "1\n",
      "39\n",
      "0\n",
      "45\n",
      "35\n",
      "87\n",
      "87\n",
      "30\n",
      "2\n",
      "2\n",
      "89\n",
      "89\n",
      "1\n",
      "0\n",
      "57\n",
      "38\n",
      "19\n",
      "19\n",
      "[ 1.00737847  0.58983723  0.48602656 ...  0.57456326 -0.26288875\n",
      "  0.01079471]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "5\n",
      "5\n",
      "73\n",
      "1\n",
      "62\n",
      "62\n",
      "0\n",
      "0\n",
      "39\n",
      "84\n",
      "19\n",
      "84\n",
      "0\n",
      "0\n",
      "77\n",
      "27\n",
      "27\n",
      "0\n",
      "1\n",
      "2\n",
      "35\n",
      "0\n",
      "0\n",
      "57\n",
      "5\n",
      "22\n",
      "22\n",
      "2\n",
      "89\n",
      "84\n",
      "2\n",
      "38\n",
      "1\n",
      "2\n",
      "0\n",
      "83\n",
      "11\n",
      "35\n",
      "45\n",
      "0\n",
      "0\n",
      "30\n",
      "0\n",
      "1\n",
      "[ 0.9855527   0.0286293   0.49261898 ... -0.21188151  0.20595637\n",
      " -0.22809143]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "5\n",
      "1\n",
      "62\n",
      "62\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "5\n",
      "5\n",
      "19\n",
      "84\n",
      "84\n",
      "77\n",
      "1\n",
      "0\n",
      "27\n",
      "0\n",
      "0\n",
      "89\n",
      "48\n",
      "68\n",
      "83\n",
      "0\n",
      "0\n",
      "30\n",
      "38\n",
      "38\n",
      "0\n",
      "35\n",
      "1\n",
      "45\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "35\n",
      "19\n",
      "11\n",
      "87\n",
      "57\n",
      "1\n",
      "0\n",
      "[ 1.17930723  0.88790467  1.16647864 ...  0.26629282 -0.08606034\n",
      "  0.57192784]\n",
      "0\n",
      "2\n",
      "13\n",
      "0\n",
      "56\n",
      "5\n",
      "40\n",
      "73\n",
      "73\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "77\n",
      "5\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "27\n",
      "2\n",
      "0\n",
      "38\n",
      "35\n",
      "2\n",
      "22\n",
      "84\n",
      "1\n",
      "5\n",
      "48\n",
      "89\n",
      "89\n",
      "68\n",
      "39\n",
      "45\n",
      "35\n",
      "1\n",
      "0\n",
      "62\n",
      "62\n",
      "30\n",
      "83\n",
      "11\n",
      "83\n",
      "0\n",
      "70\n",
      "0\n",
      "5\n",
      "[ 0.19661307 -0.19697556  0.82416391 ...  0.88715304 -0.90100803\n",
      "  0.40949615]\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.37431360585723\n",
      "F1 score for fold 1: 0.4153155471676532\n",
      "F1 score for fold 2: 0.39426713947990544\n",
      "F1 score for fold 3: 0.3823252708567267\n",
      "F1 score for fold 4: 0.3834111001964637\n",
      "F1 score for fold 5: 0.40449773323789073\n",
      "F1 score for fold 6: 0.3948341684562157\n",
      "F1 score for fold 7: 0.3786110937792475\n",
      "F1 score for fold 8: 0.4190262481915616\n",
      "F1 score for fold 9: 0.4007992622194897\n",
      "Mean validation F1 score: 0.39474011694423844\n",
      "Validation F1 score stdev: 0.014509252695100098\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "1\n",
      "73\n",
      "73\n",
      "2\n",
      "0\n",
      "39\n",
      "35\n",
      "38\n",
      "39\n",
      "84\n",
      "38\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "77\n",
      "1\n",
      "0\n",
      "0\n",
      "27\n",
      "19\n",
      "19\n",
      "89\n",
      "5\n",
      "22\n",
      "68\n",
      "82\n",
      "11\n",
      "87\n",
      "30\n",
      "1\n",
      "1\n",
      "0\n",
      "45\n",
      "35\n",
      "[ 0.34648143  0.70326984  0.06418892 ...  0.02404975  0.09640925\n",
      " -0.18666618]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "0\n",
      "19\n",
      "40\n",
      "40\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "5\n",
      "22\n",
      "0\n",
      "0\n",
      "84\n",
      "77\n",
      "0\n",
      "38\n",
      "1\n",
      "1\n",
      "2\n",
      "27\n",
      "0\n",
      "39\n",
      "35\n",
      "68\n",
      "19\n",
      "0\n",
      "89\n",
      "19\n",
      "19\n",
      "84\n",
      "82\n",
      "39\n",
      "1\n",
      "0\n",
      "57\n",
      "11\n",
      "87\n",
      "87\n",
      "2\n",
      "83\n",
      "[0.05058009 0.36809462 0.00229348 ... 1.03466325 1.05299362 1.13510979]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "73\n",
      "1\n",
      "73\n",
      "0\n",
      "0\n",
      "84\n",
      "77\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "38\n",
      "19\n",
      "0\n",
      "35\n",
      "1\n",
      "84\n",
      "5\n",
      "0\n",
      "0\n",
      "45\n",
      "27\n",
      "27\n",
      "89\n",
      "2\n",
      "39\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "68\n",
      "68\n",
      "[ 1.94695914  1.56349047  1.53189598 ... -0.04667628  0.10114224\n",
      " -0.12797337]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "19\n",
      "13\n",
      "13\n",
      "0\n",
      "56\n",
      "40\n",
      "40\n",
      "73\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "62\n",
      "62\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "27\n",
      "35\n",
      "11\n",
      "27\n",
      "1\n",
      "0\n",
      "27\n",
      "48\n",
      "84\n",
      "38\n",
      "38\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "19\n",
      "0\n",
      "89\n",
      "0\n",
      "0\n",
      "35\n",
      "[-0.14841483 -0.29524363 -0.31716774 ...  0.41690112  0.7047583\n",
      "  0.21093595]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "73\n",
      "40\n",
      "0\n",
      "0\n",
      "39\n",
      "19\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "2\n",
      "0\n",
      "1\n",
      "27\n",
      "35\n",
      "35\n",
      "89\n",
      "0\n",
      "2\n",
      "38\n",
      "0\n",
      "0\n",
      "48\n",
      "5\n",
      "89\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "68\n",
      "68\n",
      "83\n",
      "26\n",
      "30\n",
      "11\n",
      "[0.17750525 0.45942817 0.06803901 ... 0.48216455 0.76847675 1.18998115]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "0\n",
      "39\n",
      "39\n",
      "2\n",
      "84\n",
      "84\n",
      "0\n",
      "77\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "27\n",
      "19\n",
      "0\n",
      "0\n",
      "89\n",
      "35\n",
      "35\n",
      "0\n",
      "38\n",
      "45\n",
      "35\n",
      "2\n",
      "83\n",
      "11\n",
      "1\n",
      "0\n",
      "38\n",
      "57\n",
      "0\n",
      "[1.06740707 0.85871519 0.58607516 ... 1.13793099 0.81444317 0.75119354]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "56\n",
      "19\n",
      "19\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "62\n",
      "38\n",
      "62\n",
      "1\n",
      "62\n",
      "0\n",
      "2\n",
      "2\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "77\n",
      "0\n",
      "5\n",
      "1\n",
      "0\n",
      "27\n",
      "84\n",
      "1\n",
      "1\n",
      "0\n",
      "19\n",
      "19\n",
      "89\n",
      "35\n",
      "35\n",
      "0\n",
      "68\n",
      "0\n",
      "0\n",
      "1\n",
      "83\n",
      "56\n",
      "0\n",
      "30\n",
      "[1.32545996 1.13732985 0.62376207 ... 0.57807074 0.34386809 0.70954842]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "40\n",
      "40\n",
      "73\n",
      "0\n",
      "0\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "2\n",
      "39\n",
      "0\n",
      "0\n",
      "35\n",
      "35\n",
      "77\n",
      "84\n",
      "0\n",
      "1\n",
      "27\n",
      "27\n",
      "38\n",
      "38\n",
      "0\n",
      "0\n",
      "48\n",
      "89\n",
      "89\n",
      "1\n",
      "0\n",
      "5\n",
      "68\n",
      "68\n",
      "83\n",
      "35\n",
      "35\n",
      "45\n",
      "0\n",
      "2\n",
      "11\n",
      "[ 0.8200958   0.60066648  0.83274403 ...  0.37311787  0.54719075\n",
      " -0.14619525]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "62\n",
      "62\n",
      "40\n",
      "0\n",
      "19\n",
      "19\n",
      "1\n",
      "73\n",
      "73\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "2\n",
      "77\n",
      "84\n",
      "2\n",
      "2\n",
      "35\n",
      "0\n",
      "1\n",
      "35\n",
      "0\n",
      "27\n",
      "38\n",
      "38\n",
      "89\n",
      "89\n",
      "19\n",
      "68\n",
      "83\n",
      "0\n",
      "0\n",
      "5\n",
      "22\n",
      "1\n",
      "1\n",
      "30\n",
      "84\n",
      "26\n",
      "[1.80509192 1.20041177 1.63288503 ... 0.55093824 0.18457801 0.52366158]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "40\n",
      "73\n",
      "73\n",
      "0\n",
      "2\n",
      "38\n",
      "84\n",
      "19\n",
      "84\n",
      "77\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "27\n",
      "27\n",
      "35\n",
      "35\n",
      "1\n",
      "89\n",
      "89\n",
      "30\n",
      "0\n",
      "0\n",
      "68\n",
      "49\n",
      "84\n",
      "83\n",
      "83\n",
      "5\n",
      "22\n",
      "1\n",
      "68\n",
      "0\n",
      "11\n",
      "70\n",
      "[ 0.77265116  0.20824991  1.04304269 ...  1.27311138 -0.14098475\n",
      "  0.44380807]\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.30451018537152563\n",
      "F1 score for fold 1: 0.34676255894575914\n",
      "F1 score for fold 2: 0.3224627309360756\n",
      "F1 score for fold 3: 0.3111798042246265\n",
      "F1 score for fold 4: 0.30750861699970716\n",
      "F1 score for fold 5: 0.33004262461626804\n",
      "F1 score for fold 6: 0.32681465821000705\n",
      "F1 score for fold 7: 0.3010983001412751\n",
      "F1 score for fold 8: 0.343751375621781\n",
      "F1 score for fold 9: 0.3248139906039966\n",
      "Mean validation F1 score: 0.3218944845671022\n",
      "Validation F1 score stdev: 0.015006812552815416\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "62\n",
      "0\n",
      "56\n",
      "0\n",
      "0\n",
      "19\n",
      "73\n",
      "0\n",
      "13\n",
      "73\n",
      "40\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "77\n",
      "84\n",
      "1\n",
      "35\n",
      "27\n",
      "0\n",
      "0\n",
      "35\n",
      "19\n",
      "57\n",
      "11\n",
      "39\n",
      "2\n",
      "19\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "89\n",
      "83\n",
      "35\n",
      "5\n",
      "5\n",
      "68\n",
      "0\n",
      "45\n",
      "[0.70176325 0.89053822 0.80820264 ... 0.80930798 0.573787   0.49349217]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "56\n",
      "56\n",
      "40\n",
      "73\n",
      "73\n",
      "0\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "38\n",
      "39\n",
      "84\n",
      "77\n",
      "35\n",
      "1\n",
      "35\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "19\n",
      "19\n",
      "27\n",
      "27\n",
      "89\n",
      "0\n",
      "0\n",
      "38\n",
      "38\n",
      "83\n",
      "68\n",
      "84\n",
      "5\n",
      "68\n",
      "0\n",
      "0\n",
      "45\n",
      "35\n",
      "[0.05223603 0.8693899  0.54710426 ... 1.67237174 1.60816818 1.79550232]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "0\n",
      "0\n",
      "2\n",
      "38\n",
      "2\n",
      "2\n",
      "0\n",
      "84\n",
      "77\n",
      "13\n",
      "0\n",
      "0\n",
      "35\n",
      "1\n",
      "27\n",
      "0\n",
      "0\n",
      "5\n",
      "22\n",
      "89\n",
      "89\n",
      "0\n",
      "30\n",
      "0\n",
      "0\n",
      "11\n",
      "45\n",
      "0\n",
      "39\n",
      "1\n",
      "83\n",
      "5\n",
      "[2.28416657 2.19398774 2.09540633 ... 0.09914414 0.24115264 0.12106804]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "56\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "1\n",
      "62\n",
      "62\n",
      "0\n",
      "0\n",
      "84\n",
      "39\n",
      "84\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "77\n",
      "0\n",
      "1\n",
      "27\n",
      "27\n",
      "19\n",
      "89\n",
      "19\n",
      "38\n",
      "35\n",
      "35\n",
      "19\n",
      "19\n",
      "1\n",
      "0\n",
      "84\n",
      "48\n",
      "5\n",
      "5\n",
      "0\n",
      "11\n",
      "83\n",
      "5\n",
      "[0.45963578 0.53584891 0.42077993 ... 0.9863059  1.3555383  0.69524748]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "56\n",
      "19\n",
      "40\n",
      "40\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "77\n",
      "84\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "35\n",
      "35\n",
      "1\n",
      "0\n",
      "5\n",
      "27\n",
      "19\n",
      "19\n",
      "89\n",
      "68\n",
      "39\n",
      "0\n",
      "0\n",
      "83\n",
      "68\n",
      "30\n",
      "38\n",
      "0\n",
      "0\n",
      "30\n",
      "11\n",
      "45\n",
      "0\n",
      "5\n",
      "[0.80985023 0.9392112  0.49410662 ... 0.95984    1.50831622 1.28251776]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "1\n",
      "73\n",
      "73\n",
      "0\n",
      "2\n",
      "2\n",
      "39\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "0\n",
      "0\n",
      "1\n",
      "35\n",
      "0\n",
      "38\n",
      "19\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "89\n",
      "0\n",
      "27\n",
      "0\n",
      "5\n",
      "22\n",
      "1\n",
      "0\n",
      "84\n",
      "57\n",
      "45\n",
      "0\n",
      "1\n",
      "[1.2299593  0.91608954 0.98474605 ... 1.65543048 0.91746689 0.73523503]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "56\n",
      "62\n",
      "56\n",
      "0\n",
      "19\n",
      "19\n",
      "1\n",
      "0\n",
      "0\n",
      "73\n",
      "40\n",
      "40\n",
      "39\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "38\n",
      "77\n",
      "27\n",
      "27\n",
      "22\n",
      "0\n",
      "0\n",
      "89\n",
      "35\n",
      "35\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "5\n",
      "11\n",
      "0\n",
      "1\n",
      "35\n",
      "45\n",
      "0\n",
      "19\n",
      "83\n",
      "[1.66436244 1.48429429 1.19569283 ... 1.28840592 1.07580214 1.24700645]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "40\n",
      "1\n",
      "73\n",
      "73\n",
      "39\n",
      "84\n",
      "84\n",
      "0\n",
      "0\n",
      "2\n",
      "38\n",
      "0\n",
      "0\n",
      "1\n",
      "77\n",
      "0\n",
      "27\n",
      "19\n",
      "19\n",
      "70\n",
      "0\n",
      "5\n",
      "22\n",
      "0\n",
      "0\n",
      "35\n",
      "35\n",
      "84\n",
      "11\n",
      "1\n",
      "1\n",
      "0\n",
      "45\n",
      "0\n",
      "35\n",
      "2\n",
      "[1.35058403 1.03096503 1.34226371 ... 0.95248853 0.78715513 0.52628129]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "19\n",
      "19\n",
      "56\n",
      "56\n",
      "40\n",
      "0\n",
      "0\n",
      "73\n",
      "62\n",
      "62\n",
      "1\n",
      "0\n",
      "0\n",
      "39\n",
      "84\n",
      "84\n",
      "77\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "27\n",
      "2\n",
      "35\n",
      "38\n",
      "38\n",
      "0\n",
      "0\n",
      "89\n",
      "1\n",
      "83\n",
      "68\n",
      "0\n",
      "0\n",
      "84\n",
      "1\n",
      "1\n",
      "11\n",
      "82\n",
      "5\n",
      "5\n",
      "[1.89273046 1.85605319 1.70475642 ... 0.98529879 0.5600854  1.21039318]\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "13\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "0\n",
      "19\n",
      "40\n",
      "40\n",
      "73\n",
      "1\n",
      "39\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "84\n",
      "77\n",
      "1\n",
      "0\n",
      "35\n",
      "27\n",
      "84\n",
      "89\n",
      "0\n",
      "38\n",
      "19\n",
      "5\n",
      "5\n",
      "35\n",
      "1\n",
      "35\n",
      "45\n",
      "84\n",
      "5\n",
      "22\n",
      "0\n",
      "0\n",
      "83\n",
      "11\n",
      "0\n",
      "1\n",
      "0\n",
      "19\n",
      "[0.87957509 0.65063496 1.33609883 ... 1.5384814  0.10994593 1.09810519]\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.27208823643636654\n",
      "F1 score for fold 1: 0.3104531859311702\n",
      "F1 score for fold 2: 0.2880292532717475\n",
      "F1 score for fold 3: 0.2738774401366088\n",
      "F1 score for fold 4: 0.2719108565640037\n",
      "F1 score for fold 5: 0.29349563743523566\n",
      "F1 score for fold 6: 0.2900769230769231\n",
      "F1 score for fold 7: 0.26455171872239347\n",
      "F1 score for fold 8: 0.3042646777586536\n",
      "F1 score for fold 9: 0.28547071474240987\n",
      "Mean validation F1 score: 0.28542186440755124\n",
      "Validation F1 score stdev: 0.014157840929517036\n",
      "Best weight: 5\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "0\n",
      "56\n",
      "62\n",
      "62\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "39\n",
      "0\n",
      "0\n",
      "84\n",
      "84\n",
      "77\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "27\n",
      "38\n",
      "89\n",
      "35\n",
      "19\n",
      "35\n",
      "35\n",
      "38\n",
      "11\n",
      "45\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "5\n",
      "30\n",
      "83\n",
      "0\n",
      "83\n",
      "5\n",
      "22\n",
      "[ 0.65267064 -0.41199111 -0.17356651 ... -0.5467721  -0.76590512\n",
      " -0.46495813]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_recall_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m best_adaboost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#y_preds = best_adaboost.predict_proportion(X_test, proportion)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# compute F1 score on test data\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m (precision, recall, f1) \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_f1\u001b[49m(y_preds, y_test)\n\u001b[0;32m     32\u001b[0m auprc \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maverage_precision_score(y_test, y_preds)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest precision: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(precision))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_recall_f1' is not defined"
     ]
    }
   ],
   "source": [
    "w1_vals = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "best_score = 0\n",
    "best_weight = 0\n",
    "\n",
    "for w in w1_vals:\n",
    "\n",
    "    adaboost = MyWeightedAdaboost(DecisionTreeClassifier(max_depth=1), 50, w)\n",
    "\n",
    "    cv_scores = my_cross_val_imbalanced(adaboost, 'f1', None, X_train, y_train, k=10)\n",
    "    \n",
    "    print(\"Weight: \" + str(w))\n",
    "    for i in range(10):\n",
    "        print(\"F1 score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "    mean_score = sum(cv_scores)/len(cv_scores)\n",
    "    print(\"Mean validation F1 score: \" + str(mean_score))\n",
    "    print(\"Validation F1 score stdev: \" + str(np.std(cv_scores)))\n",
    "    if mean_score >= best_score:\n",
    "        best_score = mean_score\n",
    "        best_weight = w\n",
    "\n",
    "print(\"Best weight: \" + str(best_weight))\n",
    "best_adaboost = MyWeightedAdaboost(DecisionTreeClassifier(max_depth=1), 50, best_weight)\n",
    "\n",
    "best_adaboost.fit(X_train, y_train)\n",
    "\n",
    "y_preds = best_adaboost.predict(X_test)\n",
    "#y_preds = best_adaboost.predict_proportion(X_test, proportion)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_preds)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a66522ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.316182842287695\n",
      "Test recall: 0.7179045745204132\n",
      "Test F1 score: 0.4390133854714995\n",
      "Test AUPRC score: 0.2714206409182094\n"
     ]
    }
   ],
   "source": [
    "# Weight 5...\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_preds)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c415d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weight: 5\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "13\n",
      "0\n",
      "62\n",
      "0\n",
      "0\n",
      "19\n",
      "56\n",
      "19\n",
      "19\n",
      "0\n",
      "1\n",
      "73\n",
      "40\n",
      "40\n",
      "0\n",
      "0\n",
      "39\n",
      "39\n",
      "84\n",
      "77\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "19\n",
      "38\n",
      "27\n",
      "27\n",
      "89\n",
      "0\n",
      "35\n",
      "13\n",
      "0\n",
      "0\n",
      "0\n",
      "45\n",
      "5\n",
      "0\n",
      "0\n",
      "84\n",
      "68\n",
      "83\n",
      "11\n",
      "0\n",
      "[ 0.63834925 -0.6702301  -0.11222252 ... -0.6315668  -0.41247379\n",
      " -0.5587478 ]\n",
      "Test precision: 0.3234995487364621\n",
      "Test recall: 0.7052385636989671\n",
      "Test F1 score: 0.44354215003866976\n",
      "Test AUPRC score: 0.38378946447779805\n"
     ]
    }
   ],
   "source": [
    "print(\"Best weight: \" + str(best_weight))\n",
    "best_adaboost = MyWeightedAdaboost(DecisionTreeClassifier(max_depth=1), 50, best_weight)\n",
    "\n",
    "best_adaboost.fit(X_train, y_train)\n",
    "\n",
    "y_preds = best_adaboost.predict(X_test)\n",
    "#y_preds = best_adaboost.predict_proportion(X_test, proportion)\n",
    "y_values = best_adaboost.predict_values(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_values)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f50655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
