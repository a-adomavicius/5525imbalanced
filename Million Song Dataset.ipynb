{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86245666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import sklearn.metrics\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b9a538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/YearPredictionMSD.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d83424e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion by year 1922: 1.1642685967652738e-05\n",
      "Proportion by year 1923: 1.1642685967652738e-05\n",
      "Proportion by year 1924: 2.1344924274030017e-05\n",
      "Proportion by year 1925: 3.4928057902958214e-05\n",
      "Proportion by year 1926: 7.179656346719188e-05\n",
      "Proportion by year 1927: 0.00015329536524076105\n",
      "Proportion by year 1928: 0.00025419864362708476\n",
      "Proportion by year 1929: 0.0004346602761257022\n",
      "Proportion by year 1930: 0.0005122781825767205\n",
      "Proportion by year 1931: 0.0005801938507213614\n",
      "Proportion by year 1932: 0.0006015387749953914\n",
      "Proportion by year 1933: 0.0006131814609630442\n",
      "Proportion by year 1934: 0.0006694544431400324\n",
      "Proportion by year 1935: 0.0007160251870106433\n",
      "Proportion by year 1936: 0.0007645363785425298\n",
      "Proportion by year 1937: 0.0008188689130582426\n",
      "Proportion by year 1938: 0.0008557374186224762\n",
      "Proportion by year 1939: 0.0009236530867671171\n",
      "Proportion by year 1940: 0.001024556365153441\n",
      "Proportion by year 1941: 0.0010866506903142554\n",
      "Proportion by year 1942: 0.0011332214341848665\n",
      "Proportion by year 1943: 0.0011603877014427228\n",
      "Proportion by year 1944: 0.0011894944163618546\n",
      "Proportion by year 1945: 0.0012477078462001185\n",
      "Proportion by year 1946: 0.0013039808283771066\n",
      "Proportion by year 1947: 0.0014145863450698076\n",
      "Proportion by year 1948: 0.0014980255945046522\n",
      "Proportion by year 1949: 0.0016144524541811796\n",
      "Proportion by year 1950: 0.0017755096100670424\n",
      "Proportion by year 1951: 0.0019191027370014263\n",
      "Proportion by year 1952: 0.0020685172069196366\n",
      "Proportion by year 1953: 0.002326596745869272\n",
      "Proportion by year 1954: 0.002565271808206153\n",
      "Proportion by year 1955: 0.0030988949150569036\n",
      "Proportion by year 1956: 0.004195247843677537\n",
      "Proportion by year 1957: 0.005353695097458984\n",
      "Proportion by year 1958: 0.006484976083982575\n",
      "Proportion by year 1959: 0.007633721099457645\n",
      "Proportion by year 1960: 0.008456470907838439\n",
      "Proportion by year 1961: 0.009564466522426724\n",
      "Proportion by year 1962: 0.010738437357498375\n",
      "Proportion by year 1963: 0.012488721147968836\n",
      "Proportion by year 1964: 0.014322444187874142\n",
      "Proportion by year 1965: 0.016495745568502654\n",
      "Proportion by year 1966: 0.01916774199807896\n",
      "Proportion by year 1967: 0.02250143108015019\n",
      "Proportion by year 1968: 0.026124246863751466\n",
      "Proportion by year 1969: 0.030412636195170227\n",
      "Proportion by year 1970: 0.03497074775150627\n",
      "Proportion by year 1971: 0.03910584171768427\n",
      "Proportion by year 1972: 0.043545585966682517\n",
      "Proportion by year 1973: 0.0485829880953536\n",
      "Proportion by year 1974: 0.052820925787579194\n",
      "Proportion by year 1975: 0.057637116882864874\n",
      "Proportion by year 1976: 0.0618653523367841\n",
      "Proportion by year 1977: 0.06672035238529529\n",
      "Proportion by year 1978: 0.07239810224218728\n",
      "Proportion by year 1979: 0.07842901357343139\n",
      "Proportion by year 1980: 0.08444634177104658\n",
      "Proportion by year 1981: 0.09058203727599957\n",
      "Proportion by year 1982: 0.09756182751360738\n",
      "Proportion by year 1983: 0.10413218329468608\n",
      "Proportion by year 1984: 0.11066761101786182\n",
      "Proportion by year 1985: 0.1176105327499054\n",
      "Proportion by year 1986: 0.12579728143282656\n",
      "Proportion by year 1987: 0.13573625435387943\n",
      "Proportion by year 1988: 0.14662410618129604\n",
      "Proportion by year 1989: 0.1595668920820033\n",
      "Proportion by year 1990: 0.17364678031221803\n",
      "Proportion by year 1991: 0.1904258312392669\n",
      "Proportion by year 1992: 0.20894352327081858\n",
      "Proportion by year 1993: 0.22936673490574275\n",
      "Proportion by year 1994: 0.25288690100806255\n",
      "Proportion by year 1995: 0.27861141565359127\n",
      "Proportion by year 1996: 0.3060299411074135\n",
      "Proportion by year 1997: 0.33548981750089746\n",
      "Proportion by year 1998: 0.36617605681630755\n",
      "Proportion by year 1999: 0.4015659412626493\n",
      "Proportion by year 2000: 0.43898747441034647\n",
      "Proportion by year 2001: 0.48088173941728357\n",
      "Proportion by year 2002: 0.5263871775218543\n",
      "Proportion by year 2003: 0.5795205153828988\n",
      "Proportion by year 2004: 0.6369713492902813\n",
      "Proportion by year 2005: 0.704793875947181\n",
      "Proportion by year 2006: 0.777626638465494\n",
      "Proportion by year 2007: 0.854088038110392\n",
      "Proportion by year 2008: 0.9215379988163269\n",
      "Proportion by year 2009: 0.9817656133269945\n",
      "Proportion by year 2010: 0.9999980595523387\n"
     ]
    }
   ],
   "source": [
    "for year in range(1922, 2011):\n",
    "    #print(\"Songs by year \" + str(year) + \": \" + str(len(df[df[0] <= year])))\n",
    "    print(\"Proportion by year \" + str(year) + \": \" + str(len(df[df[0] <= year])/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7d40dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922        6\n",
      "1924        5\n",
      "1925        7\n",
      "1926       19\n",
      "1927       42\n",
      "1928       52\n",
      "1929       93\n",
      "1930       40\n",
      "1931       35\n",
      "1932       11\n",
      "1933        6\n",
      "1934       29\n",
      "1935       24\n",
      "1936       25\n",
      "1937       28\n",
      "1938       19\n",
      "1939       35\n",
      "1940       52\n",
      "1941       32\n",
      "1942       24\n",
      "1943       14\n",
      "1944       15\n",
      "1945       30\n",
      "1946       29\n",
      "1947       57\n",
      "1948       43\n",
      "1949       60\n",
      "1950       83\n",
      "1951       74\n",
      "1952       77\n",
      "1953      133\n",
      "1954      123\n",
      "1955      275\n",
      "1956      565\n",
      "1957      597\n",
      "1958      583\n",
      "1959      592\n",
      "1960      424\n",
      "1961      571\n",
      "1962      605\n",
      "1963      902\n",
      "1964      945\n",
      "1965     1120\n",
      "1966     1377\n",
      "1967     1718\n",
      "1968     1867\n",
      "1969     2210\n",
      "1970     2349\n",
      "1971     2131\n",
      "1972     2288\n",
      "1973     2596\n",
      "1974     2184\n",
      "1975     2482\n",
      "1976     2179\n",
      "1977     2502\n",
      "1978     2926\n",
      "1979     3108\n",
      "1980     3101\n",
      "1981     3162\n",
      "1982     3597\n",
      "1983     3386\n",
      "1984     3368\n",
      "1985     3578\n",
      "1986     4219\n",
      "1987     5122\n",
      "1988     5611\n",
      "1989     6670\n",
      "1990     7256\n",
      "1991     8647\n",
      "1992     9543\n",
      "1993    10525\n",
      "1994    12121\n",
      "1995    13257\n",
      "1996    14130\n",
      "1997    15182\n",
      "1998    15814\n",
      "1999    18238\n",
      "2000    19285\n",
      "2001    21590\n",
      "2002    23451\n",
      "2003    27382\n",
      "2004    29607\n",
      "2005    34952\n",
      "2006    37534\n",
      "2007    39404\n",
      "2008    34760\n",
      "2009    31038\n",
      "2010     9396\n",
      "2011        1\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "year = df[0]\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(year.value_counts().sort_index())\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9dea5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>before_1989</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515340</th>\n",
       "      <td>2006</td>\n",
       "      <td>51.28467</td>\n",
       "      <td>45.88068</td>\n",
       "      <td>22.19582</td>\n",
       "      <td>-5.53319</td>\n",
       "      <td>-3.61835</td>\n",
       "      <td>-16.36914</td>\n",
       "      <td>2.12652</td>\n",
       "      <td>5.18160</td>\n",
       "      <td>-8.66890</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.75991</td>\n",
       "      <td>-30.92584</td>\n",
       "      <td>26.33968</td>\n",
       "      <td>-5.03390</td>\n",
       "      <td>21.86037</td>\n",
       "      <td>-142.29410</td>\n",
       "      <td>3.42901</td>\n",
       "      <td>-41.14721</td>\n",
       "      <td>-15.46052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515341</th>\n",
       "      <td>2006</td>\n",
       "      <td>49.87870</td>\n",
       "      <td>37.93125</td>\n",
       "      <td>18.65987</td>\n",
       "      <td>-3.63581</td>\n",
       "      <td>-27.75665</td>\n",
       "      <td>-18.52988</td>\n",
       "      <td>7.76108</td>\n",
       "      <td>3.56109</td>\n",
       "      <td>-2.50351</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.75535</td>\n",
       "      <td>-61.05473</td>\n",
       "      <td>56.65182</td>\n",
       "      <td>15.29965</td>\n",
       "      <td>95.88193</td>\n",
       "      <td>-10.63242</td>\n",
       "      <td>12.96552</td>\n",
       "      <td>92.11633</td>\n",
       "      <td>10.88815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515342</th>\n",
       "      <td>2006</td>\n",
       "      <td>45.12852</td>\n",
       "      <td>12.65758</td>\n",
       "      <td>-38.72018</td>\n",
       "      <td>8.80882</td>\n",
       "      <td>-29.29985</td>\n",
       "      <td>-2.28706</td>\n",
       "      <td>-18.40424</td>\n",
       "      <td>-22.28726</td>\n",
       "      <td>-4.52429</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.15954</td>\n",
       "      <td>-123.98443</td>\n",
       "      <td>121.26989</td>\n",
       "      <td>10.89629</td>\n",
       "      <td>34.62409</td>\n",
       "      <td>-248.61020</td>\n",
       "      <td>-6.07171</td>\n",
       "      <td>53.96319</td>\n",
       "      <td>-8.09364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515343</th>\n",
       "      <td>2006</td>\n",
       "      <td>44.16614</td>\n",
       "      <td>32.38368</td>\n",
       "      <td>-3.34971</td>\n",
       "      <td>-2.49165</td>\n",
       "      <td>-19.59278</td>\n",
       "      <td>-18.67098</td>\n",
       "      <td>8.78428</td>\n",
       "      <td>4.02039</td>\n",
       "      <td>-12.01230</td>\n",
       "      <td>...</td>\n",
       "      <td>282.77624</td>\n",
       "      <td>-4.63677</td>\n",
       "      <td>144.00125</td>\n",
       "      <td>21.62652</td>\n",
       "      <td>-29.72432</td>\n",
       "      <td>71.47198</td>\n",
       "      <td>20.32240</td>\n",
       "      <td>14.83107</td>\n",
       "      <td>39.74909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515344</th>\n",
       "      <td>2005</td>\n",
       "      <td>51.85726</td>\n",
       "      <td>59.11655</td>\n",
       "      <td>26.39436</td>\n",
       "      <td>-5.46030</td>\n",
       "      <td>-20.69012</td>\n",
       "      <td>-19.95528</td>\n",
       "      <td>-6.72771</td>\n",
       "      <td>2.29590</td>\n",
       "      <td>10.31018</td>\n",
       "      <td>...</td>\n",
       "      <td>-69.18291</td>\n",
       "      <td>60.58456</td>\n",
       "      <td>28.64599</td>\n",
       "      <td>-4.39620</td>\n",
       "      <td>-64.56491</td>\n",
       "      <td>-45.61012</td>\n",
       "      <td>-5.51512</td>\n",
       "      <td>32.35602</td>\n",
       "      <td>12.17352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515345 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
       "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
       "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
       "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
       "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
       "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
       "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
       "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
       "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
       "\n",
       "               7         8         9  ...         82         83         84  \\\n",
       "0      -25.01202 -12.23257   7.83089  ...  -54.40548   58.99367   15.37344   \n",
       "1        8.76630  -0.92019  18.76548  ...  -19.68073   33.04964   42.87836   \n",
       "2       -3.27872  -2.35035  16.07017  ...   26.05866  -50.92779   10.93792   \n",
       "3        5.05097 -10.34124   3.55005  ... -171.70734  -16.96705  -46.67617   \n",
       "4      -12.48207  -9.37636  12.63699  ...  -55.95724   64.92712  -17.72522   \n",
       "...          ...       ...       ...  ...        ...        ...        ...   \n",
       "515340   2.12652   5.18160  -8.66890  ...   -3.75991  -30.92584   26.33968   \n",
       "515341   7.76108   3.56109  -2.50351  ...  -32.75535  -61.05473   56.65182   \n",
       "515342 -18.40424 -22.28726  -4.52429  ...  -71.15954 -123.98443  121.26989   \n",
       "515343   8.78428   4.02039 -12.01230  ...  282.77624   -4.63677  144.00125   \n",
       "515344  -6.72771   2.29590  10.31018  ...  -69.18291   60.58456   28.64599   \n",
       "\n",
       "              85        86         87        88         89        90  \\\n",
       "0        1.11144 -23.08793   68.40795  -1.82223  -27.46348   2.26327   \n",
       "1       -9.90378 -32.22788   70.49388  12.04941   58.43453  26.92061   \n",
       "2       -0.07568  43.20130 -115.00698  -0.05859   39.67068  -0.66345   \n",
       "3      -12.51516  82.58061  -72.08993   9.90558  199.62971  18.85382   \n",
       "4       -1.49237  -7.50035   51.76631   7.88713   55.66926  28.74903   \n",
       "...          ...       ...        ...       ...        ...       ...   \n",
       "515340  -5.03390  21.86037 -142.29410   3.42901  -41.14721 -15.46052   \n",
       "515341  15.29965  95.88193  -10.63242  12.96552   92.11633  10.88815   \n",
       "515342  10.89629  34.62409 -248.61020  -6.07171   53.96319  -8.09364   \n",
       "515343  21.62652 -29.72432   71.47198  20.32240   14.83107  39.74909   \n",
       "515344  -4.39620 -64.56491  -45.61012  -5.51512   32.35602  12.17352   \n",
       "\n",
       "        before_1989  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "515340            0  \n",
       "515341            0  \n",
       "515342            0  \n",
       "515343            0  \n",
       "515344            0  \n",
       "\n",
       "[515345 rows x 92 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f4a2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['before_1989'] = df[0] <= 1989\n",
    "df['before_1989'] = df['before_1989'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c452209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    433113\n",
       "1     82232\n",
       "Name: before_1989, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['before_1989'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6301fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'before_1989'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad47f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([0, target_feature], axis = 1) # dropping year attribute (determines label we want to classify)\n",
    "y = df[target_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf2f8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:463715], X[463715:]\n",
    "y_train, y_test = y[:463715], y[463715:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d2d75c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>-2.46783</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01620</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>4.59210</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>1.39518</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>-6.36304</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>0.93609</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463710</th>\n",
       "      <td>46.38102</td>\n",
       "      <td>-16.05772</td>\n",
       "      <td>5.67177</td>\n",
       "      <td>-9.58596</td>\n",
       "      <td>-44.80420</td>\n",
       "      <td>-5.89595</td>\n",
       "      <td>-24.16065</td>\n",
       "      <td>-7.26308</td>\n",
       "      <td>12.52231</td>\n",
       "      <td>-5.02750</td>\n",
       "      <td>...</td>\n",
       "      <td>19.74370</td>\n",
       "      <td>100.71014</td>\n",
       "      <td>1.85122</td>\n",
       "      <td>0.33202</td>\n",
       "      <td>25.06474</td>\n",
       "      <td>27.42051</td>\n",
       "      <td>-26.57589</td>\n",
       "      <td>0.14839</td>\n",
       "      <td>-94.39950</td>\n",
       "      <td>-14.08372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463711</th>\n",
       "      <td>42.62982</td>\n",
       "      <td>11.63038</td>\n",
       "      <td>-24.00713</td>\n",
       "      <td>-13.48721</td>\n",
       "      <td>-44.12528</td>\n",
       "      <td>-14.34941</td>\n",
       "      <td>6.68187</td>\n",
       "      <td>-5.60320</td>\n",
       "      <td>11.75803</td>\n",
       "      <td>-4.75869</td>\n",
       "      <td>...</td>\n",
       "      <td>3.81001</td>\n",
       "      <td>254.67990</td>\n",
       "      <td>-25.67103</td>\n",
       "      <td>-24.05268</td>\n",
       "      <td>39.26604</td>\n",
       "      <td>21.15773</td>\n",
       "      <td>-233.19754</td>\n",
       "      <td>2.85007</td>\n",
       "      <td>106.88870</td>\n",
       "      <td>-6.25309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463712</th>\n",
       "      <td>44.37612</td>\n",
       "      <td>1.62531</td>\n",
       "      <td>38.16556</td>\n",
       "      <td>2.26337</td>\n",
       "      <td>-14.94471</td>\n",
       "      <td>-6.00659</td>\n",
       "      <td>-32.69876</td>\n",
       "      <td>-2.02698</td>\n",
       "      <td>1.38501</td>\n",
       "      <td>-2.83300</td>\n",
       "      <td>...</td>\n",
       "      <td>12.44530</td>\n",
       "      <td>3.88388</td>\n",
       "      <td>-8.15594</td>\n",
       "      <td>-40.22390</td>\n",
       "      <td>14.29071</td>\n",
       "      <td>-61.00160</td>\n",
       "      <td>-72.57607</td>\n",
       "      <td>-4.39948</td>\n",
       "      <td>22.42941</td>\n",
       "      <td>-4.10893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463713</th>\n",
       "      <td>44.88723</td>\n",
       "      <td>14.14760</td>\n",
       "      <td>-5.70694</td>\n",
       "      <td>-19.70480</td>\n",
       "      <td>-58.91571</td>\n",
       "      <td>-14.32484</td>\n",
       "      <td>-3.92128</td>\n",
       "      <td>-0.48551</td>\n",
       "      <td>2.18089</td>\n",
       "      <td>2.08289</td>\n",
       "      <td>...</td>\n",
       "      <td>27.54173</td>\n",
       "      <td>186.59898</td>\n",
       "      <td>57.06463</td>\n",
       "      <td>-28.73053</td>\n",
       "      <td>20.62933</td>\n",
       "      <td>90.60712</td>\n",
       "      <td>33.54519</td>\n",
       "      <td>11.57071</td>\n",
       "      <td>106.61509</td>\n",
       "      <td>16.80881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463714</th>\n",
       "      <td>50.32201</td>\n",
       "      <td>6.71191</td>\n",
       "      <td>54.05607</td>\n",
       "      <td>-7.56020</td>\n",
       "      <td>-39.23615</td>\n",
       "      <td>-10.95161</td>\n",
       "      <td>-40.51556</td>\n",
       "      <td>-3.63441</td>\n",
       "      <td>6.58425</td>\n",
       "      <td>0.99665</td>\n",
       "      <td>...</td>\n",
       "      <td>7.75069</td>\n",
       "      <td>1.77275</td>\n",
       "      <td>12.78713</td>\n",
       "      <td>18.00105</td>\n",
       "      <td>-1.47984</td>\n",
       "      <td>-20.11586</td>\n",
       "      <td>-9.00316</td>\n",
       "      <td>3.77736</td>\n",
       "      <td>-42.94888</td>\n",
       "      <td>0.05278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463715 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1         2         3         4         5         6         7   \\\n",
       "0       49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905 -25.01202   \n",
       "1       48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   8.76630   \n",
       "2       50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940  -3.27872   \n",
       "3       48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   5.05097   \n",
       "4       50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409 -12.48207   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "463710  46.38102 -16.05772   5.67177  -9.58596 -44.80420  -5.89595 -24.16065   \n",
       "463711  42.62982  11.63038 -24.00713 -13.48721 -44.12528 -14.34941   6.68187   \n",
       "463712  44.37612   1.62531  38.16556   2.26337 -14.94471  -6.00659 -32.69876   \n",
       "463713  44.88723  14.14760  -5.70694 -19.70480 -58.91571 -14.32484  -3.92128   \n",
       "463714  50.32201   6.71191  54.05607  -7.56020 -39.23615 -10.95161 -40.51556   \n",
       "\n",
       "              8         9        10  ...        81         82        83  \\\n",
       "0      -12.23257   7.83089 -2.46783  ...  13.01620  -54.40548  58.99367   \n",
       "1       -0.92019  18.76548  4.59210  ...   5.66812  -19.68073  33.04964   \n",
       "2       -2.35035  16.07017  1.39518  ...   3.03800   26.05866 -50.92779   \n",
       "3      -10.34124   3.55005 -6.36304  ...  34.57337 -171.70734 -16.96705   \n",
       "4       -9.37636  12.63699  0.93609  ...   9.92661  -55.95724  64.92712   \n",
       "...          ...       ...      ...  ...       ...        ...       ...   \n",
       "463710  -7.26308  12.52231 -5.02750  ...  19.74370  100.71014   1.85122   \n",
       "463711  -5.60320  11.75803 -4.75869  ...   3.81001  254.67990 -25.67103   \n",
       "463712  -2.02698   1.38501 -2.83300  ...  12.44530    3.88388  -8.15594   \n",
       "463713  -0.48551   2.18089  2.08289  ...  27.54173  186.59898  57.06463   \n",
       "463714  -3.63441   6.58425  0.99665  ...   7.75069    1.77275  12.78713   \n",
       "\n",
       "              84        85        86         87        88         89        90  \n",
       "0       15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n",
       "1       42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453  26.92061  \n",
       "2       10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068  -0.66345  \n",
       "3      -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971  18.85382  \n",
       "4      -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926  28.74903  \n",
       "...          ...       ...       ...        ...       ...        ...       ...  \n",
       "463710   0.33202  25.06474  27.42051  -26.57589   0.14839  -94.39950 -14.08372  \n",
       "463711 -24.05268  39.26604  21.15773 -233.19754   2.85007  106.88870  -6.25309  \n",
       "463712 -40.22390  14.29071 -61.00160  -72.57607  -4.39948   22.42941  -4.10893  \n",
       "463713 -28.73053  20.62933  90.60712   33.54519  11.57071  106.61509  16.80881  \n",
       "463714  18.00105  -1.47984 -20.11586   -9.00316   3.77736  -42.94888   0.05278  \n",
       "\n",
       "[463715 rows x 90 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3957247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to np arrays to be able to use training functions\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba4b6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_cross_val function from hw1; modified one is in cell below \n",
    "def my_cross_val(model, loss_func, X, y, k=10):\n",
    "    (n, d) = X.shape\n",
    "    validation_losses = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        val_set = X[round(i*n/k):round((i+1)*n/k), :]\n",
    "        val_labels = y[round(i*n/k):round((i+1)*n/k)]\n",
    "        train_set = np.delete(X, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        train_labels = np.delete(y, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        model.fit(train_set, train_labels)\n",
    "        y_preds = model.predict(val_set)\n",
    "        \n",
    "        val_loss = 0\n",
    "        for j in range(len(y_preds)):\n",
    "            if loss_func == 'mse':\n",
    "                val_loss += (val_labels[j] - y_preds[j])**2\n",
    "            if loss_func == 'err_rate' and (val_labels[j] != y_preds[j]):\n",
    "                val_loss += 1\n",
    "        val_loss *= (1/len(y_preds))\n",
    "        validation_losses[i] = val_loss\n",
    "    return validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f77351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: choice of evaluation metric (f1, precision, recall, etc.)\n",
    "# Proportion: proportion of test set to predict as 1s, if needed (logistic regression may predict all 0 by default)\n",
    "def my_cross_val_imbalanced(model, metric, proportion, X, y, k=10):\n",
    "    (n, d) = X.shape\n",
    "    validation_metrics = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        val_set = X[round(i*n/k):round((i+1)*n/k), :]\n",
    "        val_labels = y[round(i*n/k):round((i+1)*n/k)]\n",
    "        train_set = np.delete(X, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        train_labels = np.delete(y, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        model.fit(train_set, train_labels)\n",
    "        if proportion == None:\n",
    "            y_preds = model.predict(val_set)\n",
    "        else:\n",
    "            y_preds = model.predict_proportion(val_set, proportion)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        score = 0\n",
    "        for j in range(len(y_preds)):\n",
    "            if val_labels[j] == 1 and y_preds[j] == 1:\n",
    "                tp += 1\n",
    "            elif val_labels[j] == 1:\n",
    "                fn += 1\n",
    "            elif y_preds[j] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        if tp == 0: # to avoid division by zero error for trivial models\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)  \n",
    "        if metric == 'precision':\n",
    "            score = precision\n",
    "        if metric == 'recall':\n",
    "            score = recall\n",
    "        if metric == 'f1':\n",
    "            if precision + recall == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = 2 * precision * recall / (precision + recall)\n",
    "        if metric == 'auprc':\n",
    "            score = sklearn.metrics.average_precision_score(val_labels, y_preds)\n",
    "        validation_metrics[i] = score\n",
    "    return validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd5a7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "def L(y, x, w, weight): # weighted log loss function\n",
    "    return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
    "\n",
    "class MyWeightedLogisticRegression:\n",
    "\n",
    "    def __init__(self, d, max_iters, eta_val, weight):\n",
    "        self.w = np.zeros(d)\n",
    "        self.w_old = np.random.uniform(-0.01, 0.01, d)\n",
    "        self.w_sum = np.zeros(d)\n",
    "        self.w_sum += self.w_old\n",
    "        self.max_iters = max_iters\n",
    "        self.eta_val = eta_val\n",
    "        self.weight = weight\n",
    "        self.iters = 0 # keep track of iterations made\n",
    "        self.losses = [] # used to keep track of losses for plotting purposes\n",
    "        self.gradient_magnitudes = [] \n",
    "    def fit(self, X, y):\n",
    "        (n, d) = X.shape\n",
    "        while self.iters < self.max_iters:\n",
    "            i = np.random.randint(n)\n",
    "            z = sigmoid(X @ self.w_old)\n",
    "            self.losses.append(L(z[i], X[i, :], self.w_old, self.weight))\n",
    "            gradient_magnitude = 0\n",
    "            for j in range(d):\n",
    "                gradient_j = - (y[i]*X[i, j]*(self.weight*sigmoid(-self.w_old @ X[i, :]) + sigmoid(self.w_old @ X[i, :])) - X[i, j] * sigmoid(self.w_old @ X[i, :]))\n",
    "                self.w[j] = self.w_old[j] - self.eta_val*gradient_j\n",
    "                gradient_magnitude += gradient_j**2\n",
    "                self.w_old[j] = self.w[j]\n",
    "            self.gradient_magnitudes.append(gradient_magnitude)\n",
    "            self.w_sum += self.w\n",
    "            self.iters += 1\n",
    "            if np.average(self.gradient_magnitudes[-10:]) < 1e-7:\n",
    "                break\n",
    "    def predict(self, X):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        return np.round(sigmoid(X @ w_avg))\n",
    "    def predict_proportion(self, X, prop): #predict a certain number of 1s\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        probs = sigmoid(X @ w_avg)\n",
    "        print(probs)\n",
    "        threshold = np.quantile(probs, 1-prop)\n",
    "        print(\"Threshold: \", threshold)\n",
    "        preds = np.zeros(len(probs))\n",
    "        for i in range(len(preds)):\n",
    "            if probs[i] >= threshold:\n",
    "                preds[i] = 1\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fc264c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_14280\\363175868.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_14280\\363175868.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_14280\\363175868.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.001\n",
      "Error rate for fold 0: 0.15160010351073924\n",
      "Error rate for fold 1: 0.17778352849841494\n",
      "Error rate for fold 2: 0.16229971318280823\n",
      "Error rate for fold 3: 0.1523764340550332\n",
      "Error rate for fold 4: 0.15078064349176226\n",
      "Error rate for fold 5: 0.1647365810528132\n",
      "Error rate for fold 6: 0.16288197364732268\n",
      "Error rate for fold 7: 0.14556197705511947\n",
      "Error rate for fold 8: 0.17174156818769948\n",
      "Error rate for fold 9: 0.1593668456578465\n",
      "Mean validation error rate: 0.15991293683395594\n",
      "Validation error rate stdev: 0.00955302625590369\n",
      "Eta: 0.01\n",
      "Error rate for fold 0: 0.1736823945484344\n",
      "Error rate for fold 1: 0.1999741217571327\n",
      "Error rate for fold 2: 0.18632334864462702\n",
      "Error rate for fold 3: 0.16501337013715173\n",
      "Error rate for fold 4: 0.1756879151211938\n",
      "Error rate for fold 5: 0.1872506523473723\n",
      "Error rate for fold 6: 0.1904423023010071\n",
      "Error rate for fold 7: 0.19278875183300268\n",
      "Error rate for fold 8: 0.23447338911412058\n",
      "Error rate for fold 9: 0.22759914601798537\n",
      "Mean validation error rate: 0.19332353918220277\n",
      "Validation error rate stdev: 0.021225181359453976\n",
      "Eta: 0.1\n",
      "Error rate for fold 0: 0.20904856378849304\n",
      "Error rate for fold 1: 0.2297341010545384\n",
      "Error rate for fold 2: 0.2188005434431002\n",
      "Error rate for fold 3: 0.20415336841197274\n",
      "Error rate for fold 4: 0.20335547313033725\n",
      "Error rate for fold 5: 0.21453063336999417\n",
      "Error rate for fold 6: 0.230273231114274\n",
      "Error rate for fold 7: 0.2076684205986371\n",
      "Error rate for fold 8: 0.23494781333563355\n",
      "Error rate for fold 9: 0.22846175411356234\n",
      "Mean validation error rate: 0.2180973902360543\n",
      "Validation error rate stdev: 0.011360641039649542\n",
      "Best eta value: 0.001\n",
      "Number of positive predictions:  0.0\n",
      "Test error rate: 0.15750532636064304\n"
     ]
    }
   ],
   "source": [
    "#Demonstration of standard LR with error rate predicting everything as 0\n",
    "\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "\n",
    "best_loss = np.inf\n",
    "best_eta = 0\n",
    "\n",
    "(_, num_features) = X_train.shape\n",
    "\n",
    "# Logistic Regression\n",
    "for eta_val in eta_vals:\n",
    "\n",
    "    # instantiate logistic regression object\n",
    "    lr = MyWeightedLogisticRegression(num_features, 100000, eta_val, 1)\n",
    "\n",
    "    # call to your CV function to compute error rates for each fold\n",
    "    cv_losses = my_cross_val(lr, 'err_rate', X_train, y_train, k=10)\n",
    "\n",
    "    # print error rates from CV\n",
    "    print(\"Eta: \" + str(eta_val))\n",
    "    for i in range(10):\n",
    "        print(\"Error rate for fold \" + str(i) + \": \" + str(cv_losses[i]))\n",
    "    mean_loss = sum(cv_losses)/len(cv_losses)\n",
    "    print(\"Mean validation error rate: \" + str(mean_loss))\n",
    "    print(\"Validation error rate stdev: \" + str(np.std(cv_losses)))\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        best_eta = eta_val\n",
    "\n",
    "# instantiate logistic regression object for best value of eta\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "best_lr = MyWeightedLogisticRegression(num_features, 100000, best_eta, 1)\n",
    "\n",
    "# fit model using all training data\n",
    "best_lr.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_preds = best_lr.predict(X_test)\n",
    "print(\"Number of positive predictions: \", sum(y_preds))\n",
    "\n",
    "# compute error rate on test data\n",
    "errors = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_preds[i] != y_test[i]:\n",
    "        errors += 1\n",
    "error_rate = errors/len(y_test)\n",
    "\n",
    "# print error rate on test data\n",
    "print(\"Test error rate: \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbd1c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(preds, truth):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for j in range(len(preds)):\n",
    "        if truth[j] == 1 and preds[j] == 1:\n",
    "            tp += 1\n",
    "        elif truth[j] == 1:\n",
    "            fn += 1\n",
    "        elif preds[j] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else: \n",
    "        f1 = 0\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eae9a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_14280\\363175868.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_14280\\363175868.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_14280\\363175868.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.001\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.16637078366725844\n",
      "F1 score for fold 1: 0.1793953108303476\n",
      "F1 score for fold 2: 0.145943979628956\n",
      "F1 score for fold 3: 0.03321764997521071\n",
      "F1 score for fold 4: 0.03270440251572327\n",
      "F1 score for fold 5: 0.03043578510491123\n",
      "F1 score for fold 6: 0.024720423778693344\n",
      "F1 score for fold 7: 0.01752057340058402\n",
      "F1 score for fold 8: 0.01847417037290455\n",
      "F1 score for fold 9: 0.02242855420233932\n",
      "Mean validation F1 score: 0.06712116334769284\n",
      "Validation F1 score stdev: 0.06400842662636805\n",
      "Eta: 0.001\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.027830637488106564\n",
      "F1 score for fold 1: 0.02173211403980635\n",
      "F1 score for fold 2: 0.015939098370405618\n",
      "F1 score for fold 3: 0.012625611955681525\n",
      "F1 score for fold 4: 0.014106583072100314\n",
      "F1 score for fold 5: 0.007351077313054499\n",
      "F1 score for fold 6: 0.008210391276459268\n",
      "F1 score for fold 7: 0.009480034472852629\n",
      "F1 score for fold 8: 0.00808526277104006\n",
      "F1 score for fold 9: 0.009715110936064067\n",
      "Mean validation F1 score: 0.01350759216955709\n",
      "Validation F1 score stdev: 0.006370322537266606\n",
      "Eta: 0.001\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.24529529296965752\n",
      "F1 score for fold 1: 0.2768449912763692\n",
      "F1 score for fold 2: 0.25544836607413984\n",
      "F1 score for fold 3: 0.2336003051106026\n",
      "F1 score for fold 4: 0.158600434171881\n",
      "F1 score for fold 5: 0.1617536534446764\n",
      "F1 score for fold 6: 0.18405377808032677\n",
      "F1 score for fold 7: 0.15401282717975387\n",
      "F1 score for fold 8: 0.1851051450419749\n",
      "F1 score for fold 9: 0.16436781609195403\n",
      "Mean validation F1 score: 0.2019082609441336\n",
      "Validation F1 score stdev: 0.04377353289643807\n",
      "Eta: 0.001\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.23773563479445833\n",
      "F1 score for fold 1: 0.2762086479804359\n",
      "F1 score for fold 2: 0.24986783160355044\n",
      "F1 score for fold 3: 0.2305316938970299\n",
      "F1 score for fold 4: 0.22797750894530583\n",
      "F1 score for fold 5: 0.25200044454323184\n",
      "F1 score for fold 6: 0.2563931324261903\n",
      "F1 score for fold 7: 0.2306421350359853\n",
      "F1 score for fold 8: 0.26577963492412576\n",
      "F1 score for fold 9: 0.24159651141663133\n",
      "Mean validation F1 score: 0.24687331755669448\n",
      "Validation F1 score stdev: 0.015306159970307646\n",
      "Eta: 0.001\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.21090828303773096\n",
      "F1 score for fold 2: 0.1867389425720795\n",
      "F1 score for fold 3: 0.16565224543510443\n",
      "F1 score for fold 4: 0.1631427604871448\n",
      "F1 score for fold 5: 0.16715864046554776\n",
      "F1 score for fold 6: 0.18705409261901676\n",
      "F1 score for fold 7: 0.15811088295687886\n",
      "F1 score for fold 8: 0.18309616213885296\n",
      "F1 score for fold 9: 0.16513591242230263\n",
      "Mean validation F1 score: 0.15869979221346586\n",
      "Validation F1 score stdev: 0.0550255330521104\n",
      "Eta: 0.001\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.2675498467115716\n",
      "F1 score for fold 1: 0.30656055065605503\n",
      "F1 score for fold 2: 0.28615704766332384\n",
      "F1 score for fold 3: 0.27263792718338176\n",
      "F1 score for fold 4: 0.2735192814209495\n",
      "F1 score for fold 5: 0.2951102461463609\n",
      "F1 score for fold 6: 0.2925900506984879\n",
      "F1 score for fold 7: 0.26201599137543796\n",
      "F1 score for fold 8: 0.30620197705906965\n",
      "F1 score for fold 9: 0.28801530623595634\n",
      "Mean validation F1 score: 0.28503582251505943\n",
      "Validation F1 score stdev: 0.014839222913197034\n",
      "Eta: 0.01\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.08628052118690463\n",
      "F1 score for fold 1: 0.07985907222548444\n",
      "F1 score for fold 2: 0.07351712614870509\n",
      "F1 score for fold 3: 0.08473465822231928\n",
      "F1 score for fold 4: 0.0854831657192829\n",
      "F1 score for fold 5: 0.002297970126388357\n",
      "F1 score for fold 6: 0.001039095986491752\n",
      "F1 score for fold 7: 0.002326595899374727\n",
      "F1 score for fold 8: 0.002470660901791229\n",
      "F1 score for fold 9: 0.0013324450366422387\n",
      "Mean validation F1 score: 0.04193413114533846\n",
      "Validation F1 score stdev: 0.04018562053419116\n",
      "Eta: 0.01\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0018552875695732837\n",
      "F1 score for fold 3: 0.01702806970866037\n",
      "F1 score for fold 4: 0.010159253157605712\n",
      "F1 score for fold 5: 0.008036162732295328\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.000295159386068477\n",
      "F1 score for fold 8: 0.00025003125390673836\n",
      "F1 score for fold 9: 0.0005366966322286329\n",
      "Mean validation F1 score: 0.003816066044033855\n",
      "Validation F1 score stdev: 0.00562140252597999\n",
      "Eta: 0.01\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.15256588072122051\n",
      "F1 score for fold 4: 0.139874213836478\n",
      "F1 score for fold 5: 0.1495060696908635\n",
      "F1 score for fold 6: 0.16221188736137376\n",
      "F1 score for fold 7: 0.14181493728021835\n",
      "F1 score for fold 8: 0.1694646659733571\n",
      "F1 score for fold 9: 0.1540034768381225\n",
      "Mean validation F1 score: 0.10694411317016336\n",
      "Validation F1 score stdev: 0.07048566651619291\n",
      "Eta: 0.01\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.12536113553573672\n",
      "F1 score for fold 1: 0.1264948834915547\n",
      "F1 score for fold 2: 0.09918359085081979\n",
      "F1 score for fold 3: 0.0880973321606567\n",
      "F1 score for fold 4: 0.08011575660650369\n",
      "F1 score for fold 5: 0.08061877138182359\n",
      "F1 score for fold 6: 0.09255009943399112\n",
      "F1 score for fold 7: 0.07236679058240397\n",
      "F1 score for fold 8: 0.23446440452428474\n",
      "F1 score for fold 9: 0.23779828189627747\n",
      "Mean validation F1 score: 0.12370510464640523\n",
      "Validation F1 score stdev: 0.058759968708631746\n",
      "Eta: 0.01\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0\n",
      "F1 score for fold 9: 0.0\n",
      "Mean validation F1 score: 0.0\n",
      "Validation F1 score stdev: 0.0\n",
      "Eta: 0.01\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.2688072774763009\n",
      "F1 score for fold 1: 0.31098492181282394\n",
      "F1 score for fold 2: 0.2883058271757887\n",
      "F1 score for fold 3: 0.27663191803498727\n",
      "F1 score for fold 4: 0.2748640510538852\n",
      "F1 score for fold 5: 0.29722332582880856\n",
      "F1 score for fold 6: 0.2941228536024075\n",
      "F1 score for fold 7: 0.26430193090116216\n",
      "F1 score for fold 8: 0.30729816724692904\n",
      "F1 score for fold 9: 0.28857795418764\n",
      "Mean validation F1 score: 0.28711182273207336\n",
      "Validation F1 score stdev: 0.014991896001800379\n",
      "Eta: 0.1\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.2652768205835056\n",
      "F1 score for fold 1: 0.24900364861072133\n",
      "F1 score for fold 2: 0.04086115992970123\n",
      "F1 score for fold 3: 0.034611786716557534\n",
      "F1 score for fold 4: 0.030267202648380235\n",
      "F1 score for fold 5: 0.02731578367661637\n",
      "F1 score for fold 6: 0.025267907501410045\n",
      "F1 score for fold 7: 0.02680025046963056\n",
      "F1 score for fold 8: 0.02820262501355895\n",
      "F1 score for fold 9: 0.031228630043309776\n",
      "Mean validation F1 score: 0.07588358151933915\n",
      "Validation F1 score stdev: 0.0908012284331187\n",
      "Eta: 0.1\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0\n",
      "F1 score for fold 9: 0.0005382131324004306\n",
      "Mean validation F1 score: 5.382131324004306e-05\n",
      "Validation F1 score stdev: 0.00016146393972012917\n",
      "Eta: 0.1\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.15992769479592808\n",
      "F1 score for fold 1: 0.17541317636404802\n",
      "F1 score for fold 2: 0.1499568262496402\n",
      "F1 score for fold 3: 0.1363613676066883\n",
      "F1 score for fold 4: 0.13591542136830334\n",
      "F1 score for fold 5: 0.13864748776346017\n",
      "F1 score for fold 6: 0.16568864342423195\n",
      "F1 score for fold 7: 0.1386535665948168\n",
      "F1 score for fold 8: 0.16929862000284535\n",
      "F1 score for fold 9: 0.1509207365892714\n",
      "Mean validation F1 score: 0.15207835407592335\n",
      "Validation F1 score stdev: 0.014009360158337272\n",
      "Eta: 0.1\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.006403118040089087\n",
      "F1 score for fold 1: 0.23589437762979157\n",
      "F1 score for fold 2: 0.21658844085100554\n",
      "F1 score for fold 3: 0.19688400013879734\n",
      "F1 score for fold 4: 0.19646791397097396\n",
      "F1 score for fold 5: 0.21269732326698695\n",
      "F1 score for fold 6: 0.22627560714409092\n",
      "F1 score for fold 7: 0.1961495535714286\n",
      "F1 score for fold 8: 0.23209043360890835\n",
      "F1 score for fold 9: 0.21826858967610627\n",
      "Mean validation F1 score: 0.19377193578981786\n",
      "Validation F1 score stdev: 0.0639590692969222\n",
      "Eta: 0.1\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.0002843332385555872\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.00026592208482914504\n",
      "F1 score for fold 3: 0.0008479366873940079\n",
      "F1 score for fold 4: 0.0005706134094151213\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0005282620179609086\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0002506579771901241\n",
      "F1 score for fold 9: 0.00027016074564365796\n",
      "Mean validation F1 score: 0.00030178861609885525\n",
      "Validation F1 score stdev: 0.0002646260675269447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.1\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.12277803475847486\n",
      "F1 score for fold 1: 0.12931575643440052\n",
      "F1 score for fold 2: 0.28068883353189644\n",
      "F1 score for fold 3: 0.270907743109374\n",
      "F1 score for fold 4: 0.2755769580228966\n",
      "F1 score for fold 5: 0.2971009663445518\n",
      "F1 score for fold 6: 0.2945663811563169\n",
      "F1 score for fold 7: 0.26484452905448286\n",
      "F1 score for fold 8: 0.30701270016565435\n",
      "F1 score for fold 9: 0.288921687016414\n",
      "Mean validation F1 score: 0.25317135895944626\n",
      "Validation F1 score stdev: 0.06470937182417537\n",
      "Best eta value: 0.01\n",
      "Best weight value: 50\n",
      "Test precision: 0.033210332103321034\n",
      "Test recall: 0.0011067388096409247\n",
      "Test F1 score: 0.0021420921099607284\n",
      "Test AUPRC score: 0.15736776426665436\n"
     ]
    }
   ],
   "source": [
    "# Weighted Logistic Regression with predicting fixed proportion of 1s \n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "weight_vals = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "best_score = 0\n",
    "best_eta = 0\n",
    "best_weight = 0\n",
    "\n",
    "(_, num_features) = X_train.shape\n",
    "proportion = sum(y_train)/len(y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "for eta_val in eta_vals:\n",
    "    for weight in weight_vals:\n",
    "\n",
    "        # instantiate logistic regression object\n",
    "        lr = MyWeightedLogisticRegression(num_features, 100000, eta_val, weight)\n",
    "\n",
    "        # call to your CV function to compute error rates for each fold\n",
    "        #cv_scores = my_cross_val_imbalanced(lr, 'f1', proportion, X_train, y_train, k=10)\n",
    "        cv_scores = my_cross_val_imbalanced(lr, 'f1', None, X_train, y_train, k=10)\n",
    "\n",
    "        # print error rates from CV\n",
    "        print(\"Eta: \" + str(eta_val))\n",
    "        print(\"Weight: \" + str(weight))\n",
    "        for i in range(10):\n",
    "            print(\"F1 score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "        mean_score = sum(cv_scores)/len(cv_scores)\n",
    "        print(\"Mean validation F1 score: \" + str(mean_score))\n",
    "        print(\"Validation F1 score stdev: \" + str(np.std(cv_scores)))\n",
    "        if mean_score >= best_score:\n",
    "            best_score = mean_score\n",
    "            best_eta = eta_val\n",
    "            best_weight = weight\n",
    "\n",
    "# instantiate logistic regression object for best value of eta\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "print(\"Best weight value: \" + str(best_weight))\n",
    "best_lr = MyWeightedLogisticRegression(num_features, 100000, best_eta, best_weight)\n",
    "\n",
    "# fit model using all training data\n",
    "best_lr.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "#y_preds = best_lr.predict_proportion(X_test, proportion)\n",
    "y_preds = best_lr.predict(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_preds)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f9654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
