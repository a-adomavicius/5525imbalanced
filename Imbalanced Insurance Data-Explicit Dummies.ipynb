{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3549af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6a9d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('aug_train.csv')\n",
    "test_data = pd.read_csv('aug_test.csv')\n",
    "y_test = np.load('answer.npy')\n",
    "train_samples = len(train_data)\n",
    "test_samples = len(test_data)\n",
    "#print(train_samples, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39a3497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167647</td>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17163</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>43327.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>135</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32023</td>\n",
       "      <td>Female</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>35841.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>253</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87447</td>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27645.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>501933</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>29023.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>211</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78268</th>\n",
       "      <td>847</td>\n",
       "      <td>Male</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78269</th>\n",
       "      <td>417524</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>32937.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78270</th>\n",
       "      <td>188087</td>\n",
       "      <td>Male</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>35247.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78271</th>\n",
       "      <td>215680</td>\n",
       "      <td>Male</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>25705.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78272</th>\n",
       "      <td>138006</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27752.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460427 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0      167647    Male   22                1          7.0                   1   \n",
       "1       17163    Male   42                1         28.0                   0   \n",
       "2       32023  Female   66                1         33.0                   0   \n",
       "3       87447  Female   22                1         33.0                   0   \n",
       "4      501933    Male   28                1         46.0                   1   \n",
       "...       ...     ...  ...              ...          ...                 ...   \n",
       "78268     847    Male   43                1         39.0                   0   \n",
       "78269  417524  Female   21                1         12.0                   1   \n",
       "78270  188087    Male   48                1         29.0                   1   \n",
       "78271  215680    Male   64                1          5.0                   1   \n",
       "78272  138006  Female   25                1         41.0                   1   \n",
       "\n",
       "      Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\n",
       "0        < 1 Year             No          2630.0                 152.0   \n",
       "1        1-2 Year            Yes         43327.0                  26.0   \n",
       "2        1-2 Year            Yes         35841.0                 124.0   \n",
       "3        < 1 Year             No         27645.0                 152.0   \n",
       "4        < 1 Year             No         29023.0                 152.0   \n",
       "...           ...            ...             ...                   ...   \n",
       "78268    1-2 Year            Yes          2630.0                 124.0   \n",
       "78269    < 1 Year             No         32937.0                 152.0   \n",
       "78270    1-2 Year             No         35247.0                 124.0   \n",
       "78271    1-2 Year             No         25705.0                  26.0   \n",
       "78272    < 1 Year             No         27752.0                 152.0   \n",
       "\n",
       "       Vintage  Response  \n",
       "0           16       0.0  \n",
       "1          135       0.0  \n",
       "2          253       0.0  \n",
       "3           69       0.0  \n",
       "4          211       0.0  \n",
       "...        ...       ...  \n",
       "78268       26       NaN  \n",
       "78269      185       NaN  \n",
       "78270      101       NaN  \n",
       "78271       86       NaN  \n",
       "78272      235       NaN  \n",
       "\n",
       "[460427 rows x 12 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([train_data, test_data])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c95489b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Region_Code_0.0</th>\n",
       "      <th>...</th>\n",
       "      <th>Policy_Sales_Channel_152.0</th>\n",
       "      <th>Policy_Sales_Channel_153.0</th>\n",
       "      <th>Policy_Sales_Channel_154.0</th>\n",
       "      <th>Policy_Sales_Channel_155.0</th>\n",
       "      <th>Policy_Sales_Channel_156.0</th>\n",
       "      <th>Policy_Sales_Channel_157.0</th>\n",
       "      <th>Policy_Sales_Channel_158.0</th>\n",
       "      <th>Policy_Sales_Channel_159.0</th>\n",
       "      <th>Policy_Sales_Channel_160.0</th>\n",
       "      <th>Policy_Sales_Channel_163.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167647</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17163</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43327.0</td>\n",
       "      <td>135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32023</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35841.0</td>\n",
       "      <td>253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87447</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27645.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>501933</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29023.0</td>\n",
       "      <td>211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78268</th>\n",
       "      <td>847</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78269</th>\n",
       "      <td>417524</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32937.0</td>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78270</th>\n",
       "      <td>188087</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35247.0</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78271</th>\n",
       "      <td>215680</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25705.0</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78272</th>\n",
       "      <td>138006</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27752.0</td>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460427 rows × 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Age  Driving_License  Previously_Insured  Annual_Premium  \\\n",
       "0      167647   22                1                   1          2630.0   \n",
       "1       17163   42                1                   0         43327.0   \n",
       "2       32023   66                1                   0         35841.0   \n",
       "3       87447   22                1                   0         27645.0   \n",
       "4      501933   28                1                   1         29023.0   \n",
       "...       ...  ...              ...                 ...             ...   \n",
       "78268     847   43                1                   0          2630.0   \n",
       "78269  417524   21                1                   1         32937.0   \n",
       "78270  188087   48                1                   1         35247.0   \n",
       "78271  215680   64                1                   1         25705.0   \n",
       "78272  138006   25                1                   1         27752.0   \n",
       "\n",
       "       Vintage  Response  Gender_Female  Gender_Male  Region_Code_0.0  ...  \\\n",
       "0           16       0.0              0            1                0  ...   \n",
       "1          135       0.0              0            1                0  ...   \n",
       "2          253       0.0              1            0                0  ...   \n",
       "3           69       0.0              1            0                0  ...   \n",
       "4          211       0.0              0            1                0  ...   \n",
       "...        ...       ...            ...          ...              ...  ...   \n",
       "78268       26       NaN              0            1                0  ...   \n",
       "78269      185       NaN              1            0                0  ...   \n",
       "78270      101       NaN              0            1                0  ...   \n",
       "78271       86       NaN              0            1                0  ...   \n",
       "78272      235       NaN              1            0                0  ...   \n",
       "\n",
       "       Policy_Sales_Channel_152.0  Policy_Sales_Channel_153.0  \\\n",
       "0                               1                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               1                           0   \n",
       "4                               1                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           1                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           1                           0   \n",
       "\n",
       "       Policy_Sales_Channel_154.0  Policy_Sales_Channel_155.0  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           0                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           0                           0   \n",
       "\n",
       "       Policy_Sales_Channel_156.0  Policy_Sales_Channel_157.0  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           0                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           0                           0   \n",
       "\n",
       "       Policy_Sales_Channel_158.0  Policy_Sales_Channel_159.0  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           0                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           0                           0   \n",
       "\n",
       "       Policy_Sales_Channel_160.0  Policy_Sales_Channel_163.0  \n",
       "0                               0                           0  \n",
       "1                               0                           0  \n",
       "2                               0                           0  \n",
       "3                               0                           0  \n",
       "4                               0                           0  \n",
       "...                           ...                         ...  \n",
       "78268                           0                           0  \n",
       "78269                           0                           0  \n",
       "78270                           0                           0  \n",
       "78271                           0                           0  \n",
       "78272                           0                           0  \n",
       "\n",
       "[460427 rows x 223 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, columns=['Gender', 'Region_Code', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4182035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[0:train_samples]\n",
    "test_data = data.iloc[train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af9432af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Region_Code_0.0</th>\n",
       "      <th>Region_Code_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>Policy_Sales_Channel_152.0</th>\n",
       "      <th>Policy_Sales_Channel_153.0</th>\n",
       "      <th>Policy_Sales_Channel_154.0</th>\n",
       "      <th>Policy_Sales_Channel_155.0</th>\n",
       "      <th>Policy_Sales_Channel_156.0</th>\n",
       "      <th>Policy_Sales_Channel_157.0</th>\n",
       "      <th>Policy_Sales_Channel_158.0</th>\n",
       "      <th>Policy_Sales_Channel_159.0</th>\n",
       "      <th>Policy_Sales_Channel_160.0</th>\n",
       "      <th>Policy_Sales_Channel_163.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57782</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38244.0</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>286811</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37577.0</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117823</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24578.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213992</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40507.0</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324756</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36783.0</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78268</th>\n",
       "      <td>847</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78269</th>\n",
       "      <td>417524</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32937.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78270</th>\n",
       "      <td>188087</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35247.0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78271</th>\n",
       "      <td>215680</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25705.0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78272</th>\n",
       "      <td>138006</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27752.0</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78273 rows × 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Age  Driving_License  Previously_Insured  Annual_Premium  \\\n",
       "0       57782   34                1                   1         38244.0   \n",
       "1      286811   55                1                   0         37577.0   \n",
       "2      117823   39                1                   1         24578.0   \n",
       "3      213992   28                1                   1         40507.0   \n",
       "4      324756   24                1                   0         36783.0   \n",
       "...       ...  ...              ...                 ...             ...   \n",
       "78268     847   43                1                   0          2630.0   \n",
       "78269  417524   21                1                   1         32937.0   \n",
       "78270  188087   48                1                   1         35247.0   \n",
       "78271  215680   64                1                   1         25705.0   \n",
       "78272  138006   25                1                   1         27752.0   \n",
       "\n",
       "       Vintage  Gender_Female  Gender_Male  Region_Code_0.0  Region_Code_1.0  \\\n",
       "0          146              1            0                0                0   \n",
       "1          109              1            0                0                0   \n",
       "2           63              0            1                0                0   \n",
       "3          129              0            1                0                0   \n",
       "4          201              1            0                0                0   \n",
       "...        ...            ...          ...              ...              ...   \n",
       "78268       26              0            1                0                0   \n",
       "78269      185              1            0                0                0   \n",
       "78270      101              0            1                0                0   \n",
       "78271       86              0            1                0                0   \n",
       "78272      235              1            0                0                0   \n",
       "\n",
       "       ...  Policy_Sales_Channel_152.0  Policy_Sales_Channel_153.0  \\\n",
       "0      ...                           0                           0   \n",
       "1      ...                           0                           0   \n",
       "2      ...                           0                           0   \n",
       "3      ...                           0                           0   \n",
       "4      ...                           1                           0   \n",
       "...    ...                         ...                         ...   \n",
       "78268  ...                           0                           0   \n",
       "78269  ...                           1                           0   \n",
       "78270  ...                           0                           0   \n",
       "78271  ...                           0                           0   \n",
       "78272  ...                           1                           0   \n",
       "\n",
       "       Policy_Sales_Channel_154.0  Policy_Sales_Channel_155.0  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           0                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           0                           0   \n",
       "\n",
       "       Policy_Sales_Channel_156.0  Policy_Sales_Channel_157.0  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           0                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           0                           0   \n",
       "\n",
       "       Policy_Sales_Channel_158.0  Policy_Sales_Channel_159.0  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "78268                           0                           0   \n",
       "78269                           0                           0   \n",
       "78270                           0                           0   \n",
       "78271                           0                           0   \n",
       "78272                           0                           0   \n",
       "\n",
       "       Policy_Sales_Channel_160.0  Policy_Sales_Channel_163.0  \n",
       "0                               0                           0  \n",
       "1                               0                           0  \n",
       "2                               0                           0  \n",
       "3                               0                           0  \n",
       "4                               0                           0  \n",
       "...                           ...                         ...  \n",
       "78268                           0                           0  \n",
       "78269                           0                           0  \n",
       "78270                           0                           0  \n",
       "78271                           0                           0  \n",
       "78272                           0                           0  \n",
       "\n",
       "[78273 rows x 222 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.drop('Response', axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5da337f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Region_Code_0.0</th>\n",
       "      <th>Region_Code_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>Policy_Sales_Channel_152.0</th>\n",
       "      <th>Policy_Sales_Channel_153.0</th>\n",
       "      <th>Policy_Sales_Channel_154.0</th>\n",
       "      <th>Policy_Sales_Channel_155.0</th>\n",
       "      <th>Policy_Sales_Channel_156.0</th>\n",
       "      <th>Policy_Sales_Channel_157.0</th>\n",
       "      <th>Policy_Sales_Channel_158.0</th>\n",
       "      <th>Policy_Sales_Channel_159.0</th>\n",
       "      <th>Policy_Sales_Channel_160.0</th>\n",
       "      <th>Policy_Sales_Channel_163.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43327.0</td>\n",
       "      <td>135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35841.0</td>\n",
       "      <td>253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27645.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29023.0</td>\n",
       "      <td>211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382149</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23938.0</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382150</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>336395.0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382151</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40443.0</td>\n",
       "      <td>187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382152</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25380.0</td>\n",
       "      <td>208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382153</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30396.0</td>\n",
       "      <td>104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382154 rows × 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Driving_License  Previously_Insured  Annual_Premium  Vintage  \\\n",
       "0        22                1                   1          2630.0       16   \n",
       "1        42                1                   0         43327.0      135   \n",
       "2        66                1                   0         35841.0      253   \n",
       "3        22                1                   0         27645.0       69   \n",
       "4        28                1                   1         29023.0      211   \n",
       "...     ...              ...                 ...             ...      ...   \n",
       "382149   24                1                   0         23938.0      105   \n",
       "382150   27                1                   1        336395.0      144   \n",
       "382151   45                1                   0         40443.0      187   \n",
       "382152   28                1                   1         25380.0      208   \n",
       "382153   29                1                   1         30396.0      104   \n",
       "\n",
       "        Response  Gender_Female  Gender_Male  Region_Code_0.0  \\\n",
       "0            0.0              0            1                0   \n",
       "1            0.0              0            1                0   \n",
       "2            0.0              1            0                0   \n",
       "3            0.0              1            0                0   \n",
       "4            0.0              0            1                0   \n",
       "...          ...            ...          ...              ...   \n",
       "382149       0.0              0            1                0   \n",
       "382150       0.0              0            1                0   \n",
       "382151       0.0              0            1                0   \n",
       "382152       0.0              1            0                0   \n",
       "382153       0.0              1            0                0   \n",
       "\n",
       "        Region_Code_1.0  ...  Policy_Sales_Channel_152.0  \\\n",
       "0                     0  ...                           1   \n",
       "1                     0  ...                           0   \n",
       "2                     0  ...                           0   \n",
       "3                     0  ...                           1   \n",
       "4                     0  ...                           1   \n",
       "...                 ...  ...                         ...   \n",
       "382149                0  ...                           1   \n",
       "382150                0  ...                           1   \n",
       "382151                0  ...                           0   \n",
       "382152                0  ...                           1   \n",
       "382153                0  ...                           1   \n",
       "\n",
       "        Policy_Sales_Channel_153.0  Policy_Sales_Channel_154.0  \\\n",
       "0                                0                           0   \n",
       "1                                0                           0   \n",
       "2                                0                           0   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "...                            ...                         ...   \n",
       "382149                           0                           0   \n",
       "382150                           0                           0   \n",
       "382151                           0                           0   \n",
       "382152                           0                           0   \n",
       "382153                           0                           0   \n",
       "\n",
       "        Policy_Sales_Channel_155.0  Policy_Sales_Channel_156.0  \\\n",
       "0                                0                           0   \n",
       "1                                0                           0   \n",
       "2                                0                           0   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "...                            ...                         ...   \n",
       "382149                           0                           0   \n",
       "382150                           0                           0   \n",
       "382151                           0                           0   \n",
       "382152                           0                           0   \n",
       "382153                           0                           0   \n",
       "\n",
       "        Policy_Sales_Channel_157.0  Policy_Sales_Channel_158.0  \\\n",
       "0                                0                           0   \n",
       "1                                0                           0   \n",
       "2                                0                           0   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "...                            ...                         ...   \n",
       "382149                           0                           0   \n",
       "382150                           0                           0   \n",
       "382151                           0                           0   \n",
       "382152                           0                           0   \n",
       "382153                           0                           0   \n",
       "\n",
       "        Policy_Sales_Channel_159.0  Policy_Sales_Channel_160.0  \\\n",
       "0                                0                           0   \n",
       "1                                0                           0   \n",
       "2                                0                           0   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "...                            ...                         ...   \n",
       "382149                           0                           0   \n",
       "382150                           0                           0   \n",
       "382151                           0                           0   \n",
       "382152                           0                           0   \n",
       "382153                           0                           0   \n",
       "\n",
       "        Policy_Sales_Channel_163.0  \n",
       "0                                0  \n",
       "1                                0  \n",
       "2                                0  \n",
       "3                                0  \n",
       "4                                0  \n",
       "...                            ...  \n",
       "382149                           0  \n",
       "382150                           0  \n",
       "382151                           0  \n",
       "382152                           0  \n",
       "382153                           0  \n",
       "\n",
       "[382154 rows x 222 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7de6df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381431"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_data['Driving_License'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f8fdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id attribute since as it is irrelevant for training\n",
    "train_data = train_data.drop('id', axis=1)\n",
    "test_data = test_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69806cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('Response', axis=1)\n",
    "y_train = train_data['Response']\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "977bf32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    319553\n",
       "1.0     62601\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n",
    "# 16% ones; not an extreme imbalance, but sufficient enough that we run into issues with standard techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3264f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to np arrays to be able to use training functions\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35522c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_cross_val function from hw1; modified one is in cell below \n",
    "def my_cross_val(model, loss_func, X, y, k=10):\n",
    "    (n, d) = X.shape\n",
    "    validation_losses = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        val_set = X[round(i*n/k):round((i+1)*n/k), :]\n",
    "        val_labels = y[round(i*n/k):round((i+1)*n/k)]\n",
    "        train_set = np.delete(X, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        train_labels = np.delete(y, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        model.fit(train_set, train_labels)\n",
    "        y_preds = model.predict(val_set)\n",
    "        \n",
    "        val_loss = 0\n",
    "        for j in range(len(y_preds)):\n",
    "            if loss_func == 'mse':\n",
    "                val_loss += (val_labels[j] - y_preds[j])**2\n",
    "            if loss_func == 'err_rate' and (val_labels[j] != y_preds[j]):\n",
    "                val_loss += 1\n",
    "        val_loss *= (1/len(y_preds))\n",
    "        validation_losses[i] = val_loss\n",
    "    return validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51f56340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: choice of evaluation metric (f1, precision, recall, etc.)\n",
    "# Proportion: proportion of test set to predict as 1s, if needed (logistic regression may predict all 0 by default)\n",
    "def my_cross_val_imbalanced(model, metric, proportion, X, y, k=10):\n",
    "    (n, d) = X.shape\n",
    "    validation_metrics = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        val_set = X[round(i*n/k):round((i+1)*n/k), :]\n",
    "        val_labels = y[round(i*n/k):round((i+1)*n/k)]\n",
    "        train_set = np.delete(X, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        train_labels = np.delete(y, [j for j in range(round(i*n/k), round((i+1)*n/k))], 0)\n",
    "        model.fit(train_set, train_labels)\n",
    "        if proportion == None:\n",
    "            y_preds = model.predict(val_set)\n",
    "        else:\n",
    "            y_preds = model.predict_proportion(val_set, proportion)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        score = 0\n",
    "        for j in range(len(y_preds)):\n",
    "            if val_labels[j] == 1 and y_preds[j] == 1:\n",
    "                tp += 1\n",
    "            elif val_labels[j] == 1:\n",
    "                fn += 1\n",
    "            elif y_preds[j] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        if tp == 0: # to avoid division by zero error for trivial models\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)  \n",
    "        if metric == 'precision':\n",
    "            score = precision\n",
    "        if metric == 'recall':\n",
    "            score = recall\n",
    "        if metric == 'f1':\n",
    "            if precision + recall == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = 2 * precision * recall / (precision + recall)\n",
    "        #if metric == 'auprc':\n",
    "            #score = sklearn.metrics.average_precision_score(val_labels, y_preds)\n",
    "        validation_metrics[i] = score\n",
    "    return validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d853012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "def L(y, x, w, weight): # weighted log loss function\n",
    "    return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
    "\n",
    "class MyWeightedLogisticRegression:\n",
    "\n",
    "    def __init__(self, d, max_iters, eta_val, weight):\n",
    "        self.w = np.zeros(d)\n",
    "        self.w_old = np.random.uniform(-0.01, 0.01, d)\n",
    "        self.w_sum = np.zeros(d)\n",
    "        self.w_sum += self.w_old\n",
    "        self.max_iters = max_iters\n",
    "        self.eta_val = eta_val\n",
    "        self.weight = weight\n",
    "        self.iters = 0 # keep track of iterations made\n",
    "        self.losses = [] # used to keep track of losses for plotting purposes\n",
    "        self.gradient_magnitudes = [] \n",
    "    def fit(self, X, y):\n",
    "        (n, d) = X.shape\n",
    "        while self.iters < self.max_iters:\n",
    "            i = np.random.randint(n)\n",
    "            z = sigmoid(X @ self.w_old)\n",
    "            self.losses.append(L(z[i], X[i, :], self.w_old, self.weight))\n",
    "            gradient_magnitude = 0\n",
    "            for j in range(d):\n",
    "                gradient_j = - (y[i]*X[i, j]*(self.weight*sigmoid(-self.w_old @ X[i, :]) + sigmoid(self.w_old @ X[i, :])) - X[i, j] * sigmoid(self.w_old @ X[i, :]))\n",
    "                self.w[j] = self.w_old[j] - self.eta_val*gradient_j\n",
    "                gradient_magnitude += gradient_j**2\n",
    "                self.w_old[j] = self.w[j]\n",
    "            self.gradient_magnitudes.append(gradient_magnitude)\n",
    "            self.w_sum += self.w\n",
    "            self.iters += 1\n",
    "            if np.average(self.gradient_magnitudes[-10:]) < 1e-7:\n",
    "                break\n",
    "    def predict(self, X):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        return np.round(sigmoid(X @ w_avg))\n",
    "    def predict_values(self, X):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        return X @ w_avg\n",
    "    def predict_proportion(self, X, prop): #predict a certain number of 1s\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        #probs = sigmoid(X @ w_avg)\n",
    "        #print(probs)\n",
    "        values = X @ w_avg\n",
    "        threshold = np.quantile(values, 1-prop)\n",
    "        print(\"Threshold: \", threshold)\n",
    "        preds = np.zeros(len(values))\n",
    "        for i in range(len(preds)):\n",
    "            if values[i] >= threshold:\n",
    "                preds[i] = 1\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "596c3c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\363175868.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\363175868.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\363175868.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.001\n",
      "Error rate for fold 0: 0.16456888656286797\n",
      "Error rate for fold 1: 0.16432907682646014\n",
      "Error rate for fold 2: 0.16218762266125866\n",
      "Error rate for fold 3: 0.16354406531295793\n",
      "Error rate for fold 4: 0.16318199659819443\n",
      "Error rate for fold 5: 0.1655109250294387\n",
      "Error rate for fold 6: 0.16480008373456145\n",
      "Error rate for fold 7: 0.1616904356927908\n",
      "Error rate for fold 8: 0.1648524178354616\n",
      "Error rate for fold 9: 0.16344367395001963\n",
      "Mean validation error rate: 0.16381091842040113\n",
      "Validation error rate stdev: 0.0011631574794504068\n",
      "Eta: 0.01\n",
      "Error rate for fold 0: 0.8354311134371321\n",
      "Error rate for fold 1: 0.16432907682646014\n",
      "Error rate for fold 2: 0.16218762266125866\n",
      "Error rate for fold 3: 0.16354406531295793\n",
      "Error rate for fold 4: 0.16318199659819443\n",
      "Error rate for fold 5: 0.1655109250294387\n",
      "Error rate for fold 6: 0.16480008373456145\n",
      "Error rate for fold 7: 0.1616904356927908\n",
      "Error rate for fold 8: 0.1648524178354616\n",
      "Error rate for fold 9: 0.16344367395001963\n",
      "Mean validation error rate: 0.23089714110782755\n",
      "Validation error rate stdev: 0.2015145226645358\n",
      "Eta: 0.1\n",
      "Error rate for fold 0: 0.748920580923721\n",
      "Error rate for fold 1: 0.16432907682646014\n",
      "Error rate for fold 2: 0.16218762266125866\n",
      "Error rate for fold 3: 0.16354406531295793\n",
      "Error rate for fold 4: 0.16318199659819443\n",
      "Error rate for fold 5: 0.1655109250294387\n",
      "Error rate for fold 6: 0.16480008373456145\n",
      "Error rate for fold 7: 0.1616904356927908\n",
      "Error rate for fold 8: 0.1648524178354616\n",
      "Error rate for fold 9: 0.16344367395001963\n",
      "Mean validation error rate: 0.22224608785648642\n",
      "Validation error rate stdev: 0.1755618357508328\n",
      "Best eta value: 0.001\n",
      "Number of positive predictions:  0.0\n",
      "Test error rate: 0.1637601727287826\n"
     ]
    }
   ],
   "source": [
    "#Demonstration of standard LR with error rate predicting everything as 0\n",
    "\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "\n",
    "best_loss = np.inf\n",
    "best_eta = 0\n",
    "\n",
    "(_, num_features) = X_train.shape\n",
    "\n",
    "# Logistic Regression\n",
    "for eta_val in eta_vals:\n",
    "\n",
    "    # instantiate logistic regression object\n",
    "    lr = MyWeightedLogisticRegression(num_features, 100000, eta_val, 1)\n",
    "\n",
    "    # call to your CV function to compute error rates for each fold\n",
    "    cv_losses = my_cross_val(lr, 'err_rate', X_train, y_train, k=10)\n",
    "\n",
    "    # print error rates from CV\n",
    "    print(\"Eta: \" + str(eta_val))\n",
    "    for i in range(10):\n",
    "        print(\"Error rate for fold \" + str(i) + \": \" + str(cv_losses[i]))\n",
    "    mean_loss = sum(cv_losses)/len(cv_losses)\n",
    "    print(\"Mean validation error rate: \" + str(mean_loss))\n",
    "    print(\"Validation error rate stdev: \" + str(np.std(cv_losses)))\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        best_eta = eta_val\n",
    "\n",
    "# instantiate logistic regression object for best value of eta\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "best_lr = MyWeightedLogisticRegression(num_features, 100000, best_eta, 1)\n",
    "\n",
    "# fit model using all training data\n",
    "best_lr.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_preds = best_lr.predict(X_test)\n",
    "print(\"Number of positive predictions: \", sum(y_preds))\n",
    "\n",
    "# compute error rate on test data\n",
    "errors = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_preds[i] != y_test[i]:\n",
    "        errors += 1\n",
    "error_rate = errors/len(y_test)\n",
    "\n",
    "# print error rate on test data\n",
    "print(\"Test error rate: \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d184a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(preds, truth):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for j in range(len(preds)):\n",
    "        if truth[j] == 1 and preds[j] == 1:\n",
    "            tp += 1\n",
    "        elif truth[j] == 1:\n",
    "            fn += 1\n",
    "        elif preds[j] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else: \n",
    "        f1 = 0\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d0a93f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\3563418974.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\3563418974.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\3563418974.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.001\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0\n",
      "F1 score for fold 9: 0.0\n",
      "Mean validation F1 score: 0.0\n",
      "Validation F1 score stdev: 0.0\n",
      "Eta: 0.001\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0\n",
      "F1 score for fold 9: 0.0\n",
      "Mean validation F1 score: 0.0\n",
      "Validation F1 score stdev: 0.0\n",
      "Eta: 0.001\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.16899407302802355\n",
      "Validation F1 score stdev: 0.13799092331721363\n",
      "Eta: 0.001\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.001\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.001\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.01\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0\n",
      "F1 score for fold 9: 0.0\n",
      "Mean validation F1 score: 0.0\n",
      "Validation F1 score stdev: 0.0\n",
      "Eta: 0.01\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.25324344225151457\n",
      "Validation F1 score stdev: 0.08443114057031724\n",
      "Eta: 0.01\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.01\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.16899407302802355\n",
      "Validation F1 score stdev: 0.13799092331721363\n",
      "Eta: 0.01\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.01\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.1\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.0\n",
      "F1 score for fold 9: 0.0\n",
      "Mean validation F1 score: 0.0\n",
      "Validation F1 score stdev: 0.0\n",
      "Eta: 0.1\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.1\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.0\n",
      "F1 score for fold 2: 0.0\n",
      "F1 score for fold 3: 0.0\n",
      "F1 score for fold 4: 0.0\n",
      "F1 score for fold 5: 0.0\n",
      "F1 score for fold 6: 0.0\n",
      "F1 score for fold 7: 0.0\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.056400963908982374\n",
      "Validation F1 score stdev: 0.11280288570255381\n",
      "Eta: 0.1\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Eta: 0.1\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.0\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.25324344225151457\n",
      "Validation F1 score stdev: 0.08443114057031724\n",
      "Eta: 0.1\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.28262628078374974\n",
      "F1 score for fold 1: 0.2822725638259619\n",
      "F1 score for fold 2: 0.27910746853398777\n",
      "F1 score for fold 3: 0.2811136598749607\n",
      "F1 score for fold 4: 0.2805786146543385\n",
      "F1 score for fold 5: 0.2840143691064212\n",
      "F1 score for fold 6: 0.2829671564002336\n",
      "F1 score for fold 7: 0.2783709510294184\n",
      "F1 score for fold 8: 0.2830442986791266\n",
      "F1 score for fold 9: 0.28096534041069704\n",
      "Mean validation F1 score: 0.28150607032988956\n",
      "Validation F1 score stdev: 0.0017182455556877898\n",
      "Best eta value: 0.1\n",
      "Best weight value: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predictions:  78273.0\n",
      "Test precision: 0.1637601727287826\n",
      "Test recall: 1.0\n",
      "Test F1 score: 0.28143285286142433\n",
      "Test AUPRC score: 0.1637601727287826\n"
     ]
    }
   ],
   "source": [
    "# Weighted Logistic Regression\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "weight_vals = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "best_score = 0\n",
    "best_eta = 0\n",
    "best_weight = 0\n",
    "\n",
    "(_, num_features) = X_train.shape\n",
    "proportion = sum(y_train)/len(y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "for eta_val in eta_vals:\n",
    "    for weight in weight_vals:\n",
    "\n",
    "        # instantiate logistic regression object\n",
    "        lr = MyWeightedLogisticRegression(num_features, 1000000, eta_val, weight)\n",
    "\n",
    "        # call to your CV function to compute error rates for each fold\n",
    "        #cv_scores = my_cross_val_imbalanced(lr, 'auprc', proportion, X_train, y_train, k=10)\n",
    "        cv_scores = my_cross_val_imbalanced(lr, 'f1', None, X_train, y_train, k=10)\n",
    "\n",
    "        # print error rates from CV\n",
    "        print(\"Eta: \" + str(eta_val))\n",
    "        print(\"Weight: \" + str(weight))\n",
    "        for i in range(10):\n",
    "            print(\"F1 score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "        mean_score = sum(cv_scores)/len(cv_scores)\n",
    "        print(\"Mean validation F1 score: \" + str(mean_score))\n",
    "        print(\"Validation F1 score stdev: \" + str(np.std(cv_scores)))\n",
    "        if mean_score >= best_score:\n",
    "            best_score = mean_score\n",
    "            best_eta = eta_val\n",
    "            best_weight = weight\n",
    "\n",
    "# instantiate logistic regression object for best value of eta\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "print(\"Best weight value: \" + str(best_weight))\n",
    "best_lr = MyWeightedLogisticRegression(num_features, 1000000, best_eta, best_weight)\n",
    "\n",
    "# fit model using all training data\n",
    "best_lr.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "#y_preds = best_lr.predict_proportion(X_test, proportion)\n",
    "y_preds = best_lr.predict(X_test)\n",
    "print(\"Number of positive predictions: \", sum(y_preds))\n",
    "y_probs = best_lr.predict_probs(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_probs)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc364351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\617410271.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\617410271.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n",
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_28780\\617410271.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return - (weight * y * np.log(sigmoid(w @ x)) + (1 - y)*np.log(sigmoid(-w @ x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  -214009.4106393658\n",
      "Threshold:  -40586.301920185346\n",
      "Threshold:  -239197.73995608123\n",
      "Threshold:  -221517.50015097504\n",
      "Threshold:  -41800.79986000931\n",
      "Threshold:  -252039.78656123648\n",
      "Threshold:  -42257.837284848116\n",
      "Threshold:  -42143.95139308479\n",
      "Threshold:  -42029.94082962953\n",
      "Threshold:  -41920.06787736678\n",
      "Eta: 0.001\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.18020891475958856\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.16821382842509602\n",
      "F1 score for fold 5: 0.1797377830750894\n",
      "F1 score for fold 6: 0.16593677840592405\n",
      "F1 score for fold 7: 0.1726826915346893\n",
      "F1 score for fold 8: 0.1832656635618183\n",
      "F1 score for fold 9: 0.16967855429393888\n",
      "Mean validation F1 score: 0.17320856247475752\n",
      "Validation F1 score stdev: 0.006656314459749049\n",
      "Threshold:  -126.9304110414535\n",
      "Threshold:  -21.30366172705752\n",
      "Threshold:  -115.55323261666506\n",
      "Threshold:  449039.57248244726\n",
      "Threshold:  417841.77477793756\n",
      "Threshold:  394490.48991183256\n",
      "Threshold:  372234.40910826693\n",
      "Threshold:  352027.2577622246\n",
      "Threshold:  331926.67815868853\n",
      "Threshold:  314909.40730351565\n",
      "Eta: 0.001\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.1800494378438721\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.19426705071526207\n",
      "Validation F1 score stdev: 0.012224376488457733\n",
      "Threshold:  -29.2222990927416\n",
      "Threshold:  -6.100619883743877\n",
      "Threshold:  -21.580899661417057\n",
      "Threshold:  -18.297920206864422\n",
      "Threshold:  1381236.1766838408\n",
      "Threshold:  1335512.1740579477\n",
      "Threshold:  1289065.3402589813\n",
      "Threshold:  1245768.6169096387\n",
      "Threshold:  1199256.3286145579\n",
      "Threshold:  1160705.0977299353\n",
      "Eta: 0.001\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.17988996092815562\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.19035053552309095\n",
      "Validation F1 score stdev: 0.015029493670069909\n",
      "Threshold:  -89.69753990811768\n",
      "Threshold:  -15.241361488864106\n",
      "Threshold:  -65.63477115450371\n",
      "Threshold:  -56.42734665705565\n",
      "Threshold:  -12.22538645471763\n",
      "Threshold:  -59.183202286514515\n",
      "Threshold:  -11.527012368619673\n",
      "Threshold:  -11.417934507132854\n",
      "Threshold:  -11.245529854038494\n",
      "Threshold:  -11.15092153099674\n",
      "Eta: 0.001\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.1794115301810063\n",
      "F1 score for fold 2: 0.16936908010916682\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.16821382842509602\n",
      "F1 score for fold 5: 0.1797377830750894\n",
      "F1 score for fold 6: 0.1678477585795047\n",
      "F1 score for fold 7: 0.1726826915346893\n",
      "F1 score for fold 8: 0.18262877159461827\n",
      "F1 score for fold 9: 0.1695186310570926\n",
      "Mean validation F1 score: 0.17322418657261013\n",
      "Validation F1 score stdev: 0.006311503221412165\n",
      "Threshold:  13661079.855669647\n",
      "Threshold:  13666450.255576767\n",
      "Threshold:  13622508.464717569\n",
      "Threshold:  13669725.392103676\n",
      "Threshold:  13517155.467514032\n",
      "Threshold:  13527909.024673104\n",
      "Threshold:  13502367.148860725\n",
      "Threshold:  13481913.885751357\n",
      "Threshold:  13398558.806042185\n",
      "Threshold:  13376747.650640525\n",
      "Eta: 0.001\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.19953781177783092\n",
      "F1 score for fold 1: 0.20062195997129417\n",
      "F1 score for fold 2: 0.20067426553218815\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20320089179074768\n",
      "F1 score for fold 7: 0.20242784789774096\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20182312490004797\n",
      "Mean validation F1 score: 0.20143085931252483\n",
      "Validation F1 score stdev: 0.00262372232365645\n",
      "Threshold:  -174.63656846359973\n",
      "Threshold:  -28.326062131475613\n",
      "Threshold:  -127.26601715503718\n",
      "Threshold:  -109.99858577925674\n",
      "Threshold:  20074366.062077407\n",
      "Threshold:  19802528.34959956\n",
      "Threshold:  19488576.957706682\n",
      "Threshold:  19191066.447332546\n",
      "Threshold:  18812578.958375182\n",
      "Threshold:  18532782.322877727\n",
      "Eta: 0.001\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.17988996092815562\n",
      "F1 score for fold 2: 0.16936908010916682\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.19033448158184837\n",
      "Validation F1 score stdev: 0.015051794413429432\n",
      "Threshold:  -6009270.212379506\n",
      "Threshold:  -1124950.3481180253\n",
      "Threshold:  -6572259.80120057\n",
      "Threshold:  -6026015.5992843285\n",
      "Threshold:  -1124951.0969797485\n",
      "Threshold:  -6737218.117196716\n",
      "Threshold:  -1124947.3557927848\n",
      "Threshold:  -1124949.9295349417\n",
      "Threshold:  -1124948.929606884\n",
      "Threshold:  -1124949.922182812\n",
      "Eta: 0.01\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.180368391675305\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.1680537772087068\n",
      "F1 score for fold 5: 0.1797377830750894\n",
      "F1 score for fold 6: 0.16975873875308542\n",
      "F1 score for fold 7: 0.1734866146796366\n",
      "F1 score for fold 8: 0.1842210015126184\n",
      "F1 score for fold 9: 0.17031824724132416\n",
      "Mean validation F1 score: 0.17383059648371962\n",
      "Validation F1 score stdev: 0.00646469900612721\n",
      "Threshold:  -3871198.430575096\n",
      "Threshold:  -722996.8502426995\n",
      "Threshold:  -4214824.342382341\n",
      "Threshold:  -2471561.690851588\n",
      "Threshold:  -78762.7896901353\n",
      "Threshold:  -446160.8045945855\n",
      "Threshold:  -78629.61006973131\n",
      "Threshold:  -78724.86927473077\n",
      "Threshold:  -78683.12618777258\n",
      "Threshold:  -78713.46454623241\n",
      "Eta: 0.01\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.18020891475958856\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.1680537772087068\n",
      "F1 score for fold 5: 0.17957886372665874\n",
      "F1 score for fold 6: 0.1664145234493192\n",
      "F1 score for fold 7: 0.17236112227671035\n",
      "F1 score for fold 8: 0.18262877159461827\n",
      "F1 score for fold 9: 0.16919878458339996\n",
      "Mean validation F1 score: 0.17308061682904324\n",
      "Validation F1 score stdev: 0.006537661173841227\n",
      "Threshold:  -173.31643630307408\n",
      "Threshold:  -25.856622478287377\n",
      "Threshold:  -126.19269869259527\n",
      "Threshold:  -109.06188546404535\n",
      "Threshold:  12168720.360878458\n",
      "Threshold:  11281836.5253777\n",
      "Threshold:  10429837.090540309\n",
      "Threshold:  9641449.913927855\n",
      "Threshold:  8864514.42397812\n",
      "Threshold:  8179942.525643295\n",
      "Eta: 0.01\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.17957100709672275\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.19031864013994765\n",
      "Validation F1 score stdev: 0.015051980729693782\n",
      "Threshold:  -3978915.1477458705\n",
      "Threshold:  -744860.3643900836\n",
      "Threshold:  -4351689.027293229\n",
      "Threshold:  -3990005.659209927\n",
      "Threshold:  -744861.3497602902\n",
      "Threshold:  -4460914.155088498\n",
      "Threshold:  -744859.3803519438\n",
      "Threshold:  51304420.45806732\n",
      "Threshold:  50359355.64753118\n",
      "Threshold:  49673438.0922415\n",
      "Eta: 0.01\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.18020891475958856\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.16821382842509602\n",
      "F1 score for fold 5: 0.1797377830750894\n",
      "F1 score for fold 6: 0.16975873875308542\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.18095806748492416\n",
      "Validation F1 score stdev: 0.013555367375446402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  148259096.0142244\n",
      "Threshold:  146952311.23770183\n",
      "Threshold:  145148122.03584543\n",
      "Threshold:  144367507.32930458\n",
      "Threshold:  122023957.3426298\n",
      "Threshold:  121420694.37636822\n",
      "Threshold:  120505664.6042361\n",
      "Threshold:  119648105.46078281\n",
      "Threshold:  118238563.42640178\n",
      "Threshold:  117395852.9776015\n",
      "Eta: 0.01\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.19937843652880707\n",
      "F1 score for fold 1: 0.20062195997129417\n",
      "F1 score for fold 2: 0.2008348049446139\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.2014151370332995\n",
      "Validation F1 score stdev: 0.0026259578668663677\n",
      "Threshold:  349380675.8119881\n",
      "Threshold:  348888648.4861126\n",
      "Threshold:  347136764.56227654\n",
      "Threshold:  347767516.9545917\n",
      "Threshold:  343248680.3425203\n",
      "Threshold:  342929012.65147966\n",
      "Threshold:  341702881.56902367\n",
      "Threshold:  340609810.14756155\n",
      "Threshold:  337905519.9825064\n",
      "Threshold:  300050901.1038999\n",
      "Eta: 0.01\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.19937843652880707\n",
      "F1 score for fold 1: 0.20062195997129417\n",
      "F1 score for fold 2: 0.20067426553218815\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.2013990830920569\n",
      "Validation F1 score stdev: 0.0026299443937710973\n",
      "Threshold:  -21295077.307857778\n",
      "Threshold:  -4167590.64072207\n",
      "Threshold:  -25322760.486264266\n",
      "Threshold:  -24042384.955578227\n",
      "Threshold:  -4630649.647210618\n",
      "Threshold:  -28525650.867474385\n",
      "Threshold:  -4886132.564902662\n",
      "Threshold:  -5001100.094522396\n",
      "Threshold:  -5108650.190214858\n",
      "Threshold:  -4461403.223281037\n",
      "Eta: 0.1\n",
      "Weight: 1\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.18020891475958856\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.16821382842509602\n",
      "F1 score for fold 5: 0.1797377830750894\n",
      "F1 score for fold 6: 0.17151047057886773\n",
      "F1 score for fold 7: 0.1734866146796366\n",
      "F1 score for fold 8: 0.1837433325372184\n",
      "F1 score for fold 9: 0.16967855429393888\n",
      "Mean validation F1 score: 0.1738940909040866\n",
      "Validation F1 score stdev: 0.006308781759707633\n",
      "Threshold:  -13600317.74184491\n",
      "Threshold:  -2551168.043379481\n",
      "Threshold:  -9101040.003851801\n",
      "Threshold:  -8632313.523533653\n",
      "Threshold:  -1709482.058716711\n",
      "Threshold:  -10185974.835694352\n",
      "Threshold:  -1801480.4039182905\n",
      "Threshold:  -1848736.9606539235\n",
      "Threshold:  -2037735.0315176873\n",
      "Threshold:  -2076058.9340224217\n",
      "Eta: 0.1\n",
      "Weight: 2\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.17957100709672275\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.16821382842509602\n",
      "F1 score for fold 5: 0.17957886372665874\n",
      "F1 score for fold 6: 0.166733020144916\n",
      "F1 score for fold 7: 0.17236112227671035\n",
      "F1 score for fold 8: 0.18262877159461827\n",
      "F1 score for fold 9: 0.16935870782024628\n",
      "Mean validation F1 score: 0.1730806731776399\n",
      "Validation F1 score stdev: 0.006416995802178245\n",
      "Threshold:  244838726.58178493\n",
      "Threshold:  242627470.0778717\n",
      "Threshold:  239580312.2137885\n",
      "Threshold:  238195617.36002755\n",
      "Threshold:  233364042.05768812\n",
      "Threshold:  196414178.77237865\n",
      "Threshold:  193304941.69007373\n",
      "Threshold:  181592002.69394642\n",
      "Threshold:  179964957.193181\n",
      "Threshold:  195668962.86493355\n",
      "Eta: 0.1\n",
      "Weight: 5\n",
      "F1 score for fold 0: 0.19937843652880707\n",
      "F1 score for fold 1: 0.20062195997129417\n",
      "F1 score for fold 2: 0.20067426553218815\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20242784789774096\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20182312490004797\n",
      "Mean validation F1 score: 0.20139899695284255\n",
      "Validation F1 score stdev: 0.002625251551228251\n",
      "Threshold:  -92.96522314634201\n",
      "Threshold:  879674511.4124715\n",
      "Threshold:  873118009.5867873\n",
      "Threshold:  872590629.0263689\n",
      "Threshold:  859189439.5475016\n",
      "Threshold:  778901302.8630385\n",
      "Threshold:  776349763.6664071\n",
      "Threshold:  774120974.047868\n",
      "Threshold:  768261932.0691864\n",
      "Threshold:  755089009.6251172\n",
      "Eta: 0.1\n",
      "Weight: 10\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.20062195997129417\n",
      "F1 score for fold 2: 0.2008348049446139\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20182312490004797\n",
      "Mean validation F1 score: 0.19947081379399104\n",
      "Validation F1 score stdev: 0.00703841431864279\n",
      "Threshold:  1365743012.818246\n",
      "Threshold:  1364167968.1413577\n",
      "Threshold:  1357659159.8653097\n",
      "Threshold:  1360357244.6759815\n",
      "Threshold:  1428166415.9142487\n",
      "Threshold:  1428893714.1263943\n",
      "Threshold:  1425810632.606422\n",
      "Threshold:  1423286796.1815796\n",
      "Threshold:  1385388270.7442505\n",
      "Threshold:  1383492104.272519\n",
      "Eta: 0.1\n",
      "Weight: 20\n",
      "F1 score for fold 0: 0.19937843652880707\n",
      "F1 score for fold 1: 0.20062195997129417\n",
      "F1 score for fold 2: 0.20067426553218815\n",
      "F1 score for fold 3: 0.20206218527695627\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20242784789774096\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20182312490004797\n",
      "Mean validation F1 score: 0.20139899695284255\n",
      "Validation F1 score stdev: 0.002625251551228251\n",
      "Threshold:  -193.10510978546742\n",
      "Threshold:  -31.07751530078176\n",
      "Threshold:  -140.88122711046807\n",
      "Threshold:  -121.53968081701173\n",
      "Threshold:  2801418080.2242475\n",
      "Threshold:  2788332821.729689\n",
      "Threshold:  2691529161.0739255\n",
      "Threshold:  2681094910.5442266\n",
      "Threshold:  2657993695.9963107\n",
      "Threshold:  2647459083.0947256\n",
      "Eta: 0.1\n",
      "Weight: 50\n",
      "F1 score for fold 0: 0.1797752808988764\n",
      "F1 score for fold 1: 0.18020891475958856\n",
      "F1 score for fold 2: 0.16952961952159254\n",
      "F1 score for fold 3: 0.16305651027096155\n",
      "F1 score for fold 4: 0.20406530089628683\n",
      "F1 score for fold 5: 0.20484704012713548\n",
      "F1 score for fold 6: 0.20304164344294928\n",
      "F1 score for fold 7: 0.20258863252673046\n",
      "F1 score for fold 8: 0.1950481649550195\n",
      "F1 score for fold 9: 0.20166320166320167\n",
      "Mean validation F1 score: 0.19038243090623425\n",
      "Validation F1 score stdev: 0.015007583008221385\n",
      "Best eta value: 0.001\n",
      "Best weight value: 20\n",
      "Threshold:  14605130.140532887\n",
      "Number of positive predictions:  12822.0\n",
      "Test precision: 0.20488223366089534\n",
      "Test recall: 0.20494616944921204\n",
      "Test F1 score: 0.20491419656786272\n",
      "Test AUPRC score: 0.18682104260677937\n"
     ]
    }
   ],
   "source": [
    "# Weighted Logistic Regression with predicting fixed proportion of 1s \n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "weight_vals = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "best_score = 0\n",
    "best_eta = 0\n",
    "best_weight = 0\n",
    "\n",
    "(_, num_features) = X_train.shape\n",
    "proportion = sum(y_train)/len(y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "for eta_val in eta_vals:\n",
    "    for weight in weight_vals:\n",
    "\n",
    "        # instantiate logistic regression object\n",
    "        lr = MyWeightedLogisticRegression(num_features, 10000000, eta_val, weight)\n",
    "\n",
    "        # call to your CV function to compute error rates for each fold\n",
    "        cv_scores = my_cross_val_imbalanced(lr, 'f1', proportion, X_train, y_train, k=10)\n",
    "        #cv_scores = my_cross_val_imbalanced(lr, 'auprc', None, X_train, y_train, k=10)\n",
    "\n",
    "        # print error rates from CV\n",
    "        print(\"Eta: \" + str(eta_val))\n",
    "        print(\"Weight: \" + str(weight))\n",
    "        for i in range(10):\n",
    "            print(\"F1 score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "        mean_score = sum(cv_scores)/len(cv_scores)\n",
    "        print(\"Mean validation F1 score: \" + str(mean_score))\n",
    "        print(\"Validation F1 score stdev: \" + str(np.std(cv_scores)))\n",
    "        if mean_score >= best_score:\n",
    "            best_score = mean_score\n",
    "            best_eta = eta_val\n",
    "            best_weight = weight\n",
    "\n",
    "# instantiate logistic regression object for best value of eta\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "print(\"Best weight value: \" + str(best_weight))\n",
    "best_lr = MyWeightedLogisticRegression(num_features, 10000000, best_eta, best_weight)\n",
    "\n",
    "# fit model using all training data\n",
    "best_lr.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_preds = best_lr.predict_proportion(X_test, proportion)\n",
    "#y_preds = best_lr.predict(X_test)\n",
    "print(\"Number of positive predictions: \", sum(y_preds))\n",
    "y_values = best_lr.predict_values(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_values)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0df024bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display = sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_values, name=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fda15bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVM:\n",
    "\n",
    "    def __init__(self, d, max_iters, eta_val, c):\n",
    "        self.w = np.zeros(d)\n",
    "        self.w_old = np.random.uniform(-0.01, 0.01, d)\n",
    "        self.w_sum = np.zeros(d)\n",
    "        self.w_sum += self.w_old\n",
    "        self.max_iters = max_iters\n",
    "        self.eta_val = eta_val\n",
    "        self.c = c\n",
    "        self.iters = 0\n",
    "        self.losses = []\n",
    "        self.gradient_magnitudes = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        (n, d) = X.shape\n",
    "        while self.iters < self.max_iters:\n",
    "            i = np.random.randint(n)\n",
    "            # compute loss; maybe modify for weighted?\n",
    "            loss = (1/2)*np.linalg.norm(self.w_old)**2 + self.c*max(0, 1 - y[i]*(self.w_old @ X[i, :]))\n",
    "            self.losses.append(loss)\n",
    "            gradient_magnitude = 0\n",
    "            for j in range(d):\n",
    "                if y[i]*(self.w_old @ X[i, :]) < 1:\n",
    "                    self.w[j] = self.w_old[j] - self.eta_val*(self.w_old[j] - self.c*y[i]*X[i,j])\n",
    "                    gradient_magnitude += (self.w_old[j] - self.c*y[i]*X[i,j])**2\n",
    "                else:\n",
    "                    self.w[j] = self.w_old[j] - self.eta_val*(self.w_old[j])\n",
    "                    gradient_magnitude += (self.w_old[j])**2\n",
    "                self.w_old[j] = self.w[j]\n",
    "            self.gradient_magnitudes.append(gradient_magnitude)\n",
    "            self.w_sum += self.w\n",
    "            self.iters += 1\n",
    "            if np.average(self.gradient_magnitudes[-10:]) < 1e-6:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        return np.sign(X @ w_avg)\n",
    "    \n",
    "    def predict_proportion(self, X, prop):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        values = X @ w_avg\n",
    "        print(values)\n",
    "        threshold = np.quantile(values, 1-prop)\n",
    "        print(\"Threshold: \", threshold)\n",
    "        preds = np.zeros(len(values))\n",
    "        for i in range(len(preds)):\n",
    "            if values[i] >= threshold:\n",
    "                preds[i] = 1\n",
    "            else:\n",
    "                preds[i] = -1 # not having this ruined it???\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202580ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWeightedSVM:\n",
    "\n",
    "    def __init__(self, d, max_iters, eta_val, c, w1):\n",
    "        self.w = np.zeros(d)\n",
    "        self.w_old = np.random.uniform(-0.01, 0.01, d)\n",
    "        self.w_sum = np.zeros(d)\n",
    "        self.w_sum += self.w_old\n",
    "        self.max_iters = max_iters\n",
    "        self.eta_val = eta_val\n",
    "        self.c = c\n",
    "        self.iters = 0\n",
    "        self.losses = []\n",
    "        self.gradient_magnitudes = []\n",
    "        self.w1 = w1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        (n, d) = X.shape\n",
    "        while self.iters < self.max_iters:\n",
    "            i = np.random.randint(n)\n",
    "            # compute loss; maybe modify for weighted?\n",
    "            loss = (1/2)*np.linalg.norm(self.w_old)**2 + self.c*max(0, 1 - y[i]*(self.w_old @ X[i, :]))\n",
    "            self.losses.append(loss)\n",
    "            gradient_magnitude = 0\n",
    "            for j in range(d):\n",
    "                if y[i]*(self.w_old @ X[i, :]) < 1:\n",
    "                    self.w[j] = self.w_old[j] - self.eta_val*(self.w_old[j] - self.c*y[i]*X[i,j])\n",
    "                    gradient_magnitude += (self.w_old[j] - self.c*y[i]*X[i,j])**2\n",
    "                else:\n",
    "                    self.w[j] = self.w_old[j] - self.eta_val*(self.w_old[j])\n",
    "                    gradient_magnitude += (self.w_old[j])**2\n",
    "                self.w_old[j] = self.w[j]\n",
    "            self.gradient_magnitudes.append(gradient_magnitude)\n",
    "            self.w_sum += self.w\n",
    "            self.iters += 1\n",
    "            if np.average(self.gradient_magnitudes[-10:]) < 1e-6:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        return np.sign(X @ w_avg)\n",
    "    \n",
    "    def predict_proportion(self, X, prop):\n",
    "        w_avg = self.w_sum / self.iters\n",
    "        values = X @ w_avg\n",
    "        print(values)\n",
    "        threshold = np.quantile(values, 1-prop)\n",
    "        print(\"Threshold: \", threshold)\n",
    "        preds = np.zeros(len(values))\n",
    "        for i in range(len(preds)):\n",
    "            if values[i] >= threshold:\n",
    "                preds[i] = 1\n",
    "            else:\n",
    "                preds[i] = -1 # not having this ruined it???\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ca3f4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 1e-05\n",
      "C: 1\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "AUPRC score for fold 0: 0.16456888656286797\n",
      "AUPRC score for fold 1: 0.1643290768264601\n",
      "AUPRC score for fold 2: 0.16218762266125866\n",
      "AUPRC score for fold 3: 0.16354406531295793\n",
      "AUPRC score for fold 4: 0.16318199659819443\n",
      "AUPRC score for fold 5: 0.1655109250294387\n",
      "AUPRC score for fold 6: 0.16480008373456145\n",
      "AUPRC score for fold 7: 0.1616904356927908\n",
      "AUPRC score for fold 8: 0.1648524178354616\n",
      "AUPRC score for fold 9: 0.16344367395001963\n",
      "Mean validation AUPRC score: 0.16381091842040113\n",
      "Validation AUPRC score stdev: 0.0011631574794504053\n",
      "Best eta value: 0.001\n",
      "Best C value: 100\n",
      "Test precision: 0.1637601727287826\n",
      "Test recall: 1.0\n",
      "Test F1 score: 0.28143285286142433\n",
      "Test AUPRC score: 0.1637601727287826\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "eta_vals = [0.00001, 0.0001, 0.001]\n",
    "C_vals = [1, 10, 100]\n",
    "best_score = 0\n",
    "best_eta, best_c = (0, 0)\n",
    "\n",
    "for eta_val in eta_vals:\n",
    "    for c_val in C_vals:\n",
    "\n",
    "        # instantiate svm object\n",
    "        svm = MySVM(num_features, 100000, eta_val, c_val)\n",
    "\n",
    "        # call to your CV function to compute error rates for each fold\n",
    "        #cv_scores = my_cross_val_imbalanced(svm, 'f1', proportion, X_train, y_train, k=10)\n",
    "        cv_scores = my_cross_val_imbalanced(svm, 'auprc', None, X_train, y_train, k=10)\n",
    "\n",
    "        # print error rates from CV\n",
    "        print(\"Eta: \" + str(eta_val))\n",
    "        print(\"C: \" + str(c_val))\n",
    "        for i in range(10):\n",
    "            print(\"AUPRC score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "        mean_score = sum(cv_scores)/len(cv_scores)\n",
    "        print(\"Mean validation AUPRC score: \" + str(mean_score))\n",
    "        print(\"Validation AUPRC score stdev: \" + str(np.std(cv_scores)))\n",
    "        if mean_score >= best_score:\n",
    "            best_score = mean_score\n",
    "            best_eta, best_c = (eta_val, c_val)\n",
    "\n",
    "# instantiate svm object for best value of eta and C\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "print(\"Best C value: \" + str(best_c))\n",
    "best_svm = MySVM(num_features, 100000, best_eta, best_c)\n",
    "\n",
    "# fit model using all training data\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "#y_preds = best_svm.predict_proportion(X_test, proportion)\n",
    "y_preds = best_svm.predict(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_preds)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fde249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 519.7323045  8564.16511868 7084.28862452 ...  519.51153389 6425.40159896\n",
      " 4708.48593585]\n",
      "Threshold:  8623.815823349525\n",
      "[8242.56461322 6928.36506666 6203.1995316  ...  519.61898471 4012.72574673\n",
      " 6050.41164306]\n",
      "Threshold:  8648.83071992823\n",
      "[6674.48430257 6418.57482983 5499.09741823 ... 8408.68559863 7013.3706258\n",
      "  519.6452891 ]\n",
      "Threshold:  8642.815839832976\n",
      "[7605.31790447 6922.3815539  8190.62793818 ... 5884.31988391 9807.0472893\n",
      " 6524.92295554]\n",
      "Threshold:  8695.873205418015\n",
      "[8406.99748308 9154.32372851 4847.13807359 ... 5629.69534703  519.64769601\n",
      " 5653.64034193]\n",
      "Threshold:  8620.216447723373\n",
      "[9737.43023933 7008.22890526  519.55688084 ... 5751.72500984 5113.93534258\n",
      " 9716.26359841]\n",
      "Threshold:  8648.813775592756\n",
      "[6139.28980729 6008.24919593 5878.42954668 ... 6093.30159753 5154.84942132\n",
      " 6639.74121256]\n",
      "Threshold:  8655.241198866835\n",
      "[ 7814.89596431  7347.70318489 10729.59840238 ...  8324.87166203\n",
      "  5172.2221726   5735.70128295]\n",
      "Threshold:  8664.041331850418\n",
      "[ 6188.14922012  7068.24689357 12054.99372921 ...  8170.39135158\n",
      "  7371.70897883   519.61014182]\n",
      "Threshold:  8631.84128682233\n",
      "[5399.02364975 7662.12432316 3895.84951538 ... 7994.07042505 5016.66317166\n",
      " 6008.1761557 ]\n",
      "Threshold:  8639.911371606642\n",
      "Eta: 1e-05\n",
      "C: 1\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.1697874038479599\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.1713918305042387\n",
      "Validation F1 score stdev: 0.0011348935138179442\n",
      "[  7742.81566474 127556.83466165 105517.63805359 ...   7742.76461353\n",
      "  95705.11039567  70130.19649752]\n",
      "Threshold:  128443.77804971527\n",
      "[122766.87084819 103191.90120432  92393.10595991 ...   7742.79527364\n",
      "  59767.14211698  90117.34675233]\n",
      "Threshold:  128816.90417542162\n",
      "[ 99411.71451942  95602.08164597  81906.37743475 ... 125239.8876781\n",
      " 104457.83988318   7742.80873769]\n",
      "Threshold:  128728.5688221623\n",
      "[113275.26215406 103103.5560193  121992.60433107 ...  87644.33204581\n",
      " 146069.01339091  97183.08660689]\n",
      "Threshold:  129516.99973239696\n",
      "[125216.30578817 136344.85243899  72196.89723624 ...  83849.48788124\n",
      "   7742.79865139  84208.63908809]\n",
      "Threshold:  128393.30218741235\n",
      "[145029.80263504 104384.18594519   7742.77503967 ...  85668.86487286\n",
      "  76168.46113052 144717.67818119]\n",
      "Threshold:  128818.02686640384\n",
      "[91439.24890328 89490.25093176 87558.91704563 ... 90756.20560068\n",
      " 76777.86600719 98893.59195129]\n",
      "Threshold:  128913.93810838586\n",
      "[116395.96548285 109439.13922422 159808.9093063  ... 123991.60324246\n",
      "  77036.9409666   85430.40739075]\n",
      "Threshold:  129044.34554168803\n",
      "[ 92169.34252357 105276.27930682 179551.69011225 ... 121692.28574384\n",
      " 109795.39781034   7742.79323023]\n",
      "Threshold:  128563.50852363992\n",
      "[ 80413.79587816 114120.21570272  58027.18948657 ... 119066.19670034\n",
      "  74719.98514098  89487.3244911 ]\n",
      "Threshold:  128685.15622138426\n",
      "Eta: 1e-05\n",
      "C: 10\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.1697874038479599\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.1713918305042387\n",
      "Validation F1 score stdev: 0.0011348935138179442\n",
      "[  88933.97222471 1465100.82127114 1211963.01395373 ...   88935.12330127\n",
      " 1099257.9746985   805506.34782715]\n",
      "Threshold:  1475287.2658099444\n",
      "[1410083.93317769 1185247.82559581 1061215.31585474 ...   88934.65616329\n",
      "  686477.62790439 1035076.29094009]\n",
      "Threshold:  1479574.0574211555\n",
      "[1141829.87131067 1098074.01904844  940766.57873695 ... 1438487.94187564\n",
      " 1199788.21958411   88934.48516231]\n",
      "Threshold:  1478559.3341707014\n",
      "[1301064.27932001 1184233.63280462 1401190.19924782 ... 1006672.00622715\n",
      " 1677729.46371647 1116231.621479  ]\n",
      "Threshold:  1487613.9333574157\n",
      "[1438218.1621042  1566037.79315562  829245.16920979 ...  963083.66836826\n",
      "   88934.51741684  967209.97946258]\n",
      "Threshold:  1474709.1897980485\n",
      "[1665791.86779522 1198943.94449427   88934.88986954 ...  983982.00342441\n",
      "  874860.64720469 1662208.66014071]\n",
      "Threshold:  1479587.3722795905\n",
      "[1050258.59350575 1027873.953194   1005692.14718832 ... 1042414.19797407\n",
      "  881860.42716864 1135878.12316497]\n",
      "Threshold:  1480688.5763536848\n",
      "[1336907.82877795 1257003.78359914 1835543.60016631 ... 1424150.37345593\n",
      "  884836.27520575  981243.14133268]\n",
      "Threshold:  1482186.0743272763\n",
      "[1058645.5588533  1209189.14501895 2062307.42535376 ... 1397741.61595292\n",
      " 1261094.80707948   88934.6788836 ]\n",
      "Threshold:  1476662.6063685766\n",
      "[ 923621.65023374 1310768.8104576   666493.4536289  ... 1367578.65557661\n",
      "  858223.82584147 1027839.54079272]\n",
      "Threshold:  1478060.7654490988\n",
      "Eta: 1e-05\n",
      "C: 100\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.16974911195636394\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.17138800131507909\n",
      "Validation F1 score stdev: 0.0011403519448976265\n",
      "[ 1126.78448947 18562.78867106 15355.52769002 ...  1126.7866619\n",
      " 13927.55626765 10205.7458649 ]\n",
      "Threshold:  18691.860399627207\n",
      "[17865.50792978 15016.87838291 13445.40048894 ...  1126.77221933\n",
      "  8697.54491997 13114.2216001 ]\n",
      "Threshold:  18745.93083527273\n",
      "[14466.59888656 13912.2191067  11919.18948269 ... 18225.16657227\n",
      " 15200.91614746  1126.75418479]\n",
      "Threshold:  18732.848729584355\n",
      "[16483.84993093 15003.66111799 17752.39792708 ... 12754.03305651\n",
      " 21256.0072013  14142.11180193]\n",
      "Threshold:  18847.345304218416\n",
      "[18221.29361723 19840.69639865 10505.99261421 ... 12201.64838152\n",
      "  1126.72713254 12253.9163846 ]\n",
      "Threshold:  18683.60740431836\n",
      "[21104.25871115 15189.65088902  1126.71472317 ... 12466.25955937\n",
      " 11083.78684758 21058.85203095]\n",
      "Threshold:  18745.183333537992\n",
      "[13305.7787744  13022.17504228 12741.14408177 ... 13206.38561785\n",
      " 11172.32936299 14390.4938942 ]\n",
      "Threshold:  18758.907696569142\n",
      "[16937.14595628 15924.84136379 23254.29931172 ... 18042.41085359\n",
      " 11209.8941986  12431.25153197]\n",
      "Threshold:  18777.65039506888\n",
      "[13411.69353256 15318.89680591 26126.82000412 ... 17707.61457228\n",
      " 15976.48186868  1126.67349425]\n",
      "Threshold:  18707.452060257034\n",
      "[11700.9776661  16605.5865634   8443.51718502 ... 17325.27695862\n",
      " 10872.47585197 13021.26882982]\n",
      "Threshold:  18724.929027659382\n",
      "Eta: 0.0001\n",
      "C: 1\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.16974911195636394\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.17138800131507909\n",
      "Validation F1 score stdev: 0.0011403519448976265\n",
      "[  7969.58730483 131291.08175792 108606.83379819 ...   7969.7109762\n",
      "  98507.07610499  72183.29251566]\n",
      "Threshold:  132203.90219481115\n",
      "[126360.89207667 106212.79882792  95097.97551095 ...   7969.66392529\n",
      "  61516.85296525  92755.59300764]\n",
      "Threshold:  132588.07030880544\n",
      "[102322.02387782  98400.97233826  84304.28419061 ... 128906.22979349\n",
      " 107515.79696485   7969.64468475]\n",
      "Threshold:  132497.1336477158\n",
      "[116591.38418452 106121.91867206 125563.89654872 ...  90210.22354845\n",
      " 150345.2309234  100028.09866858]\n",
      "Threshold:  133308.52167202326\n",
      "[128882.06771416 140336.2663219   74310.597477   ...  86304.15554091\n",
      "   7969.64863853  86673.94004794]\n",
      "Threshold:  132152.1179526569\n",
      "[149275.45832993 107440.16069989   7969.68569427 ...  88176.91707878\n",
      "  78398.28845971 148954.37928685]\n",
      "Threshold:  132589.26491979763\n",
      "[ 94116.10280233  92110.17981778  90122.42836574 ...  93413.15966436\n",
      "  79025.55628807 101788.66856484]\n",
      "Threshold:  132687.94104666417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119803.40171145 112643.02720144 164487.31747085 ... 127621.4074968\n",
      "  79292.2319312   87931.4837643 ]\n",
      "Threshold:  132822.12932308306\n",
      "[ 94867.69405588 108358.24085907 184808.16259678 ... 125254.87056935\n",
      " 113009.6221752    7969.66533219]\n",
      "Threshold:  132327.14907495925\n",
      "[ 82767.8754955  117461.02454195  59726.03400825 ... 122551.89878855\n",
      "  76907.42854226  92107.08473639]\n",
      "Threshold:  132452.45480669307\n",
      "Eta: 0.0001\n",
      "C: 10\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.16974911195636394\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.17138800131507909\n",
      "Validation F1 score stdev: 0.0011403519448976265\n",
      "[ 3.96443841 39.76330229 35.3868187  ...  6.83191862 32.903231\n",
      " 22.25362973]\n",
      "Threshold:  40.078647676152904\n",
      "[38.39740809 31.21645933 29.80519531 ...  5.71916944 18.89723822\n",
      " 28.91889006]\n",
      "Threshold:  40.15536617739521\n",
      "[31.17237382 31.75775977 26.89351684 ... 37.74728115 31.49613221\n",
      "  5.23935442]\n",
      "Threshold:  40.13891547232544\n",
      "[34.89022227 31.82822692 37.14065306 ... 28.85060095 45.92704835\n",
      " 29.65935044]\n",
      "Threshold:  40.342949131832704\n",
      "[39.57393956 40.76080934 24.67686109 ... 25.53303347  5.32089246\n",
      " 27.86914026]\n",
      "Threshold:  40.02147413327699\n",
      "[43.31976312 34.26651965  6.27604356 ... 27.80644297 23.65816561\n",
      " 46.17891327]\n",
      "Threshold:  40.12339139605349\n",
      "[27.77860128 29.37528086 30.73534636 ... 29.25885941 24.07223552\n",
      " 30.22674482]\n",
      "Threshold:  40.2139072205465\n",
      "[35.06384531 34.6705725  49.53078015 ... 37.31327082 24.46387121\n",
      " 28.13776795]\n",
      "Threshold:  40.210704482783115\n",
      "[30.13951628 32.68800559 57.40094729 ... 38.41005542 33.33831316\n",
      "  5.72004939]\n",
      "Threshold:  40.099135234725836\n",
      "[24.55190968 34.25501452 19.51617457 ... 37.57956657 23.56500338\n",
      " 27.86144036]\n",
      "Threshold:  40.06213590888815\n",
      "Eta: 0.0001\n",
      "C: 100\n",
      "F1 score for fold 0: 0.17300302599060208\n",
      "F1 score for fold 1: 0.1725435560980146\n",
      "F1 score for fold 2: 0.1708805049281789\n",
      "F1 score for fold 3: 0.17271488886697783\n",
      "F1 score for fold 4: 0.17265861886953576\n",
      "F1 score for fold 5: 0.1749756265485068\n",
      "F1 score for fold 6: 0.17372139212219642\n",
      "F1 score for fold 7: 0.17079397416759454\n",
      "F1 score for fold 8: 0.17251083603920364\n",
      "F1 score for fold 9: 0.1718125491808948\n",
      "Mean validation F1 score: 0.17256149728117054\n",
      "Validation F1 score stdev: 0.0011796173234003599\n",
      "[ 10923.64492229 179956.55762837 148863.93379033 ...  10923.7697258\n",
      " 135020.50774409  98939.36664081]\n",
      "Threshold:  181207.75455565614\n",
      "[173182.72633202 145568.96290567 130335.61291579 ...  10922.69964134\n",
      "  84311.3392167  127125.28820354]\n",
      "Threshold:  181717.31215524365\n",
      "[140223.38399248 134849.90086643 115531.6281137  ... 176654.73855856\n",
      " 147341.01497275  10921.65949883]\n",
      "Threshold:  181575.73382776318\n",
      "[159763.3892358  145417.24162293 172058.29203332 ... 123613.66234103\n",
      " 206015.75232959 137066.97781973]\n",
      "Threshold:  182670.64576923594\n",
      "[176588.62102124 192282.6998921  101817.12950857 ... 118250.227675\n",
      "  10919.62265416 118756.85646072]\n",
      "Threshold:  181069.0935014197\n",
      "[204511.70395336 147196.0893357   10918.64607353 ... 120804.89891009\n",
      " 107407.917506   204071.7751147 ]\n",
      "Threshold:  181651.0891016948\n",
      "[128929.74705993 126181.79872346 123458.74620704 ... 127966.76063136\n",
      " 108257.18540934 139440.4002314 ]\n",
      "Threshold:  181769.3236225946\n",
      "[164103.49079191 154295.37766667 225310.30098521 ... 174812.38760915\n",
      " 108612.3592428  120446.15686688]\n",
      "Threshold:  181936.17842124513\n",
      "[129935.06368184 148412.35308795 253121.59665504 ... 171554.72970168\n",
      " 154783.10716431  10915.56504115]\n",
      "Threshold:  181241.27174910603\n",
      "[113352.05385984 160864.93849186  81795.82284556 ... 167836.95597908\n",
      " 105326.06013632 126142.25940675]\n",
      "Threshold:  181395.9411937717\n",
      "Eta: 0.001\n",
      "C: 1\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.16974911195636394\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.17138800131507909\n",
      "Validation F1 score stdev: 0.0011403519448976265\n",
      "[  86542.87734622 1425707.13230834 1179375.98110874 ...   86544.30873896\n",
      " 1069701.42638382  783847.90093399]\n",
      "Threshold:  1435619.515587286\n",
      "[1372063.82327621 1153289.85494234 1032601.86719822 ...   86537.08678908\n",
      "  667968.15118931 1007167.59850004]\n",
      "Threshold:  1439680.3928202637\n",
      "[1110957.10323269 1068384.51842277  915330.29162018 ... 1399593.96831722\n",
      " 1167348.23563415   86530.17894177]\n",
      "Threshold:  1438582.1103697207\n",
      "[1265788.54647508 1152125.5336815  1363199.70099176 ...  979378.33048744\n",
      " 1632241.31055884 1085967.21601815]\n",
      "Threshold:  1447280.1598685824\n",
      "[1399116.08794571 1523460.32909269  806699.97327474 ...  936899.39007635\n",
      "   86516.88824051  940913.76050689]\n",
      "Threshold:  1434615.1053474937\n",
      "[1620377.46456809 1166257.54571564   86510.70093116 ...  957155.98855223\n",
      "  851009.46133092 1616892.27321753]\n",
      "Threshold:  1439249.80065976\n",
      "[1021546.80893815  999774.37184675  978199.15915839 ... 1013917.04774226\n",
      "  857752.36828697 1104825.70840266]\n",
      "Threshold:  1440209.9506725157\n",
      "[1300259.49081476 1222546.02447212 1785226.45215294 ... 1385110.47900803\n",
      "  860580.60730985  954344.83080219]\n",
      "Threshold:  1441555.4063967154\n",
      "[1029546.13032102 1175951.47800484 2005619.79294349 ... 1359321.16555421\n",
      " 1226430.29863844   86490.43650788]\n",
      "Threshold:  1436072.6038040982\n",
      "[ 898164.32146512 1274640.68021857  648123.42644578 ... 1329884.90525025\n",
      "  834569.10965259  999509.77460778]\n",
      "Threshold:  1437321.8148948709\n",
      "Eta: 0.001\n",
      "C: 10\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.16974911195636394\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.17138800131507909\n",
      "Validation F1 score stdev: 0.0011403519448976265\n",
      "[  852369.24087227 14041862.88938976 11615741.11496465 ...\n",
      "   852390.70151047 10535552.26689216  7720159.31937935]\n",
      "Threshold:  14139486.5717034\n",
      "[13513693.92268865 11358948.56273544 10170276.73036674 ...\n",
      "   852327.17550432  6578934.38615626  9919769.76092311]\n",
      "Threshold:  14179663.649159817\n",
      "[10942143.61036901 10522838.24862372  9015360.63691787 ...\n",
      " 13785009.17596837 11497553.22874129   852268.4254011 ]\n",
      "Threshold:  14169019.18707459\n",
      "[12467275.96272828 11347762.02012524 13426717.660447   ...\n",
      "  9646307.78587145 16076623.50300972 10696140.46007821]\n",
      "Threshold:  14254859.077747444\n",
      "[13780646.37495752 15005373.67273313  7945626.40815362 ...\n",
      "  9228023.17483722   852158.69577619  9267568.57974729]\n",
      "Threshold:  14130297.326187149\n",
      "[15960158.57468443 11487242.08072141   852110.69444515 ...\n",
      "  9427661.71311172  8382151.70653425 15925838.22440772]\n",
      "Threshold:  14176121.87416297\n",
      "[10062008.3881251   9847560.35285474  9635054.3131817  ...\n",
      "  9986861.27025702  8448671.56560977 10882287.45720671]\n",
      "Threshold:  14185751.085249\n",
      "[12807423.01020963 12041957.68648157 17584301.5537981  ...\n",
      " 13643196.47171252  8476634.19929125  9400205.14435598]\n",
      "Threshold:  14199176.478014888\n",
      "[10141054.950261   11583146.79746235 19755401.38590425 ...\n",
      " 13389343.6671603  12080362.31540421   851941.10875244]\n",
      "Threshold:  14145341.92497351\n",
      "[ 8847045.21073715 12555388.93042242  6384111.75161182 ...\n",
      " 13099556.99273131  8220625.34564288  9845313.60897332]\n",
      "Threshold:  14157825.08098154\n",
      "Eta: 0.001\n",
      "C: 100\n",
      "F1 score for fold 0: 0.1715850230955856\n",
      "F1 score for fold 1: 0.17166019056785062\n",
      "F1 score for fold 2: 0.16974911195636394\n",
      "F1 score for fold 3: 0.17129807182549\n",
      "F1 score for fold 4: 0.17146093488068873\n",
      "F1 score for fold 5: 0.17374414364333596\n",
      "F1 score for fold 6: 0.17266336120497722\n",
      "F1 score for fold 7: 0.16976298378621305\n",
      "F1 score for fold 8: 0.17084193443524698\n",
      "F1 score for fold 9: 0.1711142577550388\n",
      "Mean validation F1 score: 0.17138800131507909\n",
      "Validation F1 score stdev: 0.0011403519448976265\n",
      "Best eta value: 0.0001\n",
      "Best C value: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.66112787 18.93190292 12.74201339 ... 17.33072038 16.07872706\n",
      " 12.12901723]\n",
      "Threshold:  19.806449688603355\n",
      "Test precision: 0.21010762751520823\n",
      "Test recall: 0.21017319394601341\n",
      "Test F1 score: 0.21014040561622466\n",
      "Test AUPRC score: 0.1735011653325141\n"
     ]
    }
   ],
   "source": [
    "#SVM with predicting the top expected number of 1s as 1s\n",
    "eta_vals = [0.00001, 0.0001, 0.001]\n",
    "C_vals = [1, 10, 100]\n",
    "best_score = 0\n",
    "best_eta, best_c = (0, 0)\n",
    "\n",
    "for eta_val in eta_vals:\n",
    "    for c_val in C_vals:\n",
    "\n",
    "        # instantiate svm object\n",
    "        svm = MySVM(num_features, 100000, eta_val, c_val)\n",
    "\n",
    "        # call to your CV function to compute error rates for each fold\n",
    "        cv_scores = my_cross_val_imbalanced(svm, 'auprc', proportion, X_train, y_train, k=10)\n",
    "        #cv_scores = my_cross_val_imbalanced(svm, 'f1', None, X_train, y_train, k=10)\n",
    "\n",
    "        # print error rates from CV\n",
    "        print(\"Eta: \" + str(eta_val))\n",
    "        print(\"C: \" + str(c_val))\n",
    "        for i in range(10):\n",
    "            print(\"AUPRC score for fold \" + str(i) + \": \" + str(cv_scores[i]))\n",
    "        mean_score = sum(cv_scores)/len(cv_scores)\n",
    "        print(\"Mean validation AUPRC score: \" + str(mean_score))\n",
    "        print(\"Validation AUPRC score stdev: \" + str(np.std(cv_scores)))\n",
    "        if mean_score >= best_score:\n",
    "            best_score = mean_score\n",
    "            best_eta, best_c = (eta_val, c_val)\n",
    "\n",
    "# instantiate svm object for best value of eta and C\n",
    "print(\"Best eta value: \" + str(best_eta))\n",
    "print(\"Best C value: \" + str(best_c))\n",
    "best_svm = MySVM(num_features, 100000, best_eta, best_c)\n",
    "\n",
    "# fit model using all training data\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_preds = best_svm.predict_proportion(X_test, proportion)\n",
    "#y_preds = best_svm.predict(X_test)\n",
    "\n",
    "# compute F1 score on test data\n",
    "(precision, recall, f1) = precision_recall_f1(y_preds, y_test)\n",
    "auprc = sklearn.metrics.average_precision_score(y_test, y_preds)\n",
    "\n",
    "print(\"Test precision: \" + str(precision))\n",
    "print(\"Test recall: \" + str(recall))\n",
    "print(\"Test F1 score: \" + str(f1))\n",
    "print(\"Test AUPRC score: \" + str(auprc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b68b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "372e04c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16, 135, 253,  69, 211,  23, 209,  51, 262, 217,  73,  12, 165,\n",
       "        34, 263, 155, 151, 176, 177, 240,  44, 256, 254, 279, 293, 252,\n",
       "        42, 270, 271,  30, 247, 143, 248, 267, 109,  68, 255,  38, 224,\n",
       "       236,  74, 202,  21, 287, 161,  92, 181,  14, 275,  22, 188,  47,\n",
       "        95,  49,  77, 123, 225,  98,  96, 187, 204, 282,  79, 292, 136,\n",
       "       119, 290, 128, 168, 167, 199, 288, 272, 281,  27, 296,  39, 170,\n",
       "       213, 250, 116, 114, 102, 237, 166, 159, 158, 110, 276, 172, 106,\n",
       "       246, 259,  76,  45,  41, 121, 118,  25, 103, 261, 105, 101, 163,\n",
       "        13, 257, 183, 115, 117, 129, 107, 138, 242,  31,  82, 249,  71,\n",
       "       220,  64, 192, 182,  63, 233, 191, 284, 141, 162, 180, 113, 111,\n",
       "       265, 140, 169,  48, 235, 234, 227, 196, 294,  15,  58, 251, 216,\n",
       "       133,  86,  40, 134,  84, 149,  88, 208, 152, 144, 142, 239,  18,\n",
       "       266, 238,  57, 285, 278, 178, 283,  80, 147, 214, 157, 130, 228,\n",
       "        28,  33,  60, 274, 100, 218, 139, 195,  70, 146,  37,  29,  83,\n",
       "       164,  91, 241, 148, 194, 185, 124, 223,  20, 291,  89, 120, 280,\n",
       "        93,  55,  85,  19, 173, 207, 201, 260, 273,  11, 200,  10,  61,\n",
       "       160, 126, 189, 243, 277, 150, 215, 231, 221, 171, 299, 258, 145,\n",
       "       219,  26,  94, 244, 297,  17, 245, 154, 229,  75, 210, 298, 127,\n",
       "        46,  59,  52,  43, 175,  87,  24, 212,  50, 222, 122, 295,  81,\n",
       "       289, 206,  97,  72,  66,  67,  56,  54, 112,  53, 179, 131,  65,\n",
       "        62, 269, 268,  99,  36,  32, 197, 193, 205, 153, 156, 108, 137,\n",
       "        90, 132, 184,  35, 264, 203,  78, 230, 232, 104, 186, 174, 125,\n",
       "       226, 190, 286, 198], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Vintage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e639383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vehicle_Age_> 2 Years'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e91cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
